
% -*- fill-column: 80; -*-
\documentclass[sigconf,review,anonymous,screen,acmsmall]{acmart}
\usepackage{mathpartir}
\usepackage{tikz-cd}
\usepackage{lipsum}
\usepackage{enumitem}
\usepackage{wrapfig}
\usepackage{fancyvrb}
\usepackage{xspace}
\usepackage{hyperref}
\usepackage{cleveref}
\usepackage{stmaryrd}
\usepackage{scalerel}
\usepackage{tikz}
\usetikzlibrary{automata, positioning, arrows, fit}

\usepackage{pdfpages}

\newcommand{\sem}[1]{\llbracket{#1}\rrbracket}
\newcommand{\cat}[1]{\mathbf{#1}}
\newcommand{\lto}{\multimap}
\newcommand{\tol}{\mathrel{\rotatebox[origin=c]{180}{$\lto$}}}
\newcommand{\String}{\Sigma^{*}}
\newcommand{\Set}{\mathbf{Set}}
\newcommand{\SemAct}{\mathbf{SemAct}}
\newcommand{\Gr}{\mathbf{Gr}}
\newcommand{\Grammar}{\mathbf{Gr}}
\newcommand{\Type}{\mathbf{Type}}
\newcommand{\Prop}{\mathbf{Prop}}
\newcommand{\Bool}{\mathbf{Bool}}
\newcommand{\true}{\mathbf{true}}
\newcommand{\false}{\mathbf{false}}
\newcommand{\nat}{\mathbb{N}}
\newcommand{\theoryname}{Grammar Type Theory\xspace}
\newcommand{\theoryabbv}{GramTT\xspace}

\newcommand{\gluedNL}{{\mathcal G}_S}
\newcommand{\gluedNLUniv}{{\mathcal G}_{S,i}}
\newcommand{\gluedL}{{\mathcal G}_L}

\newcommand{\amp}{\mathrel{\&}}
\newcommand{\pair}{\amp}
\DeclareMathOperator*{\bigamp}{\scalerel*{\&}{\bigoplus}}

\newcommand{\simulsubst}[2]{#1\{#2\}}
\newcommand{\subst}[3]{\simulsubst {#1} {#2/#3}}
\newcommand{\letin}[3]{\mathsf{let}\, #1 = #2 \, \mathsf{in}\, #3}
\newcommand{\lamb}[2]{\lambda #1.\, #2}
\newcommand{\lamblto}[2]{\lambda^{{\lto}} #1.\, #2}
\newcommand{\lambtol}[2]{\lambda^{{\tol}} #1.\, #2}
\newcommand{\dlamb}[2]{\overline{\lambda} #1.\, #2}
\newcommand{\app}[2]{#1 \, #2}
\newcommand{\applto}[2]{#1 \mathop{{}^{\lto}} #2}
\newcommand{\apptol}[2]{#1 \mathop{{}^{\tol}} #2}
\newcommand{\PiTy}[3]{\textstyle\prod (#1 : #2). #3}
\newcommand{\SigTy}[3]{\textstyle\sum (#1 : #2). #3}
\newcommand{\LinPiTy}[3]{\textstyle\bigamp (#1 : #2). #3}
\newcommand{\LinSigTy}[3]{\textstyle\bigoplus (#1 : #2). #3}
\newcommand{\GrTy}{\mathsf{Gr}}

\newcommand{\ctxwff}[1]{#1 \,\, \mathsf{ok}}
\newcommand{\ctxwffjdg}[2]{#1 \vdash #2 \,\, \mathsf{type}}
\newcommand{\linctxwff}[2]{#1 \vdash #2 \,\, \mathsf{ok}}
\newcommand{\linctxwffjdg}[2]{#1 \vdash #2 \,\, \mathsf{linear}}

\newif\ifdraft
\drafttrue
\newcommand{\steven}[1]{\ifdraft{\color{orange}[{\bf Steven says}: #1]}\fi}
\renewcommand{\max}[1]{\ifdraft{\color{blue}[{\bf Max says}: #1]}\fi}
\newcommand{\pedro}[1]{\ifdraft{\color{red}[{\bf Pedro says}: #1]}\fi}
\newcommand{\pipe}{\,|\,}

\begin{document}

\pagestyle{plain}

\pagebreak

\title{Formal Grammars as Types in Non-Commutative Linear-Non-Linear Type Theory}

\author{Steven Schaefer}
\affiliation{\department{Electrical Engineering and Computer Science}
  \institution{University of Michigan}
  \country{USA}
}
\email{stschaef@umich.edu}

\author{Max S. New}
\affiliation{
  \department{Electrical Engineering and Computer Science}
  \institution{University of Michigan}
  \country{USA}
}
\email{maxsnew@umich.edu}

\author{Pedro H. Azevedo de Amorim}
\affiliation{
  \department{Department of Computer Science}
  \institution{University of Oxford}
  \country{UK}
}
\email{pedro.azevedo.de.amorim@cs.ox.ac.uk}

\makeatletter
\let\@authorsaddresses\@empty
\makeatother

\begin{abstract}
  We propose \theoryname~(\theoryabbv) a new type-theoretic calculus
  for the specification of formal grammars and intrinsic verification
  of parsers. \theoryabbv is a dependent non-commutative
  linear/non-linear type theory where the linear types can be viewed
  as formal grammars, and linear terms can be viewed as parse
  transformers, of which parsers are a special case. We formalize this
  interpretation by giving a denotational semantics of the theory in a
  novel categorical semantics of formal grammars in a category of
  families indexed by strings.

  To demonstrate the usefulness of the calculus for formal grammar
  theory, we show that different grammar formalisms \max{which ones do
    we actually show?} \steven{regular expressions, $\mu$-regular expressions
    (or context free expressions), and different automata classes (finite,
    pushdown, turing)} can be expressed as linear types in
  \theoryabbv. We give simple characterizations of the regular and
  context-free grammars as expressible by restricting which
  connectives of \theoryabbv are allowed, a form of type theoretic
  characterization of these grammar classes. Further, we demonstrate
  some central proofs of formal grammar theory can be intrinsically
  verified: \emph{strong} equivalence between regular expressions and
  NFAs is expressed by a syntactic \emph{isomorphism} definable in
  \theoryabbv.

  Finally, since \theoryabbv is a syntactic calculus, in addition to
  the ``standard'' semantics of linear types as grammars, we
  demonstrate two non-standard semantic interpretations. One is a
  semantics where linear types are instead interpreted as
  \emph{semantic actions}, rather than grammars, which are more common
  in application than formal grammars proper. Lastly we use a
  non-standard semantics to prove a \emph{canonicity} metatheorem that
  shows that the interpretation of types as grammars is canonical: the
  type-theoretic notion of a parse agrees with the expected semantic
  notion.
\end{abstract}

\maketitle

\section{Introduction}
\label{sec:intro}

Parsing data from a serialized format is one of the most common tasks
in all of computing. Accordingly, this is a quite well-studied problem
spanning theoretical computer science and linguistics with such
milestones as Chomsky's hierarchy of grammars
\cite{chomThreeModels1956}, practical algorithms for parsing of
regular and context-free grammars, and variants
\cite{KNUTH1965607,Earley1970}, as well as implementations of
practical tools for the generation of efficient parsers
\cite{Johnsonyacc}.
%
The final frontier for parsing research is the development of
\emph{formally verified} parser implementations, which due to the
pervasive nature of parsing would be a useful component of many
formally verified software systems: compilers and servers most
especially.
%

In this work we aim to refine formal language theory into a
mathematical theory of formal grammars that can be used to develop
\emph{correct-by-construction} parsers. Our approach can be summarized
in the Curry-Howard-style slogan ``grammars as types'', and the
addendum ``parsers as terms'' where since the types of the terms are
grammars, this means not just parsers but intrinsically verified
parsers as terms. That is, we develop a type theory with a novel
denotational semantics in a category with abstract formal grammars as
objects and \emph{parse transformers} as morphisms. We say these are
\emph{abstract} formal grammars because they are not tied to any
particular syntactic formalism such as Chomskyian grammars, regular
expressions, Lambek calculus etc. The category we choose is a rich
category: it is a topos as well as being monoidal closed, and these
universal properties serve as the inspiration for our type theory.

The type theory itself is a form of dependent linear-non-linear type
theory: the linear types can be interpreted as grammars, whereas the
non-linear types can be interpreted as ordinary sets. As in prior work
on dependent linear-non-linear types, the linear types are allowed to
depend on the non-linear types but not vice-versa \cite{dep-lnl}.

The substructural nature of \theoryabbv is well-aligned with the
requirements intrinsic to parsing and to the theory of formal
languages, where strings constitute a very clear notion of resource
that cannot be duplicated, reordered or dropped.


%% The theory of formal languages and parsing is one of the oldest and
%% most thoroughly developed areas of theoretical computer science. A
%% prominent topic in the 1950, 60s, and 70s led to a series of remarkable
%% developments: 
In this paper, we propose an intuitive and structured generalization of the
theoretical machinery used for parsing. In conjunction with an updated
point-of-view on formal grammars, we provide a domain-specific
language to build and verify parsers.

\steven{The above paragraph maybe isn't the right thing to say, but I want
  something that ties the first paragraph to the contributions that follow}
\pedro{As a rule of thumb, I think that it's good to avoid subjective and
  pointed adjectives such as ``intuitive'', espcially if we don't have evidence
  that corroborates our claim. This is the perfect excuse for
an annoying reviewer to complain about our work.}
\max{I agree about ``intuitive'', try to stick to objective claims}

\paragraph{Contributions} This paper begins in \cref{sec:synindnotion} by
giving a universal semantic characterization of formal grammar.
From there, in \cref{sec:tt} we provide the syntax of \theoryname, a type thoery
that axiomatizes the underlying structures needed to conduct formal grammar
theory. Then in \cref{sec:categorify}, we characterize the abstract categorical
models of our syntax. Afterwards, we demonstrate several models of \theoryabbv,
including one to prove a metatheoretic canonicity theorem. Finally, in
\cref{sec:automata} we give a presentation of automata as grammars. With this
view in mind, we may encode parsing algorithms and execute them entirely
internal to our formalism. Our contributions are then:

\begin{itemize}
  \item A syntax-independent notion of grammar given by the category $\Grammar$
  \item \theoryname: A dependent
    linear-non-linear type theory meant to syntactically capture the latent structures
    in $\Grammar$ that enable parsing
  \item Categorical structures that capture the concepts integral to formal grammar theory, and that are notably modeled by $\Grammar$
  \item An implementation of the language, which is embedded shallowly in Agda
\end{itemize}


\section{A Syntax-Independent Notion of Syntax}
\label{sec:synindnotion}

The notion of formal language is central to the theory of parsing. A
\emph{formal language} $L$ over an alphabet $\Sigma$ is classically defined as a
\emph{subset} of strings $L \subseteq \String$. This definition is
especially useful as it gives a semantics to formal grammars that is completely
independent of any particular syntactic grammar formalism. Any new
notion of grammar can be given a language semantics and it provides a precise
mathematical specification for implementing a \emph{recognizer} of a language.

That is, a recognizer should be extensionally equivalent to the indicator function
$\chi_{L} : \String \to \Prop$ mapping each string $w$ to the the proposition
that it is in the language $w \in L$. However, language recognition is
insufficient for specifying a parser --- a function that decomposes a string
into semantic components that adhere to the structure of a formal grammar. In
practice, parsers do more than simply return whether a string is valid or
invalid; instead, they emit structured \emph{parse trees}\footnote{Such parse
  trees are usually not materialized in memory, as semantic actions often perform
  computation over the trees while they are being constructed.}.

Our first contribution is a novel and syntax-independent characterization of
formal grammar:

\begin{definition}
  \label{def:grammar}
  A \emph{formal grammar} $G$ over a fixed alphabet $\Sigma$ is a function
  $G : \String \to \Set$.
\end{definition}

We say that a grammar $G$ associates to every string $w$ the set\footnote{We
  make little comment as to which foundations are used to encode $\Set$. Our
  construction is polymorphic in choice of a proper class of small sets, a
  Grothendieck universe, a type theoretic universe, or any similar foundation.} $G w$ of parse
trees showing that $w$ matches $G$. This definition of formal grammar serves as
a mathematical specification for a parser --- taking on the same role as
languages do for recognizers. In this sense, our new characterization of
grammars is just a proof-relevant generalization of the existing notion of
formal language.

By ranging over all input strings, notice that each grammar induces a language.
For each formal grammar $G$, define the language of accepted strings $L_{G}$ as,

\[ L_{G} = \{ w \in \String ~|~ G w~\text{is inhabited} \} \]

As they are simply sets, formal languages are naturally endowed with a partial
order via subset inclusion. Similarly, formal grammars naturally coalesce into a
\emph{category}. Define $\Grammar$ to be the category of formal grammars. Whose
objects are grammars, and whose morphisms are functions $f : G \to H$ between two
grammars presented by a family of functions

\[ f^{w} : G w \to H w \]

for every string $w \in \String$. Intuitively, we read a morphism of grammars as
a \emph{parse transformer} --- i.e. $f$ translates a $G$-parse of $w$ into an $H$-parse
of $w$. The induced notion of isomorphism in $\Grammar$ encodes that two grammars are equivalent if there is
a bijective translation of parses. In other words, two grammars are isomorphic
if they capture the same structural descriptions of strings, which is
precisely Chomsky's notion of \emph{strong equivalence} of
grammars \cite{chom1963}. In works like
\cite{yoshinaga2002formal}, this notion of strong equivalence is often treated
like the appropriate choice of ``sameness'' for two grammars. When abstracting over the latent
structure of formal grammars and describing it in the the language of
categories, it is encouraging to see the same definition of equivalence arise
naturally.

Upon inspection of the above definition, $\Grammar$ may equivalently
be described as the functor category $\Set^{\String}$ --- where the
set $\String$ is viewed as a discrete\footnote{Objects in $\String$
comprise the set of all strings, and the only morphisms are the
identity morphisms.}  category. In fact, in this paper we will take
$\Grammar$ to be defined as the functor category
$\Set^{\String}$. Such functor categories into $\Set$ --- often called
presheaf categories --- carry a remarkable amount of structure, and we
will demonstrate that these well-known constructions on presheaf
categories provide us with familiar compositional constructions on
grammars\max{where do we talk about the properties of this presheaf cat?}.

The category-theoretic framework can be further used to describe and compare
different notions of formal grammar. For instance, Chomskyan generative
grammars, semi-Thue systems, Montague grammars, and so on are all distinct syntactic
presentations of the same underlying idea of an abstract specification for
parsing. Each of these is an instance of a general \emph{notion of formal
  grammar} --- a category paired with a functor into
$\Grammar$. That is, they may all be interpreted in our category of grammars. In this sense, $\Grammar$ provides a universal setting to reason about parsing.

\steven{I initially wanted to say ``universal semantic domain to reason about
  parsing'', but these words have specific domain theory usage that aren't
  appropriate here, right?}
\pedro{If you want to keep ``semantic domain'', we could go for ``all-encompassing semantic domain''}
\steven{Check details for 2-cat of grammar formalisms}
\max{this section kind of trails off at the end with a very vague aside about a $2$-category of grammar formalisms. Maybe this should go in future work/discussion? What is the message of this section? How does it tie in with the message of the paper? What even is the message of this paper?}

\section{\theoryname as a Syntax for Formal Grammars}
\label{sec:tt}

Omission of the structural rules of a deductive system, such as in
linear logic \cite{GIRARD19871}, offers precise control over how a value is used
in a derivation. Namely\max{namely?}, linear logic omits the weakening and contraction rules
to ensure that every value in context is used exactly once. This control enables
\emph{resource-sensitive} reasoning, where we may treat a resource as
\emph{consumed} after usage. This viewpoint is amenable to parsing applications,
as we may treat characters of a string as finite resources that are consumed at
parse-time. That is, once a particular substring has been parsed, a parser need not go
over it again\max{what about a backtracking parser?}. One may then envision a linear type system where the types comprise
formal grammars generated over some alphabet $\Sigma$, and the type constructors
correspond precisely to inductive constructions on grammars --- such as
conjunction, disjunction, concatenation, etc.

Unfortunately, programming in a linear term language is often
cumbersome and unintuitive for users.\max{You are arguing against our own type theory here...} Code in such a language can
become unnecessarily verbose when using Girard encodings to translate
high-level reasoning into low-level linear terms.\pedro{What encodings are these?
We should add a citation here}
To alleviate this
concern, in 1995 Benton et al.\ proposed an alternative categorical
semantics of linear logic based on adjoint interactions between linear
and non-linear logics \cite{bentonMixedLinearNonlinear1995} ---
appropriately referred to as a \emph{linear-non-linear} (LNL)
system. This work is simply typed, so the boundary between linear and
non-linear subtheories is entirely characterized via a monoidal
adjunction $F \dashv G$ between linear terms and non-linear terms.

Inside of an LNL system, linearity may be thought of as an option that users can
choose to deliberately invoke at deliberate points in their developments in an
otherwise intuitionistic system. However, if we are wishing to treat parsers as
linear terms over input strings, the non-linear fragment of an LNL theory does
not really assist in the development of parsers. It is instead the case that
parsers may benefit from a \emph{dependence} on non-linear terms.
Through the approach described by Krishnaswami et al.\ in
\cite{krishnaswami_integrating_2015},
we may define a restricted form of dependent types. In particular, dependence
on linear terms
is disallowed; however, through dependence of a linear term on a non-linear
index, we recover the definition of Aho's indexed grammars \cite{AhoIndexed}
internal to \theoryabbv.

\steven{The above is meant to walk the reader into accepting LNL as a natural
  choice, but it doesn't really flow well to the beginning of the next subsection }


In this section we go through a tour of \theoryabbv. We start by presenting its syntax.
In order to avoid a notion of raw-term, we present it in an intrinsically-typed fashion,
where only well-typed terms are meaningful. Next, we show how it admits a model in $\Set^{\String}$
where the linear types have natural interpretation in terms of formal languages --- for instance,
the tensor corresponds to grammar concatenation. We conclude the section by showing how
classes of grammars correspond to certain subtype-theories inside \theoryabbv.

\pedro{Hopefully this serves as a segue between this and the syntax subsection}
\subsection{Syntax}
\label{subsec:syntax}

Below we describe \theoryname~(\theoryabbv), an instance of an LNL theory with dependent types
to serve as a deductive
setting for formal grammar theory. \theoryabbv axiomatizes the necessary structure underlying $\Set^{\String}$ \pedro{We should probably stick to either $\Set^{\String}$ or $\mathbf{Gr}$ from now on}to
specify and construct parsers.

The structural judgments are shown in \cref{fig:structjdg}, the typing
well-formedness in \cref{fig:typewf}, and the intuitionistic typing
rules in \cref{fig:inttyping}. These are mostly just as they appear in
\cite{krishnaswami_integrating_2015}. It has two main differences. The
first one it the presence of two-distinct arrow types, one for adding
variables to the beginning of contexts and another one for adding
variables to the end of contexts. This is an adequate change in the
context of grammars, which are inherently non-symmetric. The second
change is the introduction of inductive types, which also have a
natural grammar counterpart\max{what is it then?}, which usually allows kinds of recursive
behavior such as the Kleene star. The treatment of the non-linear
types is pretty\max{pretty?} standard, so below we focus on the linear syntax.

\begin{figure}
  \begin{mathpar}
    \boxed{\ctxwff \Gamma}

    \inferrule{~}{\ctxwff \cdot}
    \and
    \inferrule{\ctxwff \Gamma \\ \ctxwffjdg \Gamma X}{\ctxwff {\Gamma, x : X}}

    \\

    \boxed{\linctxwff \Gamma \Delta}

    \inferrule{~}{\linctxwff \Gamma \cdot}
    \and
    \inferrule{\linctxwff \Gamma \Delta \\ \linctxwffjdg \gamma A}{\linctxwff \Gamma {\Delta, a : A}}

    \\

    \boxed{\ctxwffjdg \Gamma X}

    \inferrule{\Gamma \vdash X : U_i}{\ctxwffjdg \Gamma X}

    \boxed{\linctxwffjdg \Gamma A}

    \inferrule{\Gamma \vdash A : L_i}{\linctxwffjdg \Gamma A}

    \\

    \boxed{\ctxwffjdg \Gamma {X \equiv Y}}

    \inferrule{\Gamma \vdash X \equiv Y : U_i}{\ctxwffjdg \Gamma {X\equiv Y}}

    \boxed{\linctxwffjdg \Gamma {A \equiv B}}

    \inferrule{\Gamma \vdash A \equiv B : L_i}{\linctxwffjdg \Gamma {A \equiv B}}

  \end{mathpar}
  \caption{Structural judgments}
  \label{fig:structjdg}
\end{figure}

\begin{figure}
  \begin{mathpar}
    \boxed{\Gamma \vdash X : U_{i}}

    \boxed{\Gamma \vdash A : L_{i}}

    \inferrule{~}{\Gamma \vdash U_i : U_{i+1}}
 %
    \and
%
    \inferrule{~}{\Gamma \vdash L_i : U_{i+1}}
%
    \\
%
    \inferrule{\Gamma \vdash X : U_i \\ \hspace{-0.1cm} \Gamma, x : X \vdash Y : U_i}{\Gamma \vdash \PiTy x X Y : U_i }%
%
    \and
%
    \inferrule{\Gamma\vdash X : U_i \\ \hspace{-0.1cm} \Gamma, x : X \vdash Y : U_i}{\Gamma \vdash \SigTy x X Y : U_i}
%
    \\
%
    \inferrule{~}{\Gamma \vdash 1 : U_i}
%
    \and
%
    \inferrule{\Gamma \vdash A : L_i}{\Gamma \vdash G A : U_i}

    \and
%
    \inferrule{~}{\Gamma \vdash I : L_i}
 %
    \and
%
    \inferrule{\Gamma \vdash A : L_i \\ \hspace{-0.1cm}\Gamma \vdash B : L_i}{\Gamma \vdash A \otimes B : L_i}
%
    \\
%
    \inferrule{\Gamma \vdash A : L_i \\ \hspace{-0.1cm}\Gamma \vdash B : L_i}{\Gamma \vdash A \lto B : L_i}
%
    \and
%
    \inferrule{\Gamma \vdash A : L_i \\ \hspace{-0.1cm}\Gamma \vdash B : L_i}{\Gamma \vdash B \tol A : L_i}
%
    \\
%
    \inferrule{\Gamma \vdash X : U_i \\ \Gamma, x : X \vdash A : L_i}{\Gamma \vdash \LinPiTy x X A : L_i}
%
    \and
%
    \inferrule{\Gamma \vdash X : U_i \\ \Gamma, x : X \vdash A : L_i}{\Gamma \vdash \LinSigTy x X A : L_i}
%
    \\
%
    \inferrule{\Gamma \vdash X : U_i \quad \{\Gamma \vdash e_i : X\}_i}{\Gamma \vdash e_1 =_X e_2 : U_i}
    %
    \and
    %
    \inferrule{~}{\Gamma \vdash \top : L_i}
%
    \and

    \inferrule{~}{\Gamma \vdash \bot : L_i}

    \\
    \inferrule{c \in \Sigma}{\Gamma \vdash c : L_0}
    %
    \and
    %
    \inferrule{\Gamma, x : L_i \vdash A : L_i \and A \textrm{ strictly positive}}{\Gamma \vdash \mu x.\, A : L_i}
  \end{mathpar}
  \caption{Type well-formedness}
  \label{fig:typewf}
\end{figure}

\begin{figure}
  \begin{mathpar}
  \boxed{\Gamma \vdash x : X}

  \inferrule{~}{\Gamma, x : X, \Gamma' \vdash x : X}
  %
  \and
  %
  \inferrule{\Gamma \vdash e : Y \quad \ctxwffjdg \Gamma {X \equiv Y}}{\Gamma \vdash e : X}
  %
  \\\
  %
  \inferrule{~}{\Gamma \vdash () : 1}
  %
  \and
  %
  \inferrule{\Gamma \vdash e : X \\ \Gamma \vdash e : \subst Y e x}{\Gamma \vdash (e, e') : \SigTy x X Y}
  %
  \\
%
  \inferrule{\Gamma \vdash e : \SigTy x X Y}{\Gamma \vdash \pi_1\, e : X}
  %
  \and
  %
  \inferrule{\Gamma \vdash e : \SigTy x X Y}{\Gamma \vdash \pi_2\, e : \subst Y {\pi_1\, e} x}
  \and
  \inferrule{\Gamma, x : X \vdash e : Y}{\Gamma \vdash \lamb x e : \PiTy x X Y}
  %
  \and
  %
  \inferrule{\Gamma \vdash e : \PiTy x X Y \\ \Gamma \vdash e' : X}{\Gamma \vdash \app e {e'} : \subst Y {e'} x}
  %
  \\
  %
  \inferrule{\Gamma \vdash e \equiv e' : X}{\Gamma \vdash \mathsf{refl} : e =_X e'}
  \and
  \inferrule{\Gamma ; \cdot \vdash e : A}{\Gamma \vdash \mathsf G e : \mathsf G A}
  \end{mathpar}
  \caption{Intuitionistic typing}
  \label{fig:inttyping}
\end{figure}

\begin{figure}
  \begin{mathpar}
    \boxed{\Gamma ; \Delta \vdash a : A}

    \inferrule{~}{\Gamma ; a : A \vdash a : A}
    \and
    \inferrule{\Gamma ; \Delta \vdash e : B \\ \linctxwffjdg \Gamma {A \equiv B}}{\Gamma ; \Delta \vdash e : A}
    %
    \\
    %
    \inferrule{~}{\Gamma ; \cdot \vdash () : I}
    \and
    \inferrule{\Gamma ; \Delta \vdash e : I \\ \Gamma ; \Delta_1',\Delta_2' \vdash e' : C}{\Gamma ; \Delta_1',\Delta,\Delta_2' \vdash \letin {()} e {e'} : C}
    %
    \\
    %
    \inferrule{\Gamma ; \Delta \vdash e : A \\ \Gamma ; \Delta' \vdash e' : B}{\Gamma ; \Delta, \Delta' \vdash e \otimes e' : A \otimes B}
    %
    \\
    %
    \inferrule{\Gamma ; \Delta \vdash e : A \otimes B \\ \Gamma ; \Delta'_1, a : A, b : B, \Delta'_2 \vdash e'}{\Gamma ;  \Delta_1', \Delta, \Delta'_2 \vdash \letin {a \otimes b} e {e'}}
    \\
    %
    \inferrule{\Gamma ; a : A , \Delta \vdash e : B}{\Gamma ; \Delta \vdash \lamblto a e : A\lto B}
    \and
    \inferrule{\Gamma ; \Delta' \vdash e' : A \\ \Gamma ; \Delta \vdash e : A \lto B}{\Gamma ; \Delta', \Delta \vdash \applto {e'} {e} : B}
    \\
    %
    \inferrule{\Gamma ; \Delta , a : A \vdash e : B}{\Gamma ; \Delta \vdash \lambtol a e : B\tol A}
    \and
    \inferrule{\Gamma ; \Delta \vdash e : B \tol A \\ \Gamma ; \Delta' \vdash e' : A}{\Gamma ; \Delta, \Delta' \vdash \apptol e {e'} : B}
    %
    \\
    %
    \inferrule{\Gamma, x : X ; \Delta  \vdash e : A}{\Gamma ; \Delta \vdash \dlamb x e : \LinPiTy x X A}
    \and
    \inferrule{\Gamma ; \Delta \vdash e : \LinPiTy x X A \\ \Gamma \vdash e' : X}{\Gamma ; \Delta \vdash \app e {e'} : \subst A {e'} x}
    %
    \\
    %
    \inferrule{\Gamma \vdash e : X \quad \Gamma ; \Delta \vdash e' : \subst A e x}{\Gamma ; \Delta \vdash (e, e') : \LinSigTy x X A}
    %
    \\
    %
    \inferrule{\Gamma ; \Delta \vdash e : \LinSigTy x X A \quad \Gamma, x : X ; \Delta'_1, a : A, \Delta'_2 \vdash e' : C}{\Gamma; \Delta'_1, \Delta, \Delta'_2 \vdash \letin {(x, a)} e {e'}: C}
    %
    \\
    %
    \inferrule{~}{\Gamma ; \Delta \vdash () : \top}

    \and

    \inferrule{~}{\Gamma ; x : \bot \vdash \mathsf{absurd}_A : A}
    %
    \and
    %
    %
    \inferrule{\Gamma \vdash e : \mathsf{G} A}{\Gamma ; \cdot \vdash \mathsf{G}^{-1}\, e : A}
    %
    \\
    %
    \inferrule{\Gamma; \Delta \vdash e : \subst A {\mu x.\, A} x}{\Gamma; \Delta \vdash \mathsf{cons}\, e : \mu x.\, A}
    \and
    \inferrule{\Gamma;\Delta\vdash e' : \mu x.\,A \and \Gamma; a:\subst A B x \vdash e : B}{\Gamma;\Delta\vdash \mathsf{fold}(a.e)(e') : B}
  \end{mathpar}
  \caption{Linear typing}
  \label{fig:linsyntax}
\end{figure}

\begin{figure}
  \begin{mathpar}
    \boxed{\Gamma \vdash e \equiv e' : X}

    \inferrule{\Gamma \vdash p : e =_X e'}{\Gamma \vdash e \equiv e' : X}
%
    \and
%
    \inferrule{~}{\Gamma \vdash \app {(\lamb x e)} {e'} \equiv \subst x e {e'} : X}
%
    \and
%
    \inferrule{~}{\Gamma \vdash e \equiv \lamb x {\app e x} : \PiTy x X Y}
%
    \and
%
    \inferrule{~}{\Gamma \vdash \pi_1\, (e_1, e_2) \equiv e_1 : X}
%
    \and
%
    \inferrule{~}{\Gamma \vdash \pi_2\, (e_1, e_2) \equiv e_2 : \subst x {e_1} Y}
%
    \and
%
    \inferrule{~}{\Gamma \vdash e \equiv (\pi_1\, e, \pi_2\, e) : \SigTy x X Y}
%
    \and
%
    \inferrule{~}{}
%
    \inferrule{~}{\Gamma \vdash t \equiv t' : 1}
%
    \and
%
    \inferrule{~}{\Gamma \vdash G\, (G^{-1} \, t) \equiv t : G A}

    \\

    \boxed{\Gamma ; \Delta \vdash a \equiv a' : A}

    \inferrule{~}{\Gamma; \cdot \vdash G^{-1}\, (G \, t ) \equiv t: A}
%
    \\
%
    \inferrule{~}{\Gamma; \Delta \vdash \app {(\lamblto a e)} {e'} \equiv \subst e x {e'} : C}
%
    \and
%
    \inferrule{~}{\Gamma; \Delta \vdash e \equiv \lamblto a {\app e a} : A \lto B}
%
    \and
%
    \inferrule{~}{\Gamma; \Delta \vdash \app {(\lambtol a e)} {e'} \equiv \subst e x {e'} : C}
%
    \and
%
    \inferrule{~}{\Gamma; \Delta \vdash e \equiv \lambtol a {\app e a} : A \tol B}
%
    \and
%
    \inferrule{~}{\Gamma; \Delta \vdash \app {(\dlamb x e)} {e'} \equiv \subst x a {e'} : C}
%
    \and
%
    \inferrule{~}{\Gamma; \Delta \vdash e \equiv \dlamb x {\app e x} : \LinPiTy x X A}
%
    \and
%
    \inferrule{~}{\Gamma; \Delta \vdash e \equiv e' : \top}
%
    \and
%
    \inferrule{~}{\Gamma; \Delta \vdash e_i \equiv \pi_i (e_1, e_2) : A_i}
%
    \and
%
    \inferrule{~}{\Gamma; \Delta \vdash e \equiv (\pi_1 e, \pi_2 e) : A\& B}
%
    \and
%
    \inferrule{~}{\Gamma; \Delta \vdash \letin {()} {()} e \equiv e : C}
    %
    \and
    %
    \inferrule{~}{\Gamma; \Delta \vdash \letin {()} e {\subst {e'} {()} a} \equiv \subst {e'} a e : C}
%
    \and
%
    \inferrule{~}{\Gamma; \Delta \vdash \letin {e \otimes e'} {a \otimes a'} e'' \equiv \subst {e''} {a, a'} {e, e'} : C}
%
    \and
    %
    \inferrule{~}{\Gamma; \Delta \vdash \letin {a \otimes b} e {\subst {e'} {a \otimes b} c} \equiv \subst {e'} c e : C}
%
    \and
%
    \inferrule{~}{\Gamma;\Delta \vdash \letin {(x, a)} {(e, e')} {e''} \equiv \subst {e''} {x, a} {e, e'} : C}
    %
    \and
    %
    \inferrule{~}{\Gamma; \Delta \vdash \letin {(x, a)} e {\subst {e'} {(x, a)} y} \equiv \subst {e'} e y : C}
\end{mathpar}
  \caption{Judgmental equality}
  \label{fig:jdgeq}
\end{figure}

\subsection{Formal Grammars as Linear Types}
\label{sec:formaltype}

The linear typing judgment in our syntax takes on the following schematic form
$\Gamma ; \Delta \vdash a : A$. First, $A$ represents a \emph{linear type} in
our syntax. The intended semantics of these linear types are formal grammars.
That is, the linear typing system is designed to syntactically reflect the
behavior of formal grammars. For this reason, we may often interchangeably use
the terms ``linear type'' and ``grammar''.

The term $a$ is an inhabitant of type $A$, which is thought of as a parse tree of
the grammar $A$. The core idea of this entire paper follows precisely from this
single correspondence: grammars are types, and the inhabitants of these types
are parse trees for the grammars.

$\Gamma$ represents a non-linear context, while $\Delta$ represents a
\emph{linear context} dependent on $\Gamma$. These linear contexts behave
substructurally. As stated earlier, they are linear --- they do not obey
weakening or contraction --- because a character is exhausted once read by a
parsing procedure. Moreover, the characters in strings appear in the order in
which we read them. We do not have the freedom to freely permute characters,
therefore any type theory that is used to reason about formal grammars ought to
omit the structural rule of exchange as well. This means that every variable
within $\Delta$ must be used \emph{exactly once} and \emph{in order of
  occurrence}. Thus, we can think of the linear contexts as an ordered list of
limited resources. Once a resource is consumed, we cannot make reference to it
again. Variables in a linear context then act like building
blocks for constructing patterns over strings.

We give the base types and type constructors for linear terms. As the
interpretation of types as grammars in $\Set^{\String}$ serves as our intended
semantics, we simultaneously give the interpretation $\sem \cdot$ of the
semantics as grammars.

\pedro{The paragraphs above are a bit circular, what about the following alternative?}

Now that we have specified the syntactic aspect of \theoryabbv, we must justify
its connections to formal grammars in order to show that it is a sound methodology
for reasoning about parsers. We do this by showing that $\mathbf{Gr}$ is a model
to our type theory.

Most of the non-linear semantics is standard \pedro{We should either cite something or add the non-linear semantics in the appendix}, so let us investigate in more
depth the linear semantics.

\begin{enumerate}
  \item A non-linear context $\Gamma$ denotes a set $\sem \Gamma$
  \item A non-linear type $\Gamma \vdash X : U_{i}$ denotes a family of sets
        $\sem X : \sem \Gamma \to \Set_{i}$
  \item A non-linear term $\Gamma \vdash e : X$ denotes a section
        $\sem e : \Pi(\gamma : \sem \Gamma)\sem{X} \gamma$ \pedro{we have not defined what sections are}
  \item Linear contexts $\Gamma \vdash \Delta$ and types
        $\Gamma \vdash A : L_{i}$ both denote families of grammars
        $\sem \Gamma \to \Gr_{i}$
  \item A linear term $\Gamma ; \Delta \vdash M : A$ denotes a family of parse
        transformers
        $\sem M : \Pi(\gamma : \sem \Gamma)\Pi(w : \String) \sem \Delta \gamma w \Rightarrow \sem M \gamma w$
\end{enumerate}

\paragraph{Linear Unit}
First, the linear unit $I$ may be built in the empty linear
context\footnote{When appropriate, statements like ``the empty linear context''
  may be taken polymorphically over all non-linear contexts that they might
  depend on.} $\cdot \dashv I$. $I$ serves as the unit for the operation
$\otimes$ described below.

As a grammar,
%
\[
  \sem {I} \gamma w = \{ () ~|~ w = \varepsilon \}
\]
%
That is, $I$ maps strings to the proposition that they are the empty string. $I$
is clearly only inhabited by the empty string $\varepsilon$, for which the
outputted set contains a single parse tree.


\paragraph{Base Types}
For each character $c$ in the alphabet $\Sigma$, we include a base type at the
lowest universe level $c : L_{0}$.

The grammar interpretation for characters is quite similar to that of $I$.
%
\[
  \sem {c} \gamma w = \{ () ~|~ w = c \}
\]
%
The grammar for the character $c$ likewise maps strings $w$ to the proposition that
they are equal to $c$.

\paragraph{Tensor Product}
The tensor product of two linear types is the first place where the ordering on
contexts really takes effect. That is, the tensor product of two types is
non-commutative.
When context $\Delta$ forms a linear term of
type $A$, and $\Delta'$ forms a linear term of type $B$, then the context
extension $\Delta, \Delta'$ forms a linear term of type $A \otimes B$.

As stated above, the type $I$ serves as the unit for $\otimes$. That is, for all
linear types $A$, the following equalities hold:
%
\[ I \otimes A \equiv A \otimes I \equiv A \]
%
In the grammar semantics,
%
\[
  \sem {A \otimes B} = \Sigma_{w_{1}, w_{2} : \String} (w_{1}w_{2} = w) \land \left(  \sem {A} \gamma w_{1} \times \sem {B} \gamma w_{2} \right)
\]
%
The string $w$ matches $A \otimes B$
precisely when it may be split into two
pieces such that the left one matches $A$
and the right one matches $B$.

\paragraph{Linear Function Types}
The monoidal structure provided by $\otimes$ is both left and right closed and
this is denoted by the left and right linear function types. Further,
for linear types $A$ and $B$ the left linear function type $A \lto B$
enjoys an elimination principle similar to function application or modus ponens.
%
\[
\inferrule{\Delta \vdash A \otimes A \lto B}{\Delta \vdash B}
\]
%
The interpretation of linear function types at a string $w$ is a linear function
that takes in parses of $A$ on a string $w_{a}$ and outputs parses of $B$ on the
string $w_{a} w$.
%
\[
  \sem{A \lto B} \gamma w = \Pi(w_a:\String) \left( A \gamma w_a \Rightarrow B\gamma (w_a w) \right)
\]
%
That is, strings match $A \lto B$ if when prepended with a parse of $A$ they
complete to parses of $B$. In this manner, the linear function types generalize
Brzozowksi's notion of derivative
\cite{brzozowskiDerivativesRegularExpressions1964}.
Brzozowski initially only gave an accounting of this operation for
generalized regular expressions, but later work from Might et al.\ demonstrates that the same
construction can be generalized to context free grammars
\cite{mightParsingDerivativesFunctional2011}. Here, via the linear function
types, the same notion of derivative extends to the grammars of \theoryabbv.
Note, to ensure that the linear function types do indeed generalize Brzozowski
derivative, we must include the equalities in \cref{fig:brzozowskideriv} as axioms\max{we must? why must we}.
\pedro{We should further explain why these equations are required, or at the very
least give some intuition}

Of course, all the above statements for the left function type also have
corresponding analogues for the
right-handed counterpart.

\begin{figure}
\begin{align*}
  c\lto c &\equiv I\\
  c\lto d &\equiv \bot\\
  c\lto I &\equiv \bot\\
  c\lto 0 &\equiv \bot\\
  c\lto (A \otimes B) & \equiv (c\lto A) \otimes B + (A \amp I) \otimes (c\lto B)\\
  c\lto A^* &\equiv (A \amp I)^{*} \otimes (c \lto A) \otimes A^* \\
  c\lto (A + B) &\equiv (c\lto A) + (c\lto B)
\end{align*}
\caption{Equality for Brzozowski Derivatives}
\label{fig:brzozowskideriv}
\end{figure}

\paragraph{LNL Dependent Types}
Given a non-linear type $X$, we may form both the dependent product type
$\LinPiTy x X A$ and the dependent pair type $\LinSigTy x X A$ as linear types
where $x$ is free in $A$.

$\sem{\LinPiTy x X A} \gamma w = \Pi(x:\sem{X}\gamma) \sem{A}(\gamma,x) w$

$\sem{\LinSigTy x X A} \gamma w = \Sigma(x:\sem{X}\gamma) \sem{A}(\gamma,x) w$

The grammar semantics of the linear product type is
indeed a dependent function out of the
semantics of $X$. Likewise, the grammar semantics of the linear dependent pair type is a dependent
pair in $\Set$.\pedro{The connection to grammar semantics needs to be better explained. For instance,
we should explain that disjunction corresponds to ``or'' of grammars}

Note that even though we do not take the binary additive conjunction and
disjunction as primitive, we may define them via these LNL dependent types. In
particular, via a dependence on $\Bool$.
%
\[
  A \amp B := \LinPiTy b \Bool {C(b)}
\]
\[
  A \oplus B := \LinSigTy b \Bool {C(b)},
\]
where $C(\true) = A, C(\false) = B$.

\paragraph{Universal Type}
The universal type $\top$ may be formed in any context.
%
\[ \sem{\top} \gamma w = \{ \ast \}\]
%
Its grammar semantics in set outputs the unit type in $\Set$ for all input strings in all contexts.

\paragraph{Empty Type}
The empty type $\bot$ has no inhabitants. The elimination for the empty type
witnesses the principle of explosion --- i.e.\ from a term of type $\bot$ we may
introduce a term $\mathsf{absurd}_{A} : A$ for any type $A$.
%
\[ \sem {\bot} \gamma w = \emptyset \]
%
\paragraph{The $G$ Modality}
The $G$ modality provides the embedding of linear types in the empty context
into non-linear types. The introduction and elimination forms for $G A$ obey the
same rules as given in \cite{bentonMixedLinearNonlinear1995} and \cite{krishnaswami_integrating_2015}.

The left adjoint $F$ from non-linear types back to linear types may be defined
as,
%
\[
  F X := \LinSigTy a A I
\]
%
\steven{Elaborate on $G$ more}

\pedro{Agreed :) maybe mention connections to persistent propositions from the
separation logic literature}

The semantics of $G A$ are exactly the semantics of $A$ at the empty word $\varepsilon$.
%
\[ \sem{G A} \gamma = \sem{A} \gamma \varepsilon \]
%
%
\paragraph{Recursive Types}
Recursive linear types may be defined via the least-fixed point operator $\mu$.
The grammar semantics of which are the fixed-point of sets induced by the
grammar semantics of $A$.
%
\[ \sem{\mu x. A} \gamma = \mu (x:\Gr_i). \sem{A}(\gamma,x) \]
%
\pedro{We should justify, even if briefly, the existence of this
  fixed-point}

Observe that we need not take the Kleene star as a primitive
grammar constructor, as it is definable as a fixed point.
The Kleene star of a grammar $g$ is given as,

\[
  g^{*} := \mu X . I \oplus (g \otimes X)
\]

\begin{figure}[h!]
\begin{mathpar}
  \inferrule
  {\Gamma ; \Delta \vdash p : I}
  {\Gamma ; \Delta \vdash \mathsf{nil} : g^{*}}

  \and

  \inferrule
  {\Gamma ; \Delta \vdash p : g \\ \Gamma ; \Delta' \vdash q
  : g^{*}}
  {\Gamma ; \Delta \vdash \mathsf{cons}(p , q) : g^{*}}

  \and

  \inferrule
  {
    \Gamma ; \Delta \vdash p : g^{*} \\
    \Gamma ; \cdot \vdash p_{\varepsilon} : h \\
    \Gamma ; x : g , y : h \vdash p_{\ast} : h
  }
  {\Gamma ; \Delta \vdash \mathsf{foldr}(p_{\varepsilon} , p_{\ast}) : g^{*}}
\end{mathpar}
\caption{Kleene Star Rules}
\label{fig:star}
\end{figure}

Likewise, $g^{*}$ has admissible introduction and
elimination rules, shown in \cref{fig:star}. Note that this
definition of $g^{*}$ and these
rules arbitrarily assigns a handedness to the Kleene star.
We could have just as well took it to be a fixed point of
$I \oplus (X \otimes g)$. In fact, the definitions are
equivalent, as the existence of the $\mathsf{foldl}$ term below
shows that $g^{*}$ is also a fixed point of
$I \oplus (X \otimes g)$.

\pedro{This is an extremely important point! We should definitely make
  a reference to the Kleene algebra world, where these things do not
  hold. I think that it might even be a good idea to add the
  ``underlying trick'' to the main body of text. It's important to
  connect our work to existing ideas and concerns from the theory of
  formal languages.}

\begin{equation}
  \label{eq:foldl}
  \inferrule
  {
    \Gamma ; \Delta \vdash p : g^{*} \\
    \Gamma ; \cdot \vdash p_{\varepsilon} : h \\
    \Gamma ; y : h , x : h \vdash p_{\ast} : h
  }
  {\Gamma ; \Delta \vdash \mathsf{foldl}(p_{\varepsilon} , p_{\ast}) : g^{*}}
\end{equation}

In fact, the $\mathsf{foldl}$ term is defined using
$\mathsf{foldr}$ --- much in the same way one
may define a left fold over lists in terms of a right fold
in a functional programming language\footnote{The
  underlying trick is to fold over a list of functions
  instead of the original string. We curry each character $c$
  of the string into a function that concatenates $c$, and
  right fold over this list of linear functions. Because function
  application is left-associative, this results in a left
  fold over the original string. }.

We only take fixed points of a single variable as a
primitive operation in the type theory, but we may apply
Beki\`c's theorem \cite{Bekić1984} to define an admissible
notion of multivariate fixed point. This is particularly
useful for defining grammars that encode the states of an
automaton, as we will see in \cref{sec:automata}. In \cref{fig:multifix} we provide the
introduction and elimination principles for such a fixed
point, where $\sigma$ is the substitution that unrolls the
mutually recursive definitions one level. That is,

\begin{align*}
  \sigma = \{ & \mu(X_{1} = A_{1} \dots, X_{n} = A_{n}).X_{1} / X_{1} , \dots, \mu(X_{1} = A_{1}, \dots, X_{n} = A_{n}).X_{n} / X_{n} \}
\end{align*}

\begin{figure}
\begin{mathpar}
  \inferrule
  {\Gamma ; \Delta \vdash e : \simulsubst {A_{k}} {\sigma}}
  {\Gamma ; \Delta \vdash \mathsf{cons}~e : \mu(X_{1} = A_{1}, \dots, X_{n} = A_{n}).X_{k}}

  \and

  \inferrule
  {\Gamma ; \Delta \vdash e : \mu(X_{1} = A_{1}, \dots, X_{n} = A_{n}).X_{k} \\
             \Gamma ; x_{j} : \simulsubst {A_{j}}{\sigma} \vdash e_{j} : B_{j}\quad \forall j
  }
  {\Gamma; \Delta \vdash \mathsf{mfold}(x_{1}.e_{1}, \dots, x_{n}.e_{n})(e) : B_{k}}
\end{mathpar}
\caption{Multi-fixed Points}
\label{fig:multifix}
\end{figure}

\subsection{Subtheories}
Each choice of connective has direct implications on the expressivity of
\theoryabbv. In particular, the set of connectives used will determine where the
grammar semantics is placed in
the Chomsky hierarchy.

\paragraph{Regular Expressions} We may realize regular expression as a subtheory
of our language by restricting the type constructors to the
linear unit, characters, $\otimes$, $\oplus$, and Kleene star. Classically, the
languages recognized by these are exactly the regular languages --- the lowest
on the Chomksy hierarchy.

\paragraph{$\mu$-Regular Expressions} Similarly, we may restrict the connectives
to be linear unit, characters, $\otimes$, $\oplus$, and arbitrary recursion via
left-fixed point rather than only Kleene star.
Instead of regular expressions these correspond to Lei{\ss}'s $\mu$-regular
expressions, which are known to be equivalent to context-free grammars
\cite{leiss,krishnaswami_typed_2019}.

\paragraph{Beyond Context-Free Grammars} The previous two subtheories induce as semantics
regular grammars and context-free grammars, respectively; however, by including
the LNL dependent types we may actually express the entirety of the Chomsky
hierarchy. Through use of the LNL dependent types, we may encode indexed
grammars, which are known to be properly between context-free and
context-sensitive grammars within the Chomsky hierarchy \cite{AhoIndexed}. We
will further see in \cref{subsubsec:tm} how to use dependence to encode Turing machines internal to our
calculus, which of course induces a semantics of unrestricted grammars.

\section{Categorifying Formal Grammars}
\label{sec:categorify}


An important part in the study of type theories is characterizing what
categorical structures they are modeling. In the case of \theoryabbv,
due to its intended connection to formal language theory, we use
Kleene algebras as a starting point in its categorical
semantics. Being posets, there is an immediate way of categorifying
them.

Kleene algebras are an important tool in the theory of regular languages
due to them being a bridge between algebraic reasoning and equivalence
of regular expressions. More broadly, through various extensions, they serve as a
theoretical substrate to studying different kinds of formal languages.

We posit that by seeing Kleene algebra through the lens of category
theory, we gain some insight into the type-theoretic essence of formal
grammars. We begin this section by providing a ``simply-typed''
version of a categorification of Kleene algebra, which we call Kleene
category, and then show define a more expressive class of categories,
which we call Chomsky category, that will serve as the categorical
semantics to \theoryabbv.

\subsection{Kleene Categories}
A Kleene algebra is a tuple $(A, +, \cdot, (-)^*, 1, 0)$, where $A$ is
a set, $+$ and $\cdot$ are binary operations over $A$, $(-)^*$ is a
function over $A$, and $1$ and $0$ are constants. These structures
satisfy the axioms depicted in Figure~\ref{fig:axioms}.

\begin{figure}
  \begin{align*}
    x + (y + z) &= (x + y) + z & x + y &= y + x\\
    x + 0 &= x & x + x &= x\\
    x(yz) &= (xy)z & x1 &= 1x = x\\
    x(y + z) &= xy + xz & (x + y)z &= xz + yz\\
    x0 &= 0x = x & & \\
    1 + aa^* &\leq a^* & 1 + a^*a &\leq a^*\\
     b + ax \leq x &\implies a^*b \leq x &  b + xa \leq x &\implies ba^* \leq x
  \end{align*}
  \caption{Kleene algebra axioms}
  \label{fig:axioms}
\end{figure}

The addition operation can be used to define the partial order
structure $a \leq b$ if, and only if, $a + b = b$. In the theory of
formal languages, this order structure can be used to model language
containment. In this section, we categorify the concept of Kleene
algebra and build on top of it in order to define an abstract theory
of parsing. We start by defining \emph{Kleene categories}.

\begin{definition}
  A Kleene category is a distributive monoidal category $\cat{K}$
  such that for every objects $A$ and $B$, the endofunctors $F_{A, B}
  = B + A \otimes X$ and $G_{A, B} = B + X \otimes A$ have initial
  algebras (denoted $\mu X.\, F_{A, B}(X)$) such that $B \otimes (\mu
  X.\, F_{A, 1}) \cong \mu X.\, F_{A, B}(X)$ and the analogous isomorphism
  for $G_{A,B}$ also holds.
\end{definition}

As a sanity check, note that Kleene algebras are indeed examples of
Kleene categories.

\begin{example}
  Every Kleene algebra, seen a posetal category, is a Kleene category.
  The product $\cdot$ is a monoidal product and the addition is a
  least-upper bound, which corresponds to a coproduct. Lastly, the
  axioms of the Kleene star have a direct correspondence to the
  coherence conditions postulated by the initial algebras of Kleene
  categories.
\end{example}

This example provides a neat categorical justification to how
restrictive Kleene algebras are in terms of reasoning about
languages. By only having at most one morphism between objects, there
is not a lot of information they can convey. In this case, the only
information you get is language containment. As demonstrated by
$\mathbf{Gr}$, the extra degrees of freedom granted by having more
morphisms gives you more algebraic structure for reasoning about
languages.

For the next example, we see an unexpected connection with the theory
of substructural logics.

\begin{example}
  The opposite category of every Kleene category is a model of a variant of
  conjunctive ordered logic, where the Kleene star plays the role of the ``of
  course'' modality from substructural logics which allows hypotheses to
  be discarded or duplicated.
\end{example}

As we have seen, the proposed axioms are a direct translation of the
Kleene algebra axioms to a categorical setting. Its most unusual aspect is the
axiomatization of the Kleene star as a family of initial algebras
satisfying certain isomorphisms. If the Kleene category $\cat{K}$ has
more structure, then these isomorphisms hold ``for free''.

\begin{theorem}
  \label{th:kleeneclosed}
  Let $\cat{K}$ be a Kleene category such that it is also monoidal
  closed.  Then, the initial algebras isomorphisms hold automatically.
\end{theorem}
\begin{proof}
  We prove this by the unicity (up-to isomorphism) of initial
  algebras. Let $[hd, tl]: I + (\mu X.\, F_{A, I}(X)) \otimes A \to
  (\mu X.\, F_{A, I}(X))$ be the initial algebra structure of $(\mu
  X.\, F_{A, I}(X))$ and consider the map $[hd, tl] : B + B \otimes
  (\mu X.\, F_{A, I}(X)) \otimes A \to B\otimes (\mu X.\, F_{A,
    I}(X))$.

  Now, let $[f,g] : B + A \otimes Y \to Y$ be an $F_{A,B}$-algebra and
  we want to show that there is a unique algebra morphism $h : \mu X.\, F_{A,I} \to B \lto Y$. We can show existence and
  uniqueness by showing that the diagram on top commutes if, and
  only if, the diagram on the bottom commutes:

% https://q.uiver.app/#q=WzAsOCxbMCwwLCJCICsgQiBcXG90aW1lcyAoXFxtdSBYLlxcLCAxICsgWCBcXG90aW1lcyBBKSJdLFsyLDAsIkIgKyBZIFxcb3RpbWVzIEEiXSxbMiwyLCJZIl0sWzAsMiwiQiBcXG90aW1lcyAoXFxtdSBYLlxcLCAxICsgWCBcXG90aW1lcyBBKSJdLFswLDMsIjEgKyAoXFxtdSBYLlxcLCAxICsgWCBcXG90aW1lcyBBKSJdLFswLDUsIlxcbXUgWC5cXCwgMSArIFggXFxvdGltZXMgQSJdLFsyLDMsIjEgKyAoIEIgXFxsdG8gWSkgXFxvdGltZXMgQSJdLFsyLDUsIkIgXFxsdG8gWSJdLFswLDEsImlkICsgKGlkIFxcb3RpbWVzIGg7IGV2KSBcXG90aW1lcyBpZF9BIl0sWzEsMiwiW2YsZ10iXSxbMywyLCJpZCBcXG90aW1lcyBoOyBldiIsMl0sWzAsMywiW2lkIFxcb3RpbWVzIGgsIGlkIFxcb3RpbWVzIHRsXSIsMl0sWzQsNiwiaWQgKyAoaCBcXG90aW1lcyBYKVxcb3RpbWVzIGlkIl0sWzUsNywiaCIsMl0sWzYsNywiW2YnLCBnJ10iXSxbNCw1LCJbaGQsIHRsXSIsMl1d
\[\begin{tikzcd}
	{B + B \otimes (\mu X.\, I + X \otimes A)} && {B + Y \otimes A} \\
	\\
	{B \otimes (\mu X.\, I + X \otimes A)} && Y \\
	{I + (\mu X.\, I + X \otimes A)} && {I + ( B \lto Y) \otimes A} \\
	\\
	{\mu X.\, I + X \otimes A} && {B \lto Y}
	\arrow["{id + (id \otimes h; ev) \otimes id_A}", from=1-1, to=1-3]
	\arrow["{[f,g]}", from=1-3, to=3-3]
	\arrow["{id \otimes h; ev}"', from=3-1, to=3-3]
	\arrow["{[id \otimes h, id \otimes tl]}"', from=1-1, to=3-1]
	\arrow["{id + (h \otimes X)\otimes id}", from=4-1, to=4-3]
	\arrow["h"', from=6-1, to=6-3]
	\arrow["{[f', g']}", from=4-3, to=6-3]
	\arrow["{[hd, tl]}"', from=4-1, to=6-1]
\end{tikzcd}\]

  This equivalence follows by using the adjunction structure given
  by the monoidal closed structure of $\cat{K}$. A completely analogous
  argument for $G_{A,B}$ also holds. Furthermore, by generalizing the
  construction of \Cref{sec:formaltype}, we can also show that from
  the monoidal closed assumption it follows that $\mu X.\, F_{A, I}(X) \cong \mu X.\, G_{A, I}(X)$
\end{proof}

Something surprising about this lemma is that it provides an alternative
perspective on the observation that if a Kleene algebra has an
residuation operation, also called action algebra \cite{kozen1994action},
then the Kleene star admits a simpler axiomatization.

Since we want Kleene categories to generalize our notion of formal
grammars as presheaves $\String \to \Set$, we prove that they do
indeed form a Kleene category. We start by presenting a well-known
construction from presheaf categories.

\begin{definition}
  Let $\cat{C}$ be a locally small monoidal category and $F$, $G$ be
  two functors $\cat{C} \to \Set$. Their Day convolution tensor
  product is defined as the following coend formula:
  \[
  (F \otimes_{Day} G)(x) = \int^{(y,z) \in \cat{C}\times\cat{C}}\cat{C}(y\otimes z, x) \times F(y) \times G(z)
  \]
  Dually, its internal hom is given by the following end formula:
  \[
  (F \lto_{Day} G)(x) = \int_{y} \Set(F(y), G(x \otimes y))
  \]
\end{definition}

\begin{lemma}[Day \cite{day1970construction}]
  Under the assumptions above, the presheaf category $\Set^{\cat{C}}$ is
  monoidal closed.
\end{lemma}

%% \begin{theorem}
%%   Let $\cat{K}$ be a Kleene category and $A$ a discrete category.
%%   The functor category $[A, \cat{K}]$.
%%   (HOW GENERAL SHOULD THIS THEOREM BE? BY ASSUMING ENOUGH STRUCTURE,
%%   E.G. K = Set, THIS THEOREM BECOMES SIMPLE TO PROVE)
%% \end{theorem}
\begin{theorem}
  If $\cat{C}$ is a locally small monoidal category, then
  $\Set^{\cat{C}}$ is a Kleene category.
\end{theorem}
\begin{proof}

  By the lemma above, $\Set^{\cat{C}}$ is monoidal closed, and since it
  is a presheaf category, it has coproducts. Furthermore, the tensor
  is a left adjoint, i.e. it preserves colimits and, therefore, it is
  a distributive category.

  As for the Kleene star, since presheaf categories admit small colimits,
  the initial algebra of the functors $F_{A,B}$ and $G_{A,B}$ can be
  defined as the filtered colimit of the diagrams:

  From Theorem~\ref{th:kleeneclosed} it follows that these initial
  algebras satisfy the required isomorphisms and this concludes the
  proof.
\end{proof}

\begin{corollary}
  For every alphabet $\Sigma$, the presheaf category $\Set^{\cat{\Sigma^*}}$
  is a Kleene category.
\end{corollary}
\begin{proof}
  Note that string concatenation and the empty string make the
  discrete category $\Sigma^*$ a strict monoidal category.
\end{proof}

Much like in the posetal case, the abstract structure of a Kleene
category is expressive enough to synthetically reason about formal
languages. A significant difference between them is that while Kleene
algebras can reason about language containment, Kleene categories can
reason about parsing as well, as illustrated by the presheaf model
above.

\subsection{Beyond Simple Types}

Though Kleene categories are expressive enough to reason about
concepts that are outside of reach of Kleene algebras, their
simply-typed nature makes them not so expressive from a type theoretic
point of view. This is limiting because type theories are sucessful
syntactic frameworks for manipulating complicated categorical
structures while avoiding some issues common in category theory, such
as coherence issues.

With this in mind, we want to design a categorical semantics that
builds on top of Kleene categories with the goal of extending them
with dependent types and making them capable reasoning about languages
and their parsers. This leads us to the abstract notion of model we
are interested in capturing with \theoryabbv: a \emph{Chomsky
category}.

\pedro{We should probably define some of the words in this definition}
\begin{definition}
  A Chomsky category is a locally Cartesian category with two hierarchies of
  universes $\{L_i\}_{i\in \nat}$ and $\{U_i\}_{i\in \nat}$ such that
  every $L_i$ and $U_i$ are $U_{i+1}$-small. Furthermore, we require
  $U_i$ to be closed under dependent products and sum,
  $L_i$ to be closed under the Kleene category connectives,
  dependent products, left and right closed structures, with
  a type constructor $G : L_i \to U_i$ and a linear dependent sum
  going the other direction.
\end{definition}

\steven{Max suggests augmenting the definition of a chomsky category to
  something like two categories $L$ and $U$, and $U$ is cartesian closed I don't
  fully recall the rest.


  My best guess is that you take $L$ a Kleene category with a hierarchy of universes
  two, and you further require that $L$ is $Psh(U)$-enriched, except he
  suggested a further adjective on these presheaves. Perhaps representability?
}

\begin{theorem}
  The presheaf category $\Grammar = \Set^{\cat{\Sigma^*}}$ is a Chomsky category.
\end{theorem}


Further, the syntactic category of \theoryname is manifestly a Chomsky category.

\pedro{This is likely true, but if we explicitly say so, this warants a proof. I think that
if we don't say anything about the syntactic category, reviewers won't mind.}

\steven{I agree that we don't want to say anything that opens unnecessary
  questions for proofs we haven't written. However, it seems hard to make the
  case that we have the right categorical model of the syntax if this statement
  isn't true. By restating our definition of Chomksy category, this should be
  obvious or a quick proof}

\pedro{I agree, then we should add the quick proof :) }

\section{Other Models}
\label{sec:othermodels}

One of the powers of type theories is that it can be profitable to interpret them
in various models. In this section, by using our just-defined Chomsky categories,
we show how other useful concepts from formal language theory can also be organized
as models of \theoryabbv. We illustrate this point by providing two examples that
are closely related to the theory of formal languages: language equivalence and
semantic actions. Furthermore, in order to justify how $\mathbf{Gr}$ relates to
more traditional notions of parsing, we define a glued model that proves a
canonicity property of grammar terms.


\subsection{Language Semantics}
Every grammar induces a language semantics. Also languages can be taken as a
propositionally truncated view of the syntax. Logical equivalence should induce
weak equivalence, and thus even give a syntactic way to reason about language equivalence.
\steven{TODO language semantics}

\subsection{Semantic Actions}
Returning to the problem of parsing, the output of a parse usually is not the
literal parse tree. Rather, the output is the result of some \emph{semantic
  actions} ran on the parse tree, which usually serve to remove some syntactic
details that are unnecessary for later processing.

Given a grammar $G : \String \to \Set$, we define a semantic action to be a set
$X$ with a function $f$ that produces a semantic element from any parse of $G$.

\[
  f : \PiTy w \String {G w \to X}
\]

Further, semantic actions can be arranged into a structured category.
Define $\SemAct$ as the comma
category $\Grammar / \Delta$, where $\Delta : \Set \to \Grammar$ defines a
discrete presheaf. That is, for a set $X$, $\Delta (X)(w) = X$ for all
$w \in \String$. As $\SemAct$ is defined as a comma category, it has a forgetul
functor into $\Grammar$. That is, $\SemAct$ serves as a notion of formal
grammar. Moreover, $\SemAct$ is a model of \theoryabbv.

\steven{It being a notion of formal grammar is distinct from being a model. This
probably warrants a proof}

\steven{TODO semantic actions}

\pedro{This is a very nice opportunity of showing off the supremacy of denotational
  reasoning ;) We should probably prove the gluing lemma in the previous section
  and apply it here and in the canonicity section. The actual proof might have to be
  moved to the appendix, though}

\subsection{Parse Canonicity}
Canonicity is an important metatheoretic theorem in the type theory
literature.  It provides insight on the normal forms of terms and,
therefore, on its computational aspects. Frequently, proving
canonicity for boolean types, i.e. every closed term of type bool
reduces to either true or false, is enough to justify that the type
theory being studied is well-behaved. In our case, however, since we
want to connect \theoryabbv to parsers, we must provide a more
detailed account of canonicity. In particular, we give a nonstandard semantics
of \theoryabbv that carries a proof of canonicity along with it.

If $\cdot \vdash A$ is a closed linear type then there are
two obvious notions of what constitutes a ``parse'' of a string w
according to the grammar $A$:
\begin{enumerate}
\item On the one hand we have the set-theoretic semantics just
  defined, $\llbracket A \rrbracket \cdot w$
\item On the other hand, we can view the string $w = c_1c_2\cdots$ as
  a linear context $\lceil w \rceil = x_1:c_1,x_2:c_2,\ldots$ and
  define a parse to be a $\beta\eta$-equivalence class of linear terms $\cdot;
  \lceil w \rceil \vdash e : G$.
\end{enumerate}
It is not difficult to see that at least for the ``purely positive''
formulae (those featuring only the positive connectives
$0,+,I,\otimes,\mu, \overline\Sigma,c$) that
every element $t \in \llbracket A \rrbracket w$ is a kind of tree and
that the nodes of the tree correspond precisely to the introduction
forms of the type. However it is far less obvious that \emph{every}
linear term $\lceil w \rceil \vdash p : \phi$ is equal to some
sequence of introduction forms since proofs can include elimination
forms as well. To show that this is indeed the case we give a
\emph{canonicity} result for the calculus: that the parses for .

\begin{definition}
  A non-linear type $X$ is purely positive if it is built up using
  only finite sums, finite products and least fixed points.

  A linear type is purely positive if it is built up using only finite
  sums, tensor products, generators $c$, least fixed points and linear
  sigma types over purely positive non-linear types.
\end{definition}

\begin{definition}
  %% Let $X$ be a closed non-linear type. The closed elements $\textrm{Cl}(X)$ of $X$ are the definitional equivalence classes of terms $\cdot \vdash e : X$.

  Let $A$ be a closed linear type. The nerve $N(A)$ is a presheaf on
  strings that takes a string $w$ to the definitional equivalence
  classes of terms $\cdot; \lceil w\rceil \vdash e: N(A)$.
\end{definition}

\begin{theorem}[Canonicity]
  Let $A$ be a closed, purely positive linear type. Then there is an
  isomorphism between $\llbracket A\rrbracket$ and $N(A)$.
\end{theorem}
\begin{proof}
  We outline the proof here, more details are in the appendix. The
  proof proceeds first by a standard logical families construction
  that combines canonicity arguments for dependent type theory
  TODO cite coquand
  % \cite{coquand,etc}
  with logical relations constructions for linear
  types
  TODO cite hylandschalk
  % \cite{hylandschalk}
  . It is easy to see by induction that the
  logical family for $A$, $\hat A$ is isomorphic to $\llbracket A
  \rrbracket$ and the fundamental lemma proves that the projection
  morphism $p : \hat A \to N(A)$ has a section, the canonicalization
  procedure. Then we establish again by induction that
  canonicalization is also a retraction by observing that introduction
  forms are taken to constructors.
\end{proof}


\begin{enumerate}
\item Every term $\lceil w \rceil \vdash p : G + H$ is equal to $\sigma_1q$ or $\sigma_2 r$ (but not both)
\item There are no terms $\lceil w \rceil \vdash p : 0$
\item If there is a term $\lceil w \rceil \vdash p : c$ then $w = c$ and $p = x$.
\item Every term $\lceil w \rceil \vdash p : G \otimes H$ is equal to $(q,r)$ for some $q,r$
\item Every term $\lceil w \rceil \vdash p : \epsilon$ is equal to $()$
\item Every term $\lceil w \rceil \vdash p : c$ is equal to $x:c$
\item Every term $\lceil w \rceil \vdash p : \mu X. G$ is equal to $\textrm{roll}(q)$ where $q : G(\mu X.G/X)$
\item Every term $\lceil w \rceil \vdash p : (x:A) \times G$ is equal
  to $(M,q)$ where $\cdot \vdash M : A$
\end{enumerate}

To prove this result we will use a logical families model. We give a
brief overview of this model concretely:
\begin{enumerate}
\item A context $\Gamma$ denotes a family of sets indexed by closing substitutions $\hat\Gamma : (\cdot \vdash \Gamma) \Rightarrow \Set_i$
\item A type $\Gamma \vdash X : U_i$ denotes a family of sets $\hat X : \Pi(\gamma:\cdot \vdash \Gamma) \hat\Gamma \Rightarrow (\cdot \vdash \simulsubst X \gamma) \Rightarrow \Set_i$
\item A term $\Gamma \vdash e : X$ denotes a section $\hat e : \Pi(\gamma)\Pi(\hat\gamma)\hat X \gamma \hat\gamma (\simulsubst e \gamma)$
\item A linear type $\Gamma \vdash A : L_i$ denotes a family of grammars $\hat A : \Pi(\gamma:\cdot\vdash\Gamma)\,\hat\Gamma \Rightarrow \Pi(w:\Sigma^*) (\cdot;\lceil w\rceil \vdash A[\gamma])\Rightarrow \Set_i$, and the denotation of a linear context $\Delta$ is similar.
\item A linear term $\Gamma;\Delta \vdash e : A$ denotes a function \[\hat e : \Pi(\gamma)\Pi(\hat\gamma)\Pi(w)\Pi(\delta : \lceil w \rceil \vdash \simulsubst \Delta \gamma) \hat\Delta \gamma \hat\gamma \delta \Rightarrow \hat A \gamma \hat\gamma w {(\simulsubst {\simulsubst e \gamma} \delta)}\]
\end{enumerate}
And some of the constructions:
\begin{enumerate}
\item $\widehat {(G A)} \gamma \hat\gamma e = \hat A \gamma \hat\gamma \varepsilon (G^{-1}e)$
\item $\widehat {(A \otimes B)} \gamma \hat\gamma w e = \Sigma(w_Aw_B = w)\Sigma(e_A)\Sigma(e_B) (e_A,e_B) = e \wedge \hat A \gamma \hat \gamma w_A e_A \times \hat B \gamma \hat \gamma w_B e_B$
\item $\widehat {(A \lto B)} \gamma \hat\gamma w e = \Pi(w_A)\Pi(e_A) \hat A \gamma\hat\gamma w_A e_A \Rightarrow \hat B \gamma\hat\gamma (ww_A) (\applto {e_A} e)$
\end{enumerate}

First, the category with families will be
the category of logical families over set contexts/types
$\Delta$/$A$. Then the propositional portion will be defined by
mapping a logical family $\hat \Gamma \to \Gamma$

First, let $L$ be the category of BI formulae and proofs (quotiented
by $\beta\eta$ equality). Define a functor $N : L \to \Set^{\Sigma^*}$ by
\[ N(\phi)(w) = L(w,\phi) \]

Then define the gluing category $\mathcal G$ as the comma category
$\Set^{\Sigma^*}/N$. That is, an object of this category is a pair of
a formula $\phi \in L$ and an object $S \in \mathcal
P(\Sigma^*)/N(\phi)$. We can then use the equivalence $\mathcal
P(\Sigma^*)/N(\phi) \cong \mathcal P(\int N(\phi))$ to get a simple
description of such an $S$: it is simply a family of sets indexed by
proofs $L(w,\phi)$:
\[ \prod_{w\in\Sigma^*} L(w,\phi) \to \Set \]
This category clearly comes with a projection functor $\pi : \mathcal
G \to \mathcal L$ and then our goal is to define a section by using
the universal property of $\mathcal L$.

To this end we define
\begin{enumerate}
\item $(\phi, S) \otimes (\psi, T) = (\phi \otimes \psi, S\otimes T)$ where
  \[ (S \otimes T)(w, p) = (w_1w_2 = w) \times (q_1,q_2 = p) \times S\,w_1\,q_1 \times T\,w_2\,q_2\]
\item $(\phi, S) \multimap (\psi, T) = (\phi \multimap \psi, S \multimap T)$ where
  \[ (S \multimap T)(w,p) = w' \to q \to S\,w'\,q \to T (ww') (p\,q) \]
\item $\mu X. ??$ ??
\end{enumerate}

\pedro{We should conclude this section by explaining the relevance of the canonicity
theorem. Could also be done before stating the theorem.}

\section{Automata as Grammars}
\label{sec:automata}

The canonicity theorem from the previous section gives a tight connection
between terms in our type theory and the theory of formal languages. In this section
we further explore these connections by using the fact that, clasically
formal language theory is closely related to the study of automata.
In the Chomsky hierarchy, each language class is associated to a class of
automata that serve as recognizers. Internal to \theoryabbv,
we can characterize these language classes syntactically; moreover, we
demonstrate the equivalence of these language classes to its associated automata class as a
proof term within our logic.

\subsection{Non-deterministic Finite Automata}
\label{subsec:finiteaut}
\begin{figure}
  \begin{tikzpicture}[node distance = 25mm ]
    \node[state, initial, accepting] (1) {$1$};
    \node[state, below left of=1] (2) {$2$};
    \node[state, right of=2] (3) {$3$};

    \path[->] (1) edge[above] node{$b$} (2)
              (1) edge[below, bend right, left=0.3] node{$\epsilon$} (3)
              (2) edge[loop left] node{$a$} (2)
              (2) edge[below] node{$a, b$} (3)
              (3) edge[above, bend right, right=0.3] node{$a$} (1);
  \end{tikzpicture}
  \caption{An example NFA}
  \label{fig:NFA}
\end{figure}

Classically, a \emph{nondeterministic finite automaton} (NFA) is a finite state machine where
transitions are labeled with characters from a fixed alphabet $\Sigma$. These
are often represented formally as a 5-tuple $(Q, \Sigma, \delta, q_{0}, F)$,

\begin{itemize}
  \item $Q$ a finite set of states
  \item $\Sigma$ a fixed, finite alphabet
  \item $\delta : Q \times (\Sigma \cup \{ \varepsilon\}) \to \mathcal{P}(Q)$ the labeled transition function
  \item $q_{0} \in Q$ the start state
  \item $F \subset Q$ a set of accepting states
\end{itemize}

Intuitively, this can be thought of like a directed graph with nodes in $Q$ with
an edge $q \overset{c}{\to} q'$ whenever $q' \in \delta(q, c)$. Note that
transitions in an NFA may be labeled with the empty string $\varepsilon$ --- such
transitions are referred to as \emph{$\varepsilon$-transitions}. We may see an
example of an NFA in \cref{fig:NFA}.

From an NFA, we may construct a grammar of traces as follows:

First, we define a mutual fixed point grammar that describes the traces through the NFA.\ Then, we have another
grammar that tells us if we're currently in an accepting state of the automaton.
A parse of the NFA grammar is then a pair of a trace and the data that we're in
an accepting state.

Consider the NFA $N$ pictured in \cref{fig:NFA}. There are three states, $1$, $2$,
and $3$. We introduce a non-linear type $Q$ with three inhabitants $q_{1}$,
$q_{2}$, and $q_{3}$ to represent each of these states, respectively. Given a
$q, q'$, we can
then define the type of traces from $q$ to $q'$. For instance, let's construct
the traces starting at $q_{1}$ and ending at $q_{2}$ as an example.

\begin{equation}
  \label{eq:nfatrace}
  \mathsf{Trace}_{N}(q_{1}, q_{2}) = \mu
  \begin{pmatrix}
    g_{q_{1}} := g_{q_{3}} \oplus ( b \otimes g_{q_{2}} ) \\
    g_{q_{2}} := ( a \otimes g_{q_{2}} ) \oplus ( a \otimes g_{q_{3}} ) \oplus ( b \otimes g_{q_{3}} ) \oplus I \\
    g_{q_{3}} := a \otimes g_{q_{1}} \\
  \end{pmatrix}. g_{q_{1}}
\end{equation}

We should read this as defining three mutually recursive grammars, one for each
state. The definitions of these mutually recursive grammars capture the
transitions of the automaton. To ensure that we only encode traces that end in
state 2, we only include the unit $I$ as a summand in
$g_{q_{2}}$. That is, by only including $I$ at this location, it makes it the
only place where derivations of $\mathsf{Trace}(q_{1}, q_{2})$ can terminate. We
can think of all of these definitions underneath
of the $\mu$ binder as bringing some local grammars into scope. With these local
grammars in scope we are ultimately constructing a term of type $g_{q_{1}}$ to denote
only traces starting in state 1.

To encode the acceptance criteria of $N$, we want to internalize a proposition
over each state of the NFA.\ That is, for each $q \in Q$ we define a term

\[
  \mathsf{acc}(q) := q \text{ is accepting}
\]

In this example, $\mathsf{acc}(q_{1}) = \top$ and
$\mathsf{acc}(q_{2}) = \mathsf{acc}(q_{3}) = \bot$. An accepting trace of the NFA $N$ is then given by the following dependent grammar,
\[
 \mathsf{AccTrace}_{N} := \LinSigTy q Q {\left( \mathsf{Trace}_{N}(q_{0} , q) \pair \mathsf{acc}(q) \right)}
\]

where $q_{0}$ is the initial state. That is, a trace is accepted by the NFA if
we can construct the trace, and the trace ends at an accepting state. This idea
is simple enough and aligns with how we intuitively treat these automata.

Generalizing over the above example, we want to define the type of traces as,

\[
  \mathsf{Trace}_{N}(q_{0}, q_{1}) = \mu
  \begin{pmatrix}
    g_{q} := \mathsf{Trans}(q), & q \neq q_{1} \\
    g_{q} := \mathsf{Trans}(q) \oplus I , & q = q_{1}
  \end{pmatrix}. g_{q_{0}}
\]

where $\mathsf{Trans}(q)$ is an iterated disjunction that describes which
transition you should take. For an NFA, $\mathsf{Trans}(q)$ must take on the
following syntactic form,

\begin{gather*}
 \mathsf{State} \in \{ g_{q} : q \in Q \} \\
 \mathsf{Char} \in \Sigma \\
 \mathsf{Trans}_{N}(q) ::= \mathsf{Char} \otimes \mathsf{State}~|~\mathsf{State}~|~\mathsf{Trans}_{N}(q) \oplus \mathsf{Trans}_{N}(q)
\end{gather*}

That is, $\mathsf{Trans}(q)$ is a disjunction of literals followed by grammars
that encode states.

When conducting proofs involving NFA grammars, we often to
need either construct terms of type $\mathsf{Trace}_{N}(q , q')$. To
this end, we give three admissible rules for constructing
traces,

\begin{figure}[h!]
  \label{fig:admissibleintro}
  \begin{mathpar}
    \inferrule
    {~}
    {\Gamma ; \cdot \vdash \mathsf{nil} : \mathsf{Trace}_N
      (q , q)}

    \and

    \inferrule
    {\Gamma ; \Delta \vdash M : \mathsf{Trace}_N
      (dst , q') \\
    \exists \text{ transition } src \overset{c}{\to} dst
    }
    {\Gamma ; x : c , \Delta \vdash \mathsf{cons}(M) : \mathsf{Trace}_N
    (src , q')}

    \and

    \inferrule
    {\Gamma ; \Delta \vdash M : \mathsf{Trace}_N (dst , q')
      \\
    \exists~\varepsilon\text{-transition } src
    \overset{\varepsilon}{\to} dst}
    {\Gamma ; \Delta \vdash \mathsf{\varepsilon cons}(M) : \mathsf{Trace}_N
    (src , q')}
  \end{mathpar}
  \caption{Admissible Trace Constructors}
\end{figure}


That is, we may use $\mathsf{nil}$ to terminate a trace that
begins and ends at state $q$. The rules $\mathsf{cons}$ and
$\mathsf{\varepsilon cons}$ are then used to inductively
stitch traces together when sound. For instance, we can read
the $\mathsf{cons}$ rule as saying that we can
create a trace coming from a state $src$ provided that we may first
transition via the character $c$ to state $dst$ and
inductively build a trace starting from $dst$. The
$\mathsf{\varepsilon cons}$ rule says something similar, but
with an $\varepsilon$-transition instead of a transition
labelled by a character. Recall that
these rules are \emph{admissible}. They are not strictly
necessary as primitives to conduct our proofs; however, they
do provide convenient shorthand notation for building terms
of type $\mathsf{Trace}_{N}(q , q')$.

Dual to constructing traces, we often want to construct
other terms in a context containing values of type
$\mathsf{Trace}_{N}(q , q')$. For this purpose, we make use
of the elimination principle for multiple-fixed points ---
which we write as $\mathsf{mfold}$ --- given
in \cref{fig:multifix}.

Note that this general construction will readily generalize to other types of
automata. If we wanted to define say deterministic finite automata\footnote{DFAs
\emph{could} just be defined as NFAs that happen to be deterministic. This is
one way to do so, or you may choose to present the transition relation for the
automaton as a transition \emph{function} instead. Concerns like these don't
matter so much when defining things on paper, but at formalization time these
are important and can make some proofs much easier.}, pushdown automata, Turing
machines, etc, we just swap out the type of traces for a different, but very
similar, trace construction.

With this setting for finite automata, we can now internalize some classical theorems inside of our formal system.

\subsection{A DFA Parser}
\label{subsec:regexparser}

Just as we encoded traces of NFAs as grammars, we likewise
encode the traces of a DFA as grammars. The key difference
between NFAs and DFAs is \emph{determinism} --- meaning,
that in a state $q$ inside of DFA, given a character $c$ there
will be exactly one transition that we may take leaving $q$
with label $c$. For us, this changes the definition of valid
transitions for a DFA, instead of the definition of
$\mathsf{Trans}$ provided in \cref{subsec:finiteaut} DFAs
obey

\begin{gather*}
 \mathsf{State} \in \{ g_{q} : q \in Q \} \\
 \mathsf{Trans}(q) ::= \bigoplus_{c \in \Sigma} (c \otimes \mathsf{State})
\end{gather*}

Meaning, each state has a transition for every character. \emph{Note:} the above use
  of a disjunction over $\Sigma$ is a bit of abuse of notation.
  As $\Sigma$ is a finite alphabet, we wish to think of this as an iterated
  binary sum, or perhaps as a finitely indexed sum defined via linear dependent
  sums in the same manner by which we defined binary sums.

We now wish to define a parser term for DFA grammars. In
particular, for a DFA $D$ we want to build a term,

\[
  w : \String \vdash \mathsf{parse}_{D} : \mathsf{AccTrace}_{D} \oplus \top
\]

where left injection into the output type denotes acceptance
by the parser, and right injection denotes rejection. To
build such a parser, it will suffice to construct a term

\[
  w : \String \vdash \mathsf{parse}_{D} : \LinSigTy q Q {\mathsf{Trace}_{D}(q_{0} , q)}
\]
This is because given a trace of a DFA, we may easily check
if we should accept or reject by simply testing
if the final state is accepting.

Because $w$ is a Kleene star of characters, we may construct
our desired $\mathsf{parse}_{D}$ term as a $\mathsf{foldl}$
over $w$. In the empty case, we just have the trace that
ends at the accepting state. In the recursive case, we
effectively add to our trace by transitioning one character
at a time, as we read them moving across $w$.

Perhaps this derivation is not too surprising. All it says
is that a DFA may be ran by transitioning a single character
at a time, and then accepting or rejecting based on the
final state. This is exactly what DFAs did initially, so
what did we gain? Well, this has the benefit of our type
system to ensure its correctness. Moreover, this construction exports to an
intrinsically verified and executable decision procedure for DFAs in \theoryabbv
embedded in Agda.

We should note that the
construction of a DFA parser requires the addition of an additional but seemingly innocuous
axiom.

\[
  \top \cong \left( \bigoplus_{c : \Sigma} c \right)^{*}
\]

That is, the characters in the alphabet $\Sigma$ really are all of the
characters. The necessity of this axiom arises when we go to take a transition
within the DFA,
as we must ensure that the nexts character read has a corresponding transition
--- which would be guaranteed by determinism provided there are no surprise characters.
\steven{The above paragraph sounds weird}

\subsection{Regular Expressions and DFAs}
\label{subsec:deriv}

In order to extend the DFA
parser from \cref{subsec:regexparser} to the construction of
a verified parser
generator for regular expression we need to perform some
plumbing establishing an equivalence between regular
expressions and DFAs.

There are several routes we may hope to take in establishing
this equivalence. First, we could prove an equivalence
between NFAs and regular expressions, and separately prove
an equivalence between NFAs and DFAs.
In \cref{subsec:eqproofs}, we include a version of
Thompson's construction --- which established the
equivalence between regular grammars and DFAs. We may additionally
hope to internalize a variant of the powerset construction \cite{rabinFiniteAutomataTheir1959}
--- which takes as input an NFA and constructs a DFA that
recognizes the same language --- and combine the results of
Thompson's construction and the powerset constructions to give an equivalence
between regular expressions and DFAs.  This route is alluring, as it
internalizes several classic grammar-theoretic constructions. However, it may
necessitate extensions to the LNL theory, like
a propositional truncation, and we have not yet investigated
how this would interact with the existing types in the
theory. The addition of a propositional truncation may seem
harmless, but it is not always immediately clear how
distinct constructions will interact. For instance, when
exploring LNL models, Benton discovered that the synthesis
of linear and dependent types require a new presentation of
the $!$ modality from linear logic
\cite{bentonMixedLinearNonlinear1995}. That is all to say,
this is a work in progress and
it is not immediate that the addition of a propositional
truncation is adequate for establishing the weak equivalence
between NFAs and DFAs.

We may instead hope to internalize an equivalence between
regular grammars and DFAs by using Brzozowski derivatives to
directly create a DFA that is weakly equivalent to a given
regular expression, as described by Owens et al.
\cite{owensRegularexpressionDerivativesReexamined2009}.
One characterization of regular grammars is that they are
precisely those grammars which have finitely many inequivalent Brzozowski
derivatives
\cite{brzozowskiDerivativesRegularExpressions1964}.
The algorithm used by Owens takes in a
regular grammar and generates a DFA that recognizes the same
language, and the states in this DFA are the finitely many
derivative equivalence classes. We initially had a version of
this theorem very roughly internalized in the LNL theory.
To our taste, too much of this presentation relied on
meta-arguments that lived outside of
our formalism, and thus this particular phrasing of the
theorem did not translate well into formalization.

In any case, we believe
that revisiting these lines of thought will lead to a
satisfactory internalization of the equivalence between
regular grammars and DFAs, and thus would bridge the gap
between our DFA parser and a full regular expression parser.

\subsection{Equivalence Between Regular Expressions and Finite Automata}
\label{subsec:eqproofs}
In this section, we describe a version of Thompson's
construction \cite{thompsonProgrammingTechniquesRegular1968}
   where we construct an NFA that recognizes a given regular
expression. Moreover, we will show that this NFA is strongly equivalent to the
original grammar. Witnessing this construction in our syntax has two benefits
\begin{enumerate}
  \item It reinforces this high-level view that the syntax is a natural and
        general setting for formal grammar reasoning, as we demonstrate that
        this formal system subsume results from existing systems, and
  \item Following the
        development of Thompson's construction, we then need
        only establish the equivalence of NFAs and DFAs to
        complete the full regular expression parser
\end{enumerate}

\begin{theorem}[Thompson]
  \label{thm:thompson}
  For $g$ a regular grammar $g$, there is an NFA $N$ that recognizes the same
  language as $g$.
\end{theorem}
We make a pretty straightforward adaptation of Thompson's theorem to our setting,

\begin{theorem}[Typed Thompson]
  \label{thm:typthompson}
  For $g$ a regular expression $g$, there is an NFA $N$ such that $g$ is isomorphic
  to $\mathsf{AccTrace}_{N}$.
\end{theorem}

\begin{proof}[Proof Sketch]
  Recall that regular expressions are inductively defined via
  disjunction, concatenation, and Kleene star over literals
  and the empty grammar. By induction over regular expressions,
  we will construct an NFA that is equivalent to $g$.

  First, define the recognizing NFA for the empty grammar
  $I$.

  \begin{figure}[h!]
  \begin{tikzpicture}[node distance = 25mm ]
    \node[state, initial] (1) {$1$};
    \node[state, right of=1, accepting] (2) {$2$};

    \path[->] (1) edge[below] node{$\varepsilon$} (2);
  \end{tikzpicture}
  \caption{$NFA(I)$}
  \label{fig:emptyNFA}
  \end{figure}

  The type of traces from the initial state of $NFA(I)$ to the single
  accepting state is given by,

  \[
    \mathsf{Trace}_{NFA(I)}(q_{1}, q_{2}) = \mu
      \begin{pmatrix}
         g_{q_{1}} := g_{q_{2}} \\
         g_{q_{2}} := I
      \end{pmatrix}. g_{q_{1}}
  \]

  The accepting traces through $NFA(I)$ are then described
  as,

  \[
    \mathsf{AccTrace}_{NFA(I)} = \LinSigTy q {\{1 , 2\}} {\left( \mathsf{Trace}_{NFA(I)}(q_{1} , q) \pair \mathsf{acc}(q) \right)}
  \]

  A quick inspection of \cref{fig:emptyNFA} reveals that the
  only reasonable choice for $q$ is $q_{2}$ --- because
  state 2 is accepting while state 1 is not. Therefore,

  \begin{align*}
    \mathsf{AccTrace}_{NFA(I)}
    & \cong \mathsf{Trace}_{NFA(I)}(q_{1} , q_{2}) \pair \mathsf{acc}(q_{2}) \\
    & \cong \mathsf{Trace}_{NFA(I)}(q_{1} , q_{2}) \pair \top \\
    & \cong \mathsf{Trace}_{NFA(I)}(q_{1} , q_{2})
  \end{align*}

  From here, to prove
  $I \cong \mathsf{AccTrace}_{NFA(I)}$ it suffices to show
  $I \cong \mathsf{Trace}_{NFA(I)}(q_{1} , q_{2})$. Below we
  give two parse transformers, one from $I$ to
  $\mathsf{Trace}_{NFA(I)}(q_{1} , q_{2})$ and vice versa.

  \[
    \inferrule
    {p : I \vdash \mathsf{nil} : \mathsf{Trace}(q_{2} , q_{2}) \\
     \exists \text{~transition~} q_{1} \overset{\varepsilon}{\to} q_{2}
    }
    {p : I \vdash \mathsf{\varepsilon cons}(\mathsf{nil}) : \mathsf{Trace}_{NFA(I)}(q_{1} , q_{2})}
  \]

  Let $\gamma$ be the substitution $\{ g_{q_{2}} / g_{q_{1}}, I / g_{q_{2}} \}$,

  \[
    \inferrule
    {
      p : \mathsf{Trace}_{NFA(I)}(q_{1} , q_{2}) \vdash p :
        \mathsf{Trace}_{NFA(I)}(q_{1} , q_{2}) \\
      x_{1} : \simulsubst {g_{q_{2}}} {\gamma} = I \vdash x_{1} : I \\
      x_{2} : \simulsubst {I} {\gamma} = I \vdash x_{2} : I
    }
    {
      p : \mathsf{Trace}_{NFA(I)}(q_{1} , q_{2}) \vdash \mathsf{mfold}(x_{1}.x_{1} , x_{2}.x_{2})(p) : I
    }
  \]

  This concludes the proof for the case of the empty
  grammar. Let's now walk through the construction for
  literal grammars. Given a character $c$, we construct an
  NFA that recognizes only the string containing the single
  character $c$ as,


  \begin{figure}[h!]
  \begin{tikzpicture}[node distance = 25mm ]
    \node[state, initial] (1) {$1$};
    \node[state, right of=1, accepting] (2) {$2$};

    \path[->] (1) edge[below] node{$c$} (2);
  \end{tikzpicture}
  \caption{$NFA(c)$}
  \label{fig:literalNFA}
  \end{figure}

  The automaton in \cref{fig:literalNFA} induces the
  following type of traces from $q_{1}$ to $q_{2}$.

  \[
    \mathsf{Trace}_{NFA(c)}(q_{1}, q_{2}) = \mu
      \begin{pmatrix}
         g_{q_{1}} := c \otimes g_{q_{2}} \\
         g_{q_{2}} := I
      \end{pmatrix}. g_{q_{1}}
  \]

  Through the same argument as the empty grammar, the only
  state that is accepting is $q_{2}$ and thus,

  \[
    \mathsf{AccTrace}_{NFA(c)} \cong \mathsf{Trace}_{NFA(c)}(q_{1} , q_{2})
  \]

  To show the desired isomorphism of $c \cong NFA(c)$ we
  make a similar argument as we did for the empty grammar
  $I$, except we leverage the $\mathsf{cons}$ rule instead
  of $\mathsf{\varepsilon cons}$. That is, the parse
  transformers in either direction are given as,

  \[
    \inferrule
    {\cdot \vdash \mathsf{nil} : \mathsf{Trace}(q_{2} , q_{2}) \\
     \exists \text{~transition~} q_{1} \overset{c}{\to} q_{2}
    }
    {p : c \vdash \mathsf{cons}(\mathsf{nil}) :
      \mathsf{Trace}_{NFA(c)}(q_{1} , q_{2})}
  \]


  \[
    \inferrule
    {
      p : \mathsf{Trace}_{NFA(c)}(q_{1} , q_{2}) \vdash p :
        \mathsf{Trace}_{NFA(c)}(q_{1} , q_{2}) \\
      x_{1} : \simulsubst {(c \otimes g_{q_{2}})} {\gamma} = c \otimes I \vdash \mathsf{unitR}(x_{1}) : c \\
      x_{2} : \simulsubst {I} {\gamma} = I \vdash x_{2} : I
    }
    {
      p : \mathsf{Trace}_{NFA(c)}(q_{1} , q_{2}) \vdash \mathsf{mfold}(x_{1}.\mathsf{unitR}(x_{1}) , x_{2}.x_{2})(p) : c
    }
  \]

  Where $\gamma$ is the substitution
  $\{ c \otimes g_{q_{2}} / g_{q_{1}}, I / g_{q_{2}} \}$ and
  $\mathsf{unitR}$ is a witness to the isomorphism
  $c \otimes I \cong c$. Again, we may see that these do
  indeed mutually invert each other in the Agda code.

  It remains to show that the desired isomorphisms are
  preserved by $\otimes$, $\oplus$, and Kleene star. Here,
  we will give the argument for the disjunction case, the
  others are defined quite similarly.

  Given two NFAs $N$ and $M$, we may define a new NFA that
  encodes the disjunction of $N$ and $M$. Denote the
  internal states of $N$ by $q_{j}$'s and the internal states
  of $M$ by $r_{k}$'s,

  \begin{figure}[h!]
  \begin{tikzpicture}[node distance = 20mm ]
    \node[state] (2) {$q_{init}$};
    \node[state, initial, below left of=2] (1) {$init$};
    \node[state, below right of=1] (3) {$r_{init}$};
    \node[right of=2] (4) {$\cdots$};
    \node[right of=3] (5) {$\cdots$};
    % \node[state, below right of=1] (3) {$3$};
    % \node[state, below of=2] (4) {$4$};

    \path[->] (1) edge[below] node{$\varepsilon$} (2);
    \path[->] (1) edge[below] node{$\varepsilon$} (3);
    \path[->] (2) edge[below] node{} (4);
    \path[->] (3) edge[below] node{} (5);

    \node[label={[name=l] $N$}, draw,line width=2pt,rounded corners=5pt, fit=(2)(4)] {};
    \node[label={[name=l] $M$}, draw,line width=2pt,rounded corners=5pt, fit=(3)(5)] {};
  \end{tikzpicture}
  \caption{$N \oplus_{NFA} M$}
  \label{fig:disjunctionNFA}
  \end{figure}

  \Cref{fig:disjunctionNFA} shows the process of
  disjunctively combining NFAs. Precisely, we add a single
  new state and we included copies of the states from each of $N$
  and $M$. The new state acts as the initial state and has
  $\varepsilon$-transitions to the initial states of $N$ and
  $M$. We include all of the internal transitions from $N$
  and $M$, and the accepting states of $N \oplus_{NFA} M$
  are exactly the accepting states in each subautomaton.

  Let $g$ and $g'$ be two regular grammars such that
  $g \cong NFA(g)$ and $g' \cong NFA(g')$. As a matter of
  notation\footnote{We shall similarly abuse notation for
    $\otimes$ and Kleene. That is, for a regular grammar
    $g$, when we write $NFA(g)$ we mean the NFA inductively
    built up with the NFA-analogues to the constructors that
    built up $g$.}, we will write $NFA(g \oplus g')$ for
  $NFA(g) \oplus_{NFA} NFA(g')$. The traces of
  $NFA(g \oplus g')$ are then given by,

  \[
    \mathsf{Trace}_{NFA(g \oplus g')}(src , dst) = \mu
      \begin{pmatrix}
        g_{init} := g_{q_{0}} \oplus g_{r_{0}} \\
        g_{q_{j}} := \mathsf{Trans}_{NFA(g)}(q_{j}) \oplus \mathsf{isDst}(q_{j}) \\
        g_{r_{k}} := \mathsf{Trans}_{NFA(g')}(r_{k}) \oplus \mathsf{isDst}(r_{k})
      \end{pmatrix}.g_{src}
    \]

  where $\mathsf{Trans}$ is used to echo the same syntactic
  definitions that appear in the $NFA(g)$ and $NFA(g')$.
  Also, $src$ and $dst$ may take on any value in
  $Q := \{init\} \cup \{q_{j}\} \cup \{r_{k}\}$, and
  $\mathsf{isDst}(q)$ checks if $q$ is equal to $dst$. Which
  is all to say, the traces of $NFA(g \oplus g')$ comprise
  either a trace of $NFA(g)$, or a trace of $NFA(g')$, and
  the transition coming out of $g_{init}$ determines which
  subautomaton we step into.

  The parse transformer from $g \oplus g'$ checks which side
  of the sum type we are on, then takes the appropriate step
  from $g_{init}$ in the automaton.

  \[
    \inferrule
    {
      u : g \vdash \iota (\phi (u)) : \mathsf{AccTrace}_{NFA(g \oplus g')} \\
      v : g' \vdash \iota' (\psi (v)) : \mathsf{AccTrace}_{NFA(g \oplus g')}
    }
    {p : g \oplus g' \vdash \mathsf{case}~p \{ \mathsf{inl}(u) \mapsto s , \mathsf{inr}(v) \mapsto r \} : \mathsf{AccTrace}_{NFA(g \oplus g')}}
  \]

  with $\iota$ and $\iota'$ as embeddings from $NFA(g)$ and
  $NFA(g')$, respectively, into $NFA(g \oplus g')$,
  $\phi: g \cong \mathsf{AccTrace}_{NFA(g)}$, and $\psi: g' \cong \mathsf{AccTrace}_{NFA(g')}$. On a
  high level, all this construction does is turn a parse of
  $g$ into a parse of $NFA(g)$ and then embeds that inside
  of the larger automaton $NFA(g \oplus g')$. Likewise for $g'$.

  In the other direction, recall that the data of an
  accepting trace for $NFA(g \oplus g')$ is a pair of a
  trace and a proof that
  the end state $q'$ of that trace is accepting. By
  multifolding over the first part of that pair, we turn the
  term of type
  $\mathsf{Trace}_{NFA(g \oplus g')}(init , q')$ into a
  trace of either of the subautomata,

  \[
    p : \mathsf{Trace}_{NFA(g \oplus g')}(init , q') \vdash \mathsf{mfold}_{NFA(g \oplus g')} : \mathsf{Trace}_{NFA(g)}(q_{0} , q') \oplus \mathsf{Trace}_{NFA(g')}(r_{0} , q')
  \]

  Additionally, we leverage the fact that the only accepting
  states for $NFA(g \oplus g')$ are those from the
  subautomata to extract that $q'$ must be an accepting
  state from a subautomaton.

  \[
    x : \mathsf{acc}_{NFA(g \oplus g')}(q') \vdash M : \mathsf{acc}_{NFA(g)}(q') \oplus \mathsf{acc}_{NFA(g')}(q')
  \]

  We then combine the trace and proof of acceptance into an
  accepting trace of one of the subautomata,

  \[
    p : \mathsf{AccTrace}_{NFA(g \oplus g')} \vdash N : \mathsf{AccTrace}_{NFA(g)} \oplus \mathsf{AccTrace}_{NFA(g')}
  \]

  Lastly, we then inductively use the isomorphisms $\phi$
  and $\psi$ to turn the accepting traces into a parse of
  $g$ or $g'$,

  \[
     N : \mathsf{AccTrace}_{NFA(g)} \oplus \mathsf{AccTrace}_{NFA(g')} \vdash \mathsf{case}~N~\{\mathsf{inl}(n) \mapsto \phi^{-1}(n), \mathsf{inr}(n') \mapsto \psi^{-1}(n')\} : g \oplus g'
  \]

  When you wish to show that these actually maps invert each other to form an isomorphism, we invoke the
  universal properties of the $\mathsf{Trace}$ types. That is,
  $\mathsf{Trace}_{\oplus NFA}$, $\mathsf{Trace}_{N}$, and $\mathsf{Trace}_{M}$
  are \emph{initial} algebras to the the functors that define them. Thus, the
  embedding of traces through $N$ into $\oplus NFA$ and then mapped back down to $N$
  compose to a map from $\mathsf{Trace}_{N}$ to itself. Such a map is unique and
  thus must be the identity. The same reasoning shows that the desired
  composite do indeed invert each other to form an isomorphism.

  As discussed above, the other cases and
  follow through a similar argument. This
  concludes the proof of our variant of Thompson's construction.
\end{proof}

\subsection{Other Automata}
\subsubsection{Pushdown Automata}
\label{subsubsec:pda}
A (nondeterministic) \emph{pushdown automaton} is an automaton that employs a
stack. Just like NFAs, they have transitions labeled with characters from a
fixed string alphabet $\Sigma$. Additionally, they maintain a stack of
characters drawn from a stack alphabet $\Gamma$. They are often represented
formally as a 7-tuple $(Q, \Sigma, \Gamma, \delta, q_{0}, Z, F)$,

\begin{itemize}
  \item $Q$ a finite set of states
  \item $\Sigma$ a fixed, finite string alphabet
  \item $S$ a fixed, finite stack alphabet
  \item
        $\delta \subset Q \times (\Sigma \cup \{ \varepsilon \}) \times S \to \mathcal{P}(Q \times S^{*})$
        the transition function
  \item $q_{0} \in Q$ the start state
  \item $Z \in S$ the initial stack symbol
  \item $F \subset Q$ the accepting states
\end{itemize}

We encode the traces of a pushdown automaton very similarly to those of an NFA,
except the transitions of a PDA are instead encoded via the LNL product type
$\bigamp$. This is because when simply transitioning via character, a PDA must
also pop and push characters onto a stack, which is used as the argument to
these dependent functions. Thus, the appropriate formation of traces through a
PDA is dependent on a stack.

Let $S$ be a non-linear type encoding the stack
alphabet, and build lists over $S$ as the (non-linear) least fixed-point
$\mathsf{List}(S) := \mu X . 1 + S \times X$. Then, the type of states for a
PDA $P$ with stack alphabet $S$ are given as a functions that takes in lists $L$,
and then makes a case distinction between possible transitions based on what was witnessed as $\mathsf{head}(L)$. The choice of transition
will then determine which character to transition by and what word $w$ should be
pushed onto the stack. The word that is added to the top of the stack is
appended to $\mathsf{tail}(L)$ and then we recursively step into another state
called on argument $w + \mathsf{tail}(L)$.

% \begin{gather*}
%   \mathsf{State} \in {g _{q} : q \in Q} \\
%   \mathsf{Word} \in \String \\
%   \mathsf{Char} \in \Sigma   \\
%   \mathsf{StackChar} \in \Gamma \\
% \end{gather*}
% \begin{align*}
%   \mathsf{Trans}_{P}(q) ::=~&
%                            \LinPiTy {(hd :: tl)} {\mathsf{List}(S)} {\left( \mathsf{Char} \otimes \mathsf{State}(\mathsf{Word} + tl) \right)}~| \\
%   & \LinPiTy {(hd :: tl)} {\mathsf{List}(S)} {\left(  \mathsf{State}(\mathsf{Word} + tl) \right)}~|~\mathsf{Trans}_{P}(q) \oplus \mathsf{Trans}_{P}(q)
% \end{align*}

% That is to say, when transitioning a PDA pops off the head $hd$ of the stack

\subsubsection{Turing Machines}
\label{subsubsec:tm}

Above we gave a grammar presentation of
traces through a PDA by using a non-linear type $S$
to encode the stack. We may similarly use pairs $S \times S$
to encode the tape of a Turing machine. With two stacks we can simulate the behavior of the
infinite tape of a Turing machine. The intuition behind this correspondence is
that the left half of the tape is on one stack, the right
half of the tape the other, and we treat the tops of stacks
like the head of the tape.

\pedro{I like the overall structure of this section! Showing how the type theory
can encode and reason about different kinds of languages is very instructive!}

\section{Related work}
\label{sec:related}

\paragraph{Kleene Algebra}

Since the early works in the theory of formal languages, Kleene
algebras have played an important role in its development. They
generalize the operations known from regular languages by introducing
operations generalizing language composition, language union and the
Kleene star.  More generally, they are defined as inequational theory
where the inequality is meant to capture language containment. This
theory is extremely successful, having found applications in algebraic
path problems, theory of programming languages, compiler optimizations
and more.

A frequently fruitful research direction is exploring varying
extensions of Kleene algebras, Kleene algebra with tests (KAT) being
one of the most notable ones. Our approach is radically different from
most extensions, which usually aim at modifying or adding new
operations to Kleene algebras, but still keeping it as an inequational
theory. By adopting a category-theoretic treatment and allowing the
``order structure'' to encode more information than merely
inequalities, we were able to extend Kleene algebra to reason about
parsing as well.

\paragraph{TODO: more stuff}

\steven{Comments from Max or Pedro to mention these:

That paper with Fritz Henglein about regexes as types

Vaughan Pratt on the residual operator

}

\steven{Mention Vakar's ``Syntax and Semantics of Linear Dependent Types'' as
  related work?}

\section{Future Work}
\label{sec:future}

\steven{Rework future work}
\steven{Mention the two-level linear,dependent compiler. ``A two level linear
  dependent type theory''. it could be a good place to build off for a language
  implementation that isn't just inference rules}

\subsection{Implementation for Context-Free Grammars}
As suggested throughout the paper and briefly explored in
\cref{subsubsec:pda}, the first extension to the work in
this paper will be to bring analogous constructions to
context-free grammars and their accompanying pushdown automata.


\subsection{Beyond Strings}
\label{subsec:beyond}

While parsing typically refers to the generation of semantic
objects from string input, many tasks in programming can be
viewed as parsing of objects with more structure, such as
trees with binding structure or graphs. Fundamental to the
frontend of many
programming language implementations are type systems. In
particular, \emph{type checking}
--- analogous to language recognition --- and \emph{typed
  elaboration} --- analogous to parsing --- arise when
producing a semantic object subject to some analysis. Just
as our string grammars were given as functors from $\String$
to $\Set$, we envision adapting the same philosophy
to functors from $\String$ to \emph{trees} to craft a syntax
that natively captures typed elaboration. This suggests an
unusual sort of bunched type theory, where context extension
no longer resembles concatenation of strings but instead
takes on the form of tree constructors.

Thus far, our theory has proved useful for internalizing long-standing
grammar-theoretic constructions, but there has been decades of research
conducted since. A fruitful avenue for future work includes
testing if our formalism can also internalize more recently proposed grammar
mechanisms, such as the interval parsing grammars given by Zhang et al. \cite{zhangIntervalParsingGrammars2023}.

\newpage

\bibliographystyle{plain}
\bibliography{refs.bib}


\end{document}
