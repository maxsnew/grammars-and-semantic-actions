\documentclass[sigconf,anonymous,review,screen]{acmart}
\usepackage{mathpartir}
\usepackage{tikz-cd}
\usepackage{enumitem}
\usepackage{wrapfig}
\usepackage{fancyvrb}
\usepackage{hyperref}
\usepackage{cleveref}
\usepackage{stmaryrd}

\newcommand{\sem}[1]{\llbracket{#1}\rrbracket}
\newcommand{\cat}[1]{\mathbf{#1}}
\newcommand{\lto}{\multimap}
\newcommand{\tol}{\mathrel{\rotatebox[origin=c]{180}{$\lto$}}}
\newcommand{\Set}{\mathbf{Set}}
\newcommand{\Gr}{\mathbf{Gr}}
\newcommand{\Type}{\mathbf{Type}}
\newcommand{\Prop}{\mathbf{Prop}}
\newcommand{\Bool}{\mathbf{Bool}}

\newcommand{\gluedNL}{{\mathcal G}_S}
\newcommand{\gluedNLUniv}{{\mathcal G}_{S,i}}
\newcommand{\gluedL}{{\mathcal G}_L}

\newcommand{\simulsubst}[2]{#1\{#2\}}
\newcommand{\subst}[3]{\simulsubst {#1} {#2/#3}}
\newcommand{\letin}[3]{\mathsf{let}\, #1 = #2 \, \mathsf{in}\, #3}
\newcommand{\lamb}[2]{\lambda #1.\, #2}
\newcommand{\lamblto}[2]{\lambda^{{\lto}} #1.\, #2}
\newcommand{\lambtol}[2]{\lambda^{{\tol}} #1.\, #2}
\newcommand{\dlamb}[2]{\overline{\lambda} #1.\, #2}
\newcommand{\app}[2]{#1 \, #2}
\newcommand{\applto}[2]{#1 \mathop{{}^{\lto}} #2}
\newcommand{\apptol}[2]{#1 \mathop{{}^{\tol}} #2}
\newcommand{\PiTy}[3]{\Pi #1 : #2.\, #3}
\newcommand{\SigTy}[3]{\Sigma #1 : #2.\, #3}
\newcommand{\LinPiTy}[3]{\widebar\Pi #1 : #2.\, #3}
\newcommand{\LinSigTy}[3]{\widebar\Sigma #1 : #2.\, #3}
\newcommand{\amp}{\mathrel{\&}}
\newcommand{\GrTy}{\mathsf{Gr}}

\newcommand{\ctxwff}[1]{#1 \,\, \mathsf{ok}}
\newcommand{\ctxwffjdg}[2]{#1 \vdash #2 \,\, \mathsf{type}}
\newcommand{\linctxwff}[2]{#1 \vdash #2 \,\, \mathsf{ok}}
\newcommand{\linctxwffjdg}[2]{#1 \vdash #2 \,\, \mathsf{linear}}

\newif\ifdraft
\drafttrue
\newcommand{\steven}[1]{\ifdraft{\color{orange}[{\bf Steven}: #1]}\fi}
\renewcommand{\max}[1]{\ifdraft{\color{blue}[{\bf Max}: #1]}\fi}
\newcommand{\pedro}[1]{\ifdraft{\color{red}[{\bf Pedro}: #1]}\fi}
\newcommand{\pipe}{\,|\,}

%% Rights management information.  This information is sent to you
%% when you complete the rights form.  These commands have SAMPLE
%% values in them; it is your responsibility as an author to replace
%% the commands and values with those provided to you when you
%% complete the rights form.
\setcopyright{acmcopyright}
\copyrightyear{2018}
\acmYear{2018}
\acmDOI{XXXXXXX.XXXXXXX}

%% These commands are for a PROCEEDINGS abstract or paper.
\acmConference[Woodstock '18]{Woodstock '18: ACM Symposium on Neural
  Gaze Detection}{June 03--05, 2018}{Woodstock, NY}
\acmBooktitle{Woodstock '18: ACM Symposium on Neural Gaze Detection,
  June 03--05, 2018, Woodstock, NY}
\acmPrice{15.00}
\acmISBN{978-1-4503-XXXX-X/18/06}


\begin{document}

\title{Formal Grammars as Functors and Formal Grammars as Types in Non-commutative Linear-Non-Linear Type Theory}
\author{Steven Schaefer}
\affiliation{
  \department{Electrical Engineering and Computer Science}
  \institution{University of Michigan}
  \country{USA}
}
\email{stschaef@umich.edu}

\author{Max S. New}
\affiliation{
  \department{Electrical Engineering and Computer Science}
  \institution{University of Michigan}
  \country{USA}
}
\email{maxsnew@umich.edu}

\author{Pedro H. Azevedo de Amorim}
\affiliation{
  \department{Department of Computer Science}
  \institution{University of Oxford}
  \country{UK}
}
\email{pedro.azevedo.de.amorim@cs.ox.ac.uk}

\begin{abstract}
  We propose a semantic framework for the study of formal
  grammars. First, we provide a syntax-independent notion of formal
  grammar as a function from strings to sets, generalizing the
  familiar notion of a language as a set of strings. The set of such
  grammars naturally form a very rich category whose notion of
  isomorphism corresponds to strong equivalence of grammars, and for
  which many language-theoretic constructs can be defined using
  universal properties.

  Based on these category-theoretic constructions we propose a new
  syntactic formalism for formal grammars: a version of dependent
  linear-non-linear type theory in which the tensor product is not
  commutative. The linear types can be interpreted as grammars, and
  linear terms as parse transformers. The non-dependent fragment of
  this type theory is already enough to express regular/context-free
  expressions as well as finite automata, with the isomorphism between
  regular languages and automata definable as linear terms. This
  generalizes the language theoretic formalisms such as Kleene
  algebras to incorporate ambiguity. By incorporating a dependency of
  linear types on non-linear types, we can further express indexed
  grammars, pushdown automata and Turing machines. This provides a new
  form of ``logical characterization'' of grammar classes based on a
  substructural logic rather than ordinary first-order logic. Further,
  the type theory can not only express the grammars as types, but
  equivalences between grammars and parsers as terms of the grammar.

  We give this type theory a semantics in the category of grammars and
  prove a canonicity theorem that shows that every term in a context
  corresponding to a fixed string is equal to a term in a canonical
  form encoding a parse tree of that string.
\end{abstract}

\maketitle

\section{A Syntax-Independent Notion of Syntax}

The theory of formal languages and parsing is one of the oldest and
most thoroughly developed areas of theoretical computer science. A
prominent topic in the 1950s and 60s led to a series of remarkable
developments: Chomsky's hierarchy of grammar formalisms, practical
algorithms for parsing of regular and context-free grammars and
variants, as well as implementations of practical tools for the
generation of efficient parsers.

A central notion is that of a \emph{formal language} $L$ over an
alphabet $\Sigma$ as simply a \emph{subset} of the set of strings $L
\subseteq \Sigma^*$. This definition is especially useful as it gives
a semantics to formal grammars that is completely independent of any
particular syntactic grammar formalism: any new notion of grammar can
be given a semantics in this common framework and it provides a
precise mathematical speicification for implementing a
\emph{recognizer} of a language and comparing the power of different
formalisms by demonstrating what languages can be formalized within
it. This also allows for completely \emph{language theoretic}
formulations of language classes: e.g., the regular \emph{languages}
can be characterized as those that have finitely many derivatives
\cite{brzozowski-or-myhill-nerode-idk}. This notion is remarkable
because it makes no reference to any specific syntactic formalism for
defining languages such as regular expressions or finite automata.

Formal language theory alone is not sufficient as a specification of a
parser: a language $L \subseteq \Sigma^*$ is equivalently defined as
its indicator function $\chi_L : \Sigma^* \to \Prop$ mapping each
string $w$ to the proposition that it is in the language $w \in
L$. This can then serve as the specification for a language
\emph{recognizer}: a program $r$ of type $\Sigma^* \to \Bool$ such
that $r(w) = \texttt{true}$ if and only if $\chi_L(w)$ holds, but
recognition is insufficient for most tasks for which the theory of
parsing was developed: the production of \emph{semantic} structures
from synactic descriptions in linguistics, compilation or
deserialization. The output of a parser is not just a boolean but a
\emph{parse tree}\footnote{such parse trees are usually not
materialized in memory, as the construction of the parse tree is fused
with application of semantic actions, which can be seen as a kind of
fold over the generated tree.}.

Due to this limitation, parsers are typically specified not by a
formal language, but by some \emph{formal grammar}, which specifies
not just \emph{which} strings are in the language but specifies what
are the \emph{parse trees} for the string. However unlike formal
language theory, there is no common universal notion of what
constitutes a formal grammar, but rather many syntactic systems such
as Chomskyan(?)  grammars, Thue systems, Montague grammars, etc which
are all syntactic presentations of the same underlying idea of an
abstract specification for parsing. The most common of these is the
Chomsky hierarchy of \emph{generative} grammars which specify parse
trees using \emph{derivations} of the string from a set of rewrite
rules, the parse tree encoding which rules were used.

Our first contribution is conceptual: we propose a simple,
syntax-independent definition of \emph{formal grammar}:
\begin{definition}
  A \emph{formal grammar} over an alphabet $\Sigma$ is a function
  $\Sigma^* \to \Set$.
\end{definition}
We say that a grammar $G$ associates to every string $w$ the set $Gw$
of \emph{parses} of that string. While we will work informally in this
paper, the collection $\Set$ can be formalized in many different
logical foundations: as a universe in type theory, a proper class in
set theory, etc. We argue that this is already a common intuition
latent in much prior work on grammars, especially when comparing
different formalisms for equivalence such as \cite{??}. This notion is
also completely natural in the context of constructive type theories
such as Agda which until recently did not include a universe of
propositions, and so for example in Elliott (\cite{??}) a formal
language was defined as a function to the universe $\Type$ with little
comment.

Every formal grammar $G$ induces a formal language, which can be seen
most easily by using the characteristic function formulation: we can
``squash'' any set into the proposition that it is inhabited, so
$\chi_G(w) = ||G(w)||$ which defines a subset $L_G = \{ w \in
\Sigma^*| G(w) \mathrm{inhabited}\}$.

Formal languages naturally arrange themselves into a partial order by
using the subset inclusion as the ordering. Formal grammars naturally
arrange themselves into a \emph{category} where a morphism $\alpha : G
\to H$ is given by a \emph{family} of functions
\[ \alpha^w : Gw \to Hw \]
for every string $w \in \Sigma^*$. The intuition is that a morphism of
grammars is a \emph{translation of parses}: a $G$-parse of $w$ gets
transformed to an $H$-parse of $w$. This then induces a notion of
\emph{isomorphism} of grammars: two grammars are equivalent if there
is a bijective translation of parses, precisely capturing the notion
of a \emph{strong equivalence} of grammars.

This category-theoretic framework can be further used to describe and
compare different notions of formal grammar. We can define a
\emph{notion of grammar} to be a category (or perhaps just groupoid)
paired with a functor into $\Set^{\Sigma^*}$. This naturally forms a
2-category the slice 2-category over $\Set^{\Sigma^*}$ and equivalence
in this 2-category defines a sensible notion of \emph{strong
equivalence of grammar formalisms} (TODO: check details).

The category of grammars is quite rich in structure, which can be most
easily observed from the fact that it is equivalently defined as the
category of functors $\Set^{\Sigma^*}$ where $\Sigma^*$ is here viewed
as a discrete category, that is, the objects are strings and only
morphisms are identity morphisms. Such functor categories, often
called presheaf categories, are incredibly rich in structure, which we
will exploit in section \ref{blah} to succinctly define the semantics
of regular and context-free grammars.

\section{Kleene Category}

Kleene algebras are an important tool in the theory of regular
languages. More broadly, they serve as a theoretical substrate to
studying various kinds of formal languages. Formally, they are a tuple
$(A, +, \cdot, (-)^*, 1, 0)$, where $A$ is a set, $+$ and $\cdot$
are binary operations over $A$, $(-)^*$ is a function over $A$, and
$1$ and $0$ are constants. These structures satisfy the axoims depicted
in Figure~\ref{fig:axioms}.

\begin{figure}
  \begin{align*}
    x + (y + z) &= (x + y) + z & x + y &= y + x\\
    x + 0 &= x & x + x &= x\\
    x(yz) &= (xy)z & x1 &= 1x = x\\
    x(y + z) &= xy + xz & (x + y)z &= xz + yz\\
    x0 &= 0x = x & & \\
    1 + aa^* &\leq a^* & 1 + a^*a &\leq a^*\\
     b + ax \leq x &\implies a^*b \leq x &  b + xa \leq x &\implies ba^* \leq x
  \end{align*}
  \label{fig:axioms}
  \caption{Kleene algebra axioms}
\end{figure}

The addition operation can be used to define the partial order
structure $a \leq b$ if $a + b = b$. In the theory of formal languages
this order structure can be used to model language containment. In this
section, we want to categorify the concept of Kleene algebra and
build on top of it in order to define an abstract theory of parsing.
We start by defining \emph{Kleene categories}.

\begin{definition}
  A Kleene category is a distributive monoidal category $\cat{K}$
  such that for every objects $A$ and $B$, the endofunctors $F_{A, B}
  = B + A \otimes X$ and $G_{A, B} = B + X \otimes A$ have initial
  algebras (denoted $\mu X.\, F_{A, B}(X)$) such that $B \otimes (\mu
  X.\, F_{A, 1}) \cong \mu X.\, F_{A, B}(X)$ and the analogue isomorphism
  for $G_{A,B}$ also holds.
\end{definition}

As a sanity check, note that Kleene algebras are indeed examples of
Kleene categories.

\begin{example}
  Every Kleene algebra, seen a posetal category, is a Kleene category.
\end{example}

An unexpected example comes from the theory of substructural logics.

\begin{example}
  The opposite category of every Kleene category is a model of a variant of
  conjunctive ordered logic, where the Kleene star plays the role of the ``of
  course'' modality from substructural logics which allows hypotheses to
  be discarded or duplicated.
\end{example}

The proposed axioms are a direct translation of the Kleene algebra
axioms to a categorical setting. Its most unusual aspect is the
axiomatization of the Kleene star as a family of initial algebras
satisfying certain isomorphisms. If the Kleene category $\cat{K}$ has
more structure, then these isomorphisms hold ``for free''.

\begin{theorem}
  \label{th:kleeneclosed}
  Let $\cat{K}$ be a Kleene category such that it is also monoidal
  closed.  Then, the initial algebras isomorphisms hold automatically.
\end{theorem}
\begin{proof}
  We prove this by the unicity (up-to isomorphism) of initial
  algebras. Let $[hd, tl]: 1 + (\mu X.\, F_{A, 1}(X)) \otimes A \to
  (\mu X.\, F_{A, 1}(X))$ be the initial algebra structure of $(\mu
  X.\, F_{A, 1}(X))$ and consider the map $[hd, tl] : B + B \otimes
  (\mu X.\, F_{A, 1}(X)) \otimes A \to B\otimes (\mu X.\, F_{A,
    1}(X))$.

  Now, let $[f,g] : B + A \otimes Y \to Y$ be an $F_{A,B}$-algebra and
  we want to show that there is a unique algebra morphism $h : B
  \otimes \mu X.\, F_{A,1} \to Y$. We can show existence and uniqueness
  by showing that the diagram on the left commute if, and only if,
  the diagram on the right commutes:

  This equivalence follows by using the adjunction structure given
  by the monoidal closed structure of $\cat{K}$. A completely analogous
  argument for $G_{A,B}$ also holds.
\end{proof}

This result feels similar in spirit to the definition of action
algebras, which are algebras where the product also has adjoint
operations which results in the Kleene star being more easily
axiomatized \cite{kozen1994}.

We are now ready to prove that our concept of formal grammars fits
nicely within our categorical framework. We start by presenting a
well-known construction from presheaf categories.

\begin{definition}
  Let $\cat{C}$ be a locally small monoidal category and $F$, $G$ be
  two functors $\cat{C} \to \Set$. Their Day convolution tensor
  product is defined as the following coend formula:
  \[
  (F \otimes_{Day} G)(x) = \int^{(y,z) \in \cat{C}\times\cat{C}}\cat{C}(y\otimes z, x) \times F(y) \times G(z) 
  \]
  Dually, its internal hom is given by the following end formula:
  \[
  (F \lto_{Day} G)(x) = \int_{y} \Set(F(y), G(x \otimes y))
  \]
\end{definition}

\begin{lemma}[\cite{day1970}]
  Under the assumptions above, the presheaf category $\Set^{\cat{C}}$ is
  monoidal closed.
\end{lemma}

%% \begin{theorem}
%%   Let $\cat{K}$ be a Kleene category and $A$ a discrete category.
%%   The functor category $[A, \cat{K}]$.
%%   (HOW GENERAL SHOULD THIS THEOREM BE? BY ASSUMING ENOUGH STRUCTURE,
%%   E.G. K = Set, THIS THEOREM BECOMES SIMPLE TO PROVE)
%% \end{theorem}
\begin{theorem}
  If $\cat{C}$ is a locally small monoidal category, then
  $\Set^{\cat{C}}$ is a Kleene category.
\end{theorem}
\begin{proof}

  By the lemma above, $\Set^{\cat{C}}$ is monoidal closed, and since it
  is a presheaf category, it has coproducts. Furthermore, the tensor
  is a left adjoint, i.e. it preserves colimits and, therefore, it is
  a distributive category.

  As for the Kleene star, since presheaf categories admit small colimits,
  the initial algebra of the functors $F_{A,B}$ and $G_{A,B}$ can be
  defined as the filtered colimit of the diagrams:

  From Theorem~\ref{th:kleeneclosed} it follows that these initial
  algebras satisfy the required isomorphisms and this concludes the
  proof.
\end{proof}

\begin{corollary}
  For every alphabet $\Sigma$, the presheaf category $\Set^{\cat{\Sigma^*}}$
  is a Kleene category.
\end{corollary}
\begin{proof}
  Note that string concatenation and the empty string make the
  discrete category $\Sigma^*$ a strict monoidal category.
\end{proof}

Much like in the posetal case, the abstract structure of a 2-Kleene
algebra is expressive enough to synthetically reason about formal
languages. A significant difference between them is that while Kleene
algebras can reason about language containment, 2-Kleene algebras can
reason about parsing as well.

For the rest of the paper we will use the presheaf category
$\Set^{\cat{\Sigma^*}}$ as a concrete model that will serve as our
guide when expanding abstract formalism presented in this section so
that it can handle more classes of languages beyond regular ones.

\paragraph{Beyond Simple Types}

\begin{definition}
  A model is a locally Cartesian category equipped with an internal Kleene category.
\end{definition}

\begin{theorem}
  The presheaf category $\Set^{\cat{\Sigma^*}}$ is a model.
\end{theorem}

\section{Related work}

\paragraph{Kleene Algebra}

Since the early works in the theory of formal languages, Kleene
algebras have played an important role in its development. They
generalize the operations known from regular languages by introducing
operations generalizing language composition, language union and the
Kleene star.  More generally, they are defined as inequational theory
where the inequality is meant to capture language containment. This
theory is extremely successful, having found applications in algebraic
path problems, theory of programming languages, compiler optimizations
and more.

A frequently fruitful research direction is exploring varying
extensions of Kleene algebras, Kleene algebra with tests (KAT) being
one of the most notable ones. Our approach is radically different from
most extensions, which usually aim at modifying or adding new
operations to Kleene algebras, but still keeping it as an inequational
theory. By adopting a category-theoretic treatment and allowing the
``order structure'' to encode more information than merely
inequalities, we were able to extend Kleene algebra to reason about
parsing as well.

\paragraph{TODO: more stuff}

%% That paper with Fritz Henglein about regexes as types

%% Vaughan Pratt on the residual operator


%% \section{Universal Constructions in the Category of Grammars}

%% Predicate BI: bicartesian closed, monoidal biclosed, omega-colimits
%% and therefore certain initial algebras



<<<<<<< Updated upstream
\section{Non-commutative Linear-Non-Linear Type Theory as a Syntax for Grammars}
=======
\section{A Bunched Type Theory for Grammars}
\label{sec:bunchedtypetheory}
>>>>>>> Stashed changes

\subsection{Syntax}
Judgments: $\Delta$ set ctx, $\Delta \vdash A$ set, $\Delta \vdash M :
A$ term, $\Delta \vdash \Gamma$ grammar ctx, $\Delta \vdash G$
grammar, $\Delta\pipe \Gamma \vdash p : G$ parse term.

\begin{figure}
  \label{fig:structjdg}
  \begin{mathpar}
    \inferrule{~}{\ctxwff \cdot}
    \and
    \inferrule{\ctxwff \Gamma \\ \ctxwffjdg \Gamma X}{\ctxwff {\Gamma, x : X}}
    
    \\

    \inferrule{~}{\linctxwff \Gamma \cdot}
    \and
    \inferrule{\linctxwff \Gamma \Delta \\ \linctxwffjdg \gamma A}{\linctxwff \Gamma {\Delta, a : A}}

    \\

    \inferrule{\Gamma \vdash X : U_i}{\ctxwffjdg \Gamma X}

    \and
    
    \inferrule{\Gamma \vdash A : L_i}{\linctxwffjdg \Gamma A}

    \\

    \inferrule{\Gamma \vdash X \equiv Y : U_i}{\ctxwffjdg \Gamma {X\equiv Y}}

    \and
    
    \inferrule{\Gamma \vdash A \equiv B : L_i}{\linctxwffjdg \Gamma {A \equiv B}}

  \end{mathpar}
  \caption{Structural judgements}
\end{figure}

\begin{figure}
  \label{fig:typewf}
  \begin{mathpar}
    \inferrule{~}{\Gamma \vdash U_i : U_{i+1}}
 %   
    \and
%
    \inferrule{~}{\Gamma \vdash L_i : U_{i+1}}
%
    \\
%
    \inferrule{\Gamma \vdash X : U_i \\ \hspace{-0.1cm} \Gamma, x : X \vdash Y : U_i}{\Gamma \vdash \PiTy x X Y : U_i }%
%
    \and
%
    \inferrule{\Gamma\vdash X : U_i \\ \hspace{-0.1cm} \Gamma, x : X \vdash Y : U_i}{\Gamma \vdash \SigTy x X Y : U_i}
%
    \\
%
    \inferrule{~}{\Gamma \vdash 1 : U_i}
%
    \and
%
    \inferrule{\Gamma \vdash A : L_i}{\Gamma \vdash G A : U_i}
%
    \\
%
    \inferrule{~}{\Gamma \vdash I : L_i}
 %   
    \and
%
    \inferrule{\Gamma \vdash A : L_i \\ \hspace{-0.1cm}\Gamma \vdash B : L_i}{\Gamma \vdash A \otimes B : L_i}
%
    \and
%
    \inferrule{\Gamma \vdash A : L_i \\ \hspace{-0.1cm}\Gamma \vdash B : L_i}{\Gamma \vdash A \lto B : L_i}
%
    \and
%
    \inferrule{\Gamma \vdash A : L_i \\ \hspace{-0.1cm}\Gamma \vdash B : L_i}{\Gamma \vdash B \tol A : L_i}
%
    \\
%
    \inferrule{\Gamma \vdash X : U_i \\ \Gamma, x : X \vdash A : L_i}{\Gamma \vdash \LinPiTy x X A : L_i}
%
    \and
%
    \inferrule{\Gamma \vdash X : U_i \\ \Gamma, x : X \vdash A : L_i}{\Gamma \vdash \LinSigTy x X A : L_i}
%
    \\
%
    \inferrule{~}{\Gamma \vdash \top : L_i}
%
    \and
%
    \inferrule{\Gamma \vdash A : L_i \\ \Gamma \vdash B : L_i}{\Gamma \vdash A \amp B : L_i}
%
    \\
%
    \inferrule{\Gamma, x : L_i \vdash A : L_i \and A \textrm{ strictly positive}}{\Gamma \vdash \mu x.\, A : L_i}
  \end{mathpar}
  \caption{Type well-formedness}
\end{figure}

\begin{figure}
  \label{fig:inttyping}
  \begin{mathpar}
  \inferrule{~}{\Gamma, x : X, \Gamma' \vdash x : X}
  %
  \and
  %
  \inferrule{\Gamma \vdash e : Y \quad \ctxwffjdg \Gamma {X \equiv Y}}{\Gamma \vdash e : X}
  %
  \\\
  %
  \inferrule{~}{\Gamma \vdash () : 1}
  %
  \and
  %
  \inferrule{\Gamma \vdash e : X \\ \Gamma \vdash e : \subst Y e x}{\Gamma \vdash (e, e') : \SigTy x X Y}
  %
  \\
%
  \inferrule{\Gamma \vdash e : \SigTy x X Y}{\Gamma \vdash \pi_1\, e : X}
  %
  \and
  %
  \inferrule{\Gamma \vdash e : \SigTy x X Y}{\Gamma \vdash \pi_2\, e : \subst Y {\pi_1\, e} x}
  \and
  \inferrule{\Gamma, x : X \vdash e : Y}{\Gamma \vdash \lamb x e : \PiTy x X Y}
  %
  \and
  %
  \inferrule{\Gamma \vdash e : \PiTy x X Y \\ \Gamma \vdash e' : X}{\Gamma \vdash \app e {e'} : \subst Y {e'} x}
  %
  \\
  %
  \inferrule{\Gamma \vdash e \equiv e' : X}{\Gamma \vdash \mathsf{refl} : e =_X e'}
  \and
  \inferrule{\Gamma \mid \cdot \vdash e : A}{\Gamma \vdash \mathsf G e : \mathsf G A}
  \end{mathpar}
  \caption{Intuitionistic typing}
\end{figure}

\begin{figure}
  \label{fig:linsyntax}
  \begin{mathpar}
    \inferrule{~}{\Gamma \mid a : A \vdash a : A}
    \and
    \inferrule{\Gamma \mid \Delta \vdash e : B \\ \linctxwffjdg \Gamma {A \equiv B}}{\Gamma \mid \Delta \vdash e : A}
    %
    \\
    %
    \inferrule{~}{\Gamma \mid \cdot \vdash () : I}
    \and
    \inferrule{\Gamma \mid \Delta \vdash e : I \\ \Gamma \mid \Delta_1',\Delta_2' \vdash e' : C}{\Gamma \mid \Delta_1',\Delta,\Delta_2' \vdash \letin {()} e {e'} : C}
    %
    \\
    %
    \inferrule{\Gamma \mid \Delta \vdash e : A \\ \Gamma \mid \Delta' \vdash e' : B}{\Gamma \mid \Delta, \Delta' \vdash e \otimes e' : A \otimes B}
    %
    \\
    %
    \inferrule{\Gamma \mid \Delta \vdash e : A \otimes B \\ \Gamma \mid \Delta'_1, a : A, b : B, \Delta'_2 \vdash e'}{\Gamma \mid  \Delta_1', \Delta, \Delta'_2 \vdash \letin {a \otimes b} e {e'}}
    \\
    %
    \inferrule{\Gamma \mid \Delta, a : A \vdash e : B}{\Gamma \mid \Delta \vdash \lamblto a e : A\lto B}
    \and
    \inferrule{\Gamma \mid \Delta' \vdash e' : A \\ \Gamma \mid \Delta \vdash e : A \lto B}{\Gamma \mid \Delta', \Delta \vdash \applto {e'} {e} : B}
    \\
    %
    \inferrule{\Gamma \mid a : A, \Delta \vdash e : B}{\Gamma \mid \Delta \vdash \lambtol a e : B\tol A}
    \and
    \inferrule{\Gamma \mid \Delta \vdash e : B \tol A \\ \Gamma \mid \Delta' \vdash e' : A}{\Gamma \mid \Delta, \Delta' \vdash \apptol e {e'} : B}
    %
    \\
    %
    \inferrule{\Gamma, x : X \mid \Delta  \vdash e : A}{\Gamma \mid \Delta \vdash \dlamb x e : \LinPiTy x X A}
    \and
    \inferrule{\Gamma \mid \Delta \vdash e : \LinPiTy x X A \\ \Gamma \vdash e' : X}{\Gamma \mid \Delta \vdash \app e {e'} : \subst A {e'} x}
    %
    \\
    %
    \inferrule{\Gamma \vdash e : X \quad \Gamma \mid \Delta \vdash e' : \subst A e x}{\Gamma \mid \Delta \vdash (e, e') : \LinSigTy x X A}
    %
    \\
    %
    \inferrule{\Gamma \mid \Delta \vdash e : \LinSigTy x X A \quad \Gamma, x : X \mid \Delta'_1, a : A, \Delta'_2 \vdash e' : C}{\Gamma\mid \Delta'_1, \Delta, \Delta'_2 \vdash \letin {(x, a)} e {e'}: C}
    %
    \\
    %
    \inferrule{~}{\Gamma \mid \Delta \vdash () : \top}
    %
    \\
    %
    \inferrule{\Gamma \mid \Delta \vdash e_1 : A_1 \quad \Gamma \mid \Delta \vdash e_2 : A_2}{\Gamma \mid \Delta \vdash (e_1, e_2) : A_1 \amp A_2}
    \and
    \inferrule{\Gamma \mid \Delta \vdash e : A_1 \amp A_2 }{\Gamma \mid \Delta \vdash \pi_i \, e : A_i}
    %
    \\
    %
    \inferrule{\Gamma \vdash e : \mathsf{G} A}{\Gamma \mid \cdot \vdash \mathsf{G}^{-1}\, e : A}
    %
    \\
    %
    \inferrule{\Gamma; \Delta \vdash e : \subst A {\mu x.\, A} x}{\Gamma; \Delta \vdash \mathsf{cons}\, e : \mu x.\, A}
    \and
    \inferrule{\Gamma;\Delta\vdash e' : \mu x.\,A \and \Gamma; a:\subst A B x \vdash e : B}{\Gamma;\Delta\vdash \mathsf{fold}(a.e)(e') : B}
  \end{mathpar}
  \caption{Linear typing}
\end{figure}

\pedro{Need to figure out the rules for inductive types and write down
  the judgemental equality rules}

\subsection{Semantics in Grammars}

For illustrative purposes, we give a brief overview of the concrete semantics in sets and grammars:
\begin{enumerate}
\item A non-linear context $\Gamma$ denotes a set $\sem{\Gamma}$
\item A non-linear type $\Gamma \vdash X : U_i$ denotes a family of sets $\sem{X} : \sem{\Gamma} \to \Set_i$
\item A non-linear term $\Gamma \vdash e : X$ denotes a section $\sem{e} : \Pi(\gamma:\sem{\Gamma})\,\sem{X}\,\gamma$
\item Linear contexts $\Gamma \vdash \Delta$ and types $\Gamma \vdash A : L_i$ both denote families of grammars $\sem{\Gamma}\to \Gr_i$
\item A linear term $\Gamma; \Delta \vdash e : A$ denotes a family of parse transformers $\sem{e} : \Pi(\gamma:\sem{\Gamma})\Pi(w:\Sigma^*)\,\sem{\Delta}\gamma w \Rightarrow \sem{A}\gamma w)$
\end{enumerate}
Most of the non-linear semantics is standard, we show just the $G$ type and the linear semantics:
\begin{enumerate}
\item $\sem{G A} \gamma = \sem{A} \gamma \varepsilon$
\item $\sem{\LinPiTy x X A} \gamma w = \Pi(x:\sem{X}\gamma) \sem{A}(\gamma,x) w$
\item $\sem{\LinSigTy x X A} \gamma w = \Sigma(x:\sem{X}\gamma) \sem{A}(\gamma,x) w$
\item $\sem{I} \gamma w = \{ () \pipe w = \epsilon \}$
\item $\sem{A \otimes B} \gamma w = \Sigma(w1,w2:\Sigma^*) w1w1 = w \wedge \sem{A} \gamma w_1 \times \sem{B} \gamma w_2$
\item $\sem{A \lto B} \gamma w = \Pi(w_a:\Sigma^*) A \gamma w_a \Rightarrow B\gamma (w_aw)$
\item $\sem{B \tol A} \gamma w = \Pi(w_a:\Sigma^*) A \gamma w_a \Rightarrow B\gamma (ww_a)$
\item $\sem{\mu x. A} \gamma = \mu (x:\Gr_i). \sem{A}(\gamma,x)$
\end{enumerate}

Propositional fragment: monoidal biclosed category that is also
bicartesian closed and has $\omega$-colimits. Call such a BIC.

Set theoretic fragment: category with families $\mathcal C$ + $\Pi,\Sigma,W$.

Combination: category with families $\mathcal C$ + a functor $L : C
\to \textrm{BIC}$.

Grammatical fragment: functor from $\mathcal C$ to the category of BI-models

Example: use $\Set$ as the CwF, use the functor $\Delta \mapsto
(\Set^{\Sigma^*})^\Delta$ as the model of the propositional fragment
using the pointwise-Day convolution monoidal structure.

\subsection{Canonicity}

Let $\cdot \vdash A$ be a closed linear type. Then there are
two obvious notions of what constitutes a ``parse'' of a string w
according to the grammar $A$:
\begin{enumerate}
\item On the one hand we have the set-theoretic semantics just
  defined, $\llbracket A \rrbracket \cdot w$
\item On the other hand, we can view the string $w = c_1c_2\cdots$ as
  a linear context $\lceil w \rceil = x_1:c_1,x_2:c_2,\ldots$ and
  define a parse to be a $\beta\eta$-equivalence class of linear terms $\cdot;
  \lceil w \rceil \vdash e : G$.
\end{enumerate}
It is not difficult to see that at least for the ``purely positive''
formulae (those featuring only the positive connectives
$0,+,I,\otimes,\mu, \overline\Sigma,c$) that
every element $t \in \llbracket A \rrbracket w$ is a kind of tree and
that the nodes of the tree correspond precisely to the introduction
forms of the type. However it is far less obvious that \emph{every}
linear term $\lceil w \rceil \vdash p : \phi$ is equal to some
sequence of introduction forms since proofs can include elimination
forms as well. To show that this is indeed the case we give a
\emph{canonicity} result for the calculus: that the parses for .

\begin{definition}
  A non-linear type $X$ is purely positive if it is built up using
  only finite sums, finite products and least fixed points.

  A linear type is purely positive if it is built up using only finite
  sums, tensor products, generators $c$, least fixed points and linear
  sigma types over purely positive non-linear types.
\end{definition}

\begin{definition}
  %% Let $X$ be a closed non-linear type. The closed elements $\textrm{Cl}(X)$ of $X$ are the definitional equivalence classes of terms $\cdot \vdash e : X$.

  Let $A$ be a closed linear type. The nerve $N(A)$ is a presheaf on
  strings that takes a string $w$ to the definitional equivalence
  classes of terms $\cdot; \lceil w\rceil \vdash e: N(A)$.
\end{definition}

\begin{theorem}[Canonicity]
  Let $A$ be a closed, purely positive linear type. Then there is an
  isomorphism between $\llbracket A\rrbracket$ and $N(A)$.
\end{theorem}
\begin{proof}
  We outline the proof here, more details are in the appendix. The
  proof proceeds first by a standard logical families construction
  that combines canonicity arguments for dependent type theory
  \cite{coquand,etc} with logical relations constructions for linear
  types \cite{hylandschalk}. It is easy to see by induction that the
  logical family for $A$, $\hat A$ is isomorphic to $\llbracket A
  \rrbracket$ and the fundamental lemma proves that the projection
  morphism $p : \hat A \to N(A)$ has a section, the canonicalization
  procedure. Then we establish again by induction that
  canonicalization is also a retraction by observing that introduction
  forms are taken to constructors.
\end{proof}


\begin{enumerate}
\item Every term $\lceil w \rceil \vdash p : G + H$ is equal to $\sigma_1q$ or $\sigma_2 r$ (but not both)
\item There are no terms $\lceil w \rceil \vdash p : 0$
\item If there is a term $\lceil w \rceil \vdash p : c$ then $w = c$ and $p = x$.
\item Every term $\lceil w \rceil \vdash p : G \otimes H$ is equal to $(q,r)$ for some $q,r$
\item Every term $\lceil w \rceil \vdash p : \epsilon$ is equal to $()$
\item Every term $\lceil w \rceil \vdash p : c$ is equal to $x:c$
\item Every term $\lceil w \rceil \vdash p : \mu X. G$ is equal to $\textrm{roll}(q)$ where $q : G(\mu X.G/X)$
\item Every term $\lceil w \rceil \vdash p : (x:A) \times G$ is equal
  to $(M,q)$ where $\cdot \vdash M : A$
\end{enumerate}

To prove this result we will use a logical families model. We give a
brief overview of this model concretely:
\begin{enumerate}
\item A context $\Gamma$ denotes a family of sets indexed by closing substitutions $\hat\Gamma : (\cdot \vdash \Gamma) \Rightarrow \Set_i$
\item A type $\Gamma \vdash X : U_i$ denotes a family of sets $\hat X : \Pi(\gamma:\cdot \vdash \Gamma) \hat\Gamma \Rightarrow (\cdot \vdash \simulsubst X \gamma) \Rightarrow \Set_i$
\item A term $\Gamma \vdash e : X$ denotes a section $\hat e : \Pi(\gamma)\Pi(\hat\gamma)\hat X \gamma \hat\gamma (\simulsubst e \gamma)$
\item A linear type $\Gamma \vdash A : L_i$ denotes a family of grammars $\hat A : \Pi(\gamma:\cdot\vdash\Gamma)\,\hat\Gamma \Rightarrow \Pi(w:\Sigma^*) (\cdot;\lceil w\rceil \vdash A[\gamma])\Rightarrow \Set_i$, and the denotation of a linear context $\Delta$ is similar.
\item A linear term $\Gamma;\Delta \vdash e : A$ denotes a function \[\hat e : \Pi(\gamma)\Pi(\hat\gamma)\Pi(w)\Pi(\delta : \lceil w \rceil \vdash \simulsubst \Delta \gamma) \hat\Delta \gamma \hat\gamma \delta \Rightarrow \hat A \gamma \hat\gamma w {(\simulsubst {\simulsubst e \gamma} \delta)}\]
\end{enumerate}
And some of the constructions:
\begin{enumerate}
\item $\widehat {(G A)} \gamma \hat\gamma e = \hat A \gamma \hat\gamma \varepsilon (G^{-1}e)$
\item $\widehat {(A \otimes B)} \gamma \hat\gamma w e = \Sigma(w_Aw_B = w)\Sigma(e_A)\Sigma(e_B) (e_A,e_B) = e \wedge \hat A \gamma \hat \gamma w_A e_A \times \hat B \gamma \hat \gamma w_B e_B$
\item $\widehat {(A \lto B)} \gamma \hat\gamma w e = \Pi(w_A)\Pi(e_A) \hat A \gamma\hat\gamma w_A e_A \Rightarrow \hat B \gamma\hat\gamma (ww_A) (\applto {e_A} e)$
\end{enumerate}

First, the category with families will be
the category of logical families over set contexts/types
$\Delta$/$A$. Then the propositional portion will be defined by
mapping a logical family $\hat \Gamma \to \Gamma$ 

First, let $L$ be the category of BI formulae and proofs (quotiented
by $\beta\eta$ equality). Define a functor $N : L \to \Set^{\Sigma^*}$ by
\[ N(\phi)(w) = L(w,\phi) \]

Then define the gluing category $\mathcal G$ as the comma category
$\Set^{\Sigma^*}/N$. That is, an object of this category is a pair of
a formula $\phi \in L$ and an object $S \in \mathcal
P(\Sigma^*)/N(\phi)$. We can then use the equivalence $\mathcal
P(\Sigma^*)/N(\phi) \cong \mathcal P(\int N(\phi))$ to get a simple
description of such an $S$: it is simply a family of sets indexed by
proofs $L(w,\phi)$:
\[ \prod_{w\in\Sigma^*} L(w,\phi) \to \Set \]
This category clearly comes with a projection functor $\pi : \mathcal
G \to \mathcal L$ and then our goal is to define a section by using
the universal property of $\mathcal L$.

To this end we define
\pedro{I'm pretty sure that these definitions have been somewhat presented
  in ``Glueing and orthogonality for models of linear logic'' by Hyland and Schalk.
We should probably cite them so that we don't have to rederive their constructions and proofs here.}
\begin{enumerate}
\item $(\phi, S) \otimes (\psi, T) = (\phi \otimes \psi, S\otimes T)$ where
  \[ (S \otimes T)(w, p) = (w_1w_2 = w) \times (q_1,q_2 = p) \times S\,w_1\,q_1 \times T\,w_2\,q_2\]
\item $(\phi, S) \multimap (\psi, T) = (\phi \multimap \psi, S \multimap T)$ where
  \[ (S \multimap T)(w,p) = w' \to q \to S\,w'\,q \to T (ww') (p\,q) \]
\item $\mu X. ??$ ??
\end{enumerate}

\section{Type-theoretic Characterizations of Language Classes}




\steven{idk if the following should go here or in another section}

Classically, formal language theory is closely related to automata theory. In the styles of Chomsky and Sch\"utzenberger, there is a hierarchy of language classes and the types of automata that recognize them. Inside of the type theory of \cref{sec:bunchedtypetheory}, we can characterize these language classes; moreover, we demonstrate equivalence of each class to the associated automaton as an isomorphism within our logic.

Note, predicate BI with existential quantifiers is already enough to define
formulae that correspond to arbitrary recursively enumerable languages
since we can simulate the tape of a Turing machine using two
stacks. I.e., we can define a set $S$ of lists of tape symbols, and
then define a formula that first ``copies'' the input onto one stack, then executes

\[ \mu \textrm{Copy} : S \to *. \lambda s. (\epsilon \wedge T\,s\,[])\bigvee_{c \in \Sigma} c (S\,{c::s}) \]

We reflect on this expressive power to motivate why we put heavy syntactic restrictions on our presentation of certain types of automata. For instance, when defining a pushdown automata, we ought to be very careful about how the set representing the stack can be accessed and added to. Careless definitions may accidentally endow us with too much expressive power, launching us further up in the Chomsky hierarchy. To begin, let us focus only on language classes, and their associated automata whose parses are definable in the context $\cdot | \Delta$. Intuitively, this means the class of automata definable without access to any memory --- i.e.\ finite state machines.

\subsection{Finite State Machines}
\steven{Formatting of this whole section should likely change. This is a messy first pass, and a fair bit of this is maybe better suited for an appendix}
We may encode a finite state machine via a particular syntactic restriction on grammars. A nondeterministic finite automaton (NFA) is defined as a mutual \footnote{For convenience we refer to a mutual fixed point here, even though we only take single fixed points as primitive. As in Beki\'c's theorem, we may decompose our mutual fixed points into a series of single fixed points.} fixed point in the following manner,

\steven{Is there a better way to present this?}
\[
  NFA ::= \mu \begin{pmatrix}
        Y_{s} = \langle Q \rangle \\
        Y_{1} = \langle Q \rangle \\
        \vdots \\
        Y_{e} = I
        \end{pmatrix}~\textbf{in}~Y_{s}
\]
where
\[
  Q ::= c \otimes Y ~|~ Y ~|~ Q \oplus Q
\]
and $Y \in \{ Y_{s}, Y_{1}, \dots, Y_{N} \}$. Intuitively, we read each $Y_{j}$ as the label for a state, and the definitions in form $Q$ describe the transitions between states --- $c \otimes Y$ denotes a labeled transition, and $Y$ an $\epsilon$-transition. Without loss of generality, $Y_{s}$ is reserved for a single distinguished start state, and $Y_{e}$ is reserved for a single distinguished accepting state. A disjunction\footnote{There is an admissible notion of coproduct making use of the primitive dependent pairing --- i.e. $A \oplus B := \LinPiTy b \Bool {\textbf{if}~b~A~B}$. We will utilize this without much comment to represent the usual notion of alternation of grammars.} of transitions in an NFA grammar corresponds to multiple outgoing edges of a given state in an automaton.

An NFA is \emph{deterministic} if there are no $\epsilon$-transitions and for every state $Y_{j} = \bigoplus_{j} (c_{j} \otimes Y_{n_{j}})$, the $c_{j}$ are distinct. \steven{Is this the right notion of determinism?} Perhaps, it is is better to define determinism as something like: there is at most one derivation for all $j, k$, for all $a \in \Sigma$ we have $x : Y_{k}, y : a \vdash z : Y_{j}$.

With this setting for finite automata, we can now internalize classical theorems inside of our formal system. First, let us describe a version of Thompson's construction \cite{??} where we show that any regular expression can be recognized by some NFA\@. Moreover, we will show that this NFA is strongly equivalent to the original grammar.

\subsection{Equivalence Between Regular Expressions and Finite Automata}

\steven{Determinization/DFAs to follow}

\subsubsection{Grammar to NFA}

We will define the recognizing NFA of a regular expression inductively. To handle the atomic grammars --- literals and the empty grammar --- we define the following,
\[
  NFA(c) =
  \mu \begin{pmatrix}
        Y_{s} = c \otimes Y_{e} \\
        Y_{e} = I
  \end{pmatrix}~\textbf{in}~Y_{s}
\]
\[
  NFA(I) =
  \mu \begin{pmatrix}
        Y_{s} = Y_{e} \\
        Y_{e} = I
  \end{pmatrix}~\textbf{in}~Y_{s}
\]

The isomorphism between the grammars $c$ and $NFA(c)$ are witnessed by the following derivations,
\[
  \inferrule{p : c \vdash p : c \\ \cdot \vdash () : I}{p : c \vdash p \otimes () : Y_{s} = c \otimes I}
\]
and
\[
  \inferrule{q : Y_{s} = c \otimes I \vdash q : Y_{s} \\ p : c, e : I \vdash p : c }{q : Y_{s} \vdash \letin {p \otimes e} q p : c}
\]

Similar terms are used to show the equivalence between $I$ and $NFA(I)$.

We can now inductively define some NFA combinators and show that they preserve grammar equivalence. That is, assuming that $g \cong NFA(g)$ and $g' \cong NFA(g')$, we define an NFA that recognizes $g \oplus g'$. Let the states in $NFA(g)$ be represented by $X$'s and the states in $NFA(g')$ be represented by $Z$'s,

\[
  NFA(g \oplus g') =
  \mu \begin{pmatrix}
        Y_{s} =  X_{s}' \oplus Z_{s}' \\
        X_{s}', X_{1}', \dots = \dots \\
        X'_{e} = Y_{e} \\
        Z_{s}', Z_{1}, \dots = \dots \\
        Z_{e}' = Y_{e} \\
        Y_{e} = I
  \end{pmatrix}~\textbf{in}~Y_{s}
\]

That is, for each defintion $X_{j}$ in $NFA(g)$ we create an analogous definition $X_{j}'$ inside of $NFA(g \oplus g')$ with the syntactic equality $X_{j}' = X_{j}$ for $j \neq e$. Lastly, $X_{e}'$ has an $\epsilon$-transition to the accepting state of $Y_{e}$. Likewise, we build analogous defintions for the states $Z_{j}$ in $NFA(g')$, then we flatten all of these defintions into one mutally recursive list.

We define a morphism $NFA(g \oplus g') \vdash g \oplus g'$ via a multi-fold over all the mutually recursive defintions of $NFA(g \oplus g')$,

\steven{I have the rest of these tables for Thompson's construction, but I don't like this way of presenting it. Maybe just give as a single multifold dertivation tree. Argue that multifold is admissible as sequential single folds}
\steven{For the Kleene star case, I also should mention its admissibility under the $\mu$ fixed point inference rules}

\begin{tabular}{c c c}
  Defintion & Term \\
  \hline
  $Y_{s}$ & $p : g \oplus g' \vdash p : g \oplus g' $ \\
  $X_{j}', j \not \in \{s, e\}$ & $p : X_{j} \vdash p : X_{j}$\\
  $X_{s}'$ & $p : X_{s} \vdash \phi^{{-1}}(p) : g$ \\
  $X_{e}'$ & $p : I \vdash p : I = X_{e}$ \\
  $Z_{j}', j \not \in \{s, e\}$ & $p : Z_{j} \vdash p : Z_{j}$\\
  $Z_{s}'$ & $p : Z_{s} \vdash \psi^{{-1}}(p) : g'$ \\
  $Z_{e}'$ & $p : I \vdash p : I = Z_{e}$ \\
  $Y_{e}$ & $p : I \vdash p : I$
\end{tabular}

We can read this as providing tail calls that replace the state $X_{s}'$ and $Z_{s}'$ with $g$ and $g'$, respectively. Likewise, we have a map in the other direction:

\[
\inferrule{p_{1} : g \vdash \textbf{inl}(\phi(p_{1})) : NFA(g \oplus g') \\ p_{2} : g' \vdash \textbf{inr}(\psi(p_{2})) : NFA(g \oplus g')}{p : g \oplus g' \vdash \textbf{case}~Y_{e} \{ g \mapsto \textbf{inl}(\phi(p_{1})), g' \mapsto \textbf{inr}(\psi(p_{2})) \} : NFA(g \oplus g')}
\]
where $\phi$ and $\psi$ are the witnesses to the isomorphisms $g \cong NFA(g)$ amd $g' \cong NFA(g')$, respectively. The above table and this most recent inference rule evidently invert each other.

\subsubsection{NFA to Grammar}

\steven{This section and following are basically todos for rn}

Weaken syntactic form to a generalized NFA, where guards on transition can be arbitrary regexps. Then show that taking away states from the NFA gives an algebraic description of the regexp inside of the guards. Repeat until single grammar --- ambiguity should e preserved if we don't use any algebraic simplification across disjunctions. However, this feels like a messy way to write this. Instead use the Taylor expansion of a grammar??

\steven{An aside on some axioms}

Something like $A \cong (I \& A) \oplus \sum_{c} c \otimes (c \lto A)$

For this taylor iso to hold, we need the map $c \otimes (c \lto A) \vdash (c \otimes \top) \amp A$ to be an iso. Specifically, this term needs an iso $\phi : c \otimes (c \lto A) \to (c \otimes \top) \amp A$ defined below,

\[
p : c \otimes (c \lto A) \vdash \phi(p) = \letin {p_{1} \otimes p_{2}} p {(p_{1} \otimes \top, p_{1}^{\lto}p_{2})} : (c \otimes \top) \amp A
\]

Also add the axiom that $\top \cong (\sum_{c \in \mathcal{A}} c)^{*}$

If we have the above, we can then show the following isos for some alphabet $\mathcal{A}$,
\begin{align*}
  A
  & \cong \top \amp A \\
  & \cong (\Sigma_{c \in \mathcal{A}})^{*}\amp A \\
  & \cong (I \oplus (\Sigma_{c} c \otimes \top)) \amp A \\
  & \cong (I \amp A) \oplus ((\Sigma_{c} c \otimes \top)) \amp A) \\
  & \cong (I \amp A) \oplus (\Sigma_{c }c \otimes (c \lto A)) & \text{using } \phi^{-1}
\end{align*}

From here we need to also show that derivatives of regular languages are regular (look at Brzozowksi's proof) and that regular languages have finitely many derivatives (potentially with some size constraint on the derivative?). Then the last iso in the chain above can be used to build a DFA equivalent to $A$ using Brzozowksi's algorithm.

Need to also add in axioms for $\amp$ distributing over $\oplus$ and $\lto$ distributing over $\oplus$. And that $c \amp c' \vdash 0$ for $c \neq c'$

\subsection{DFAs and NFAs}

All DFAs are NFAs, so NFAs are at least as powerful as DFAs. It is perhaps surprising that DFAs are as powerful as NFAs too! We can internalize the classic powerset construction to transform an NFA into a DFA that recognizes the same language. Note however that this loses some information. Throughout this process we may lose data such as the ambiguity of the grammar. Effectively we are forgetting the structure of our proof terms. Under the ``grammars as types'' lens, we are erasing the structure underlying the inhabitants of the type to simply mere existence --- this is a form of propostional truncation.

\steven{Worth spending some more time on the propositional truncation idea}

\steven{Put powerset construction here}

\subsection{TODO More Complicated Grammars}
Want to put PDA, CFGs, CSGs here. Basically, use this section to highlight the usefulness of the intuitionistic part of the contexts

\section{Future work}

\subsection{Implementation}



\subsection{Beyond Strings}

While parsing typically refers to the production of semantic objects
from their description as strings, many tasks in programming can be
viewed as parsing of more structured objects such as trees, trees with
binding structure or graphs. Fundamental to programming language
implementation is \emph{type checking}, analogous to language
recognition, or more generally \emph{typed elaboration}, analogous to
parsing, which produces a semantic object in addition to performing
some analysis. A type system could then be interpreted as a \emph{tree
grammar}. Our definition of string grammars as functors from
$\Sigma^*$ to $\Set$ can then be easily adapted to functors to $\Set$
from the set of \emph{trees} freely generated form some signature of
nodes. Then just as the set of string grammars inherits a monoidal
structure from the free monoid of strings using the Day convolution,
this set of tree grammars inherits a kind of weak algebraic structure
corresponding to the tree constructors. This suggests an even more
unusual form of bunched type theory, where the monoidal concatenation
and unit are replaced with context constructors corresponding to the
tree constructors.

\bibliographystyle{ACM-Reference-Format}
\bibliography{references}

\clearpage
\appendix

\section{Semantics in Sets and Grammars}

A model of Grammar Type Theory is a category with families equipped
with a functor $L : \mathcal C \to \textrm{GrLog}$ from the category
of contexts to the category of grammar logic models and strict morphisms.

First, we give the set theoretic model. We use the standard category
with families structure given by the category of sets and a cumulative
tower of universes.
%
Next, we define $L$ as follows:
\[ L(\Gamma) = \mathcal P \Sigma^*/\textrm{Disc} \Gamma \]
Where $\textrm{Disc} \Gamma$ is the presheaf that is constantly $\Gamma$.
Note that
\[ \mathcal P \Sigma^*/\textrm{Disc} \Gamma \cong (\Set/\Gamma)/\textrm{Disc} \Sigma^*\]
where $\textrm{Disc} \Sigma^*$ is the projection $\Gamma \times \Sigma^*
\to \Gamma$.  Then by the fundamental theorem of topos thoery, $\Set/\Gamma$ is
a topos. Since $\textrm{Disc} : \Set \to \Set/\Gamma$ is monoidal,
$\textrm{Disc} \Sigma^*$ carries a monoid structure in $\Set/\Gamma$ and
this induces a monoidal closed structure on $(\Set/\Gamma)/\textrm{Disc}
\Sigma^*$. Again by the fundamental theorem, it also has finite
products and coproducts and a natural numbers object, and so can
interpret least fixed points.\max{More details about constructing initial algebras?}

Then we can define the remaining type structure as follows. Unraveling
the above, a non-linear type $\Gamma \vdash X$ is interpreted as an
object of $\Set/\sem{\Gamma}$ and a linear type $\Gamma \vdash A$ is
interpreted as an object of $\mathcal P\Sigma^*/\textrm{Disc}\sem{\Gamma}$. We
define them below in indexed style:
\begin{enumerate}
\item $\sem{c} \gamma w = (w = c)$
\item $\sem{\LinPiTy x X A} \gamma w = \Pi(x:\sem{X}\gamma) \sem{A}(\gamma,x) w$
\item $\sem{\LinSigTy x X A} \gamma w = \Sigma(x:\sem{X}\gamma) \sem{A}(\gamma,x) w$
\item $\sem{G A} \gamma = \sem{A} \gamma \varepsilon$
\item $\sem{L_i}\gamma = (\mathcal P_i\Sigma^*/\textrm{Disc}\sem{\Gamma})_0$
\end{enumerate}

\section{Canonicity}

For the canonicity argument, we construct a second ``glued'' model
that contains a projection morphism to the syntactic model.

Let $S$ be the syntactic category with families for the type
theory. Define the glued category with families to be the comma category
% https://q.uiver.app/#q=WzAsNSxbMCwwLCJcXG1hdGhjYWwgRyJdLFswLDIsIlMiXSxbMiwyLCJcXFNldCJdLFsyLDAsIlxcU2V0Il0sWzEsMSwiXFxzd2Fycm93Il0sWzMsMl0sWzEsMl0sWzAsMV0sWzAsM11d
\[\begin{tikzcd}[ampersand replacement=\&]
	{\gluedNL} \&\& \Set \\
	\& \rotatebox[origin=c]{-45}{$\Downarrow$} \\
	S \&\& \Set
	\arrow[Rightarrow, no head, from=1-3, to=3-3]
	\arrow["{S(\cdot,-)}"', from=3-1, to=3-3]
	\arrow[from=1-1, to=3-1]
	\arrow[from=1-1, to=1-3]
\end{tikzcd}\]
This is a standard glued category for canonicity of dependent type
theory, and carries a category with families structure that we will
reuse from prior work for the (non-linear) universes,
$\Pi/\Sigma/\textrm{Id}$ \cite{coquand}. Concretely an object of
$\mathcal G$ is equivalent to a pair of a syntactic context $\Gamma$,
and a family $\hat \Gamma$ over (equivalence classes of) closing
substitutions for $\Gamma$.

Next we turn to interpreting the linear types and morphisms. For this
we define a functor $\gluedL : \gluedNL \to \textrm{MonCat}$ where
$\textrm{MonCat}$ is the category\footnote{in univalent foundations
this would be the category of \emph{strict} monoidal categories.} of
monoidal categories and lax monoidal functors.  We construct
$\gluedL(p : \hat \Gamma \to S(\cdot,\Gamma))$ as the following comma
category:

% https://q.uiver.app/#q=WzAsNixbMCwwLCJcXGdsdWVkTkwocCkiXSxbMCwyLCJMX1xcR2FtbWEiXSxbMSwyLCJcXG1hdGhjYWwgUCBcXFNpZ21hXiovXFx0ZXh0cm17RGlzY30oUyhcXGNkb3QsXFxHYW1tYSkpIl0sWzIsMiwiXFxtYXRoY2FsIFAgXFxTaWdtYV4qL1xcdGV4dHJte0Rpc2N9KFxcaGF0XFxHYW1tYSkiXSxbMiwwLCJcXG1hdGhjYWwgUCBcXFNpZ21hXiovXFx0ZXh0cm17RGlzY30oXFxoYXRcXEdhbW1hKSJdLFsxLDEsIlxccm90YXRlYm94W29yaWdpbj1jXXstNDV9e1xcRG93bmFycm93fSJdLFs0LDMsIiIsMCx7ImxldmVsIjoyLCJzdHlsZSI6eyJoZWFkIjp7Im5hbWUiOiJub25lIn19fV0sWzAsNF0sWzAsMV0sWzEsMiwiTiIsMl0sWzIsMywiXFx0ZXh0cm17RGlzY30ocF4qKSIsMl1d
\[\begin{tikzcd}[ampersand replacement=\&]
	{\gluedNL(p)} \&\& {\mathcal P \Sigma^*/\textrm{Disc}(\hat\Gamma)} \\
	\& {\rotatebox[origin=c]{-45}{$\Downarrow$}} \\
	{L_\Gamma} \& {\mathcal P \Sigma^*/\textrm{Disc}(S(\cdot,\Gamma))} \& {\mathcal P \Sigma^*/\textrm{Disc}(\hat\Gamma)}
	\arrow[Rightarrow, no head, from=1-3, to=3-3]
	\arrow[from=1-1, to=1-3]
	\arrow[from=1-1, to=3-1]
	\arrow["N"', from=3-1, to=3-2]
	\arrow["{\textrm{Disc}(p^*)}"', from=3-2, to=3-3]
\end{tikzcd}\]

Where $N : L_\Gamma \to {\mathcal P
  \Sigma^*/\textrm{Disc}(S(\cdot,\Gamma))}$ is the ``dependent nerve''
of the inclusion $\lceil\cdot\rceil : \Sigma^* \to L_\Gamma$ of
strings into linear contexts defined in indexed style as
\[ N(\Gamma \vdash A) \gamma w = \lceil w \rceil \vdash \cdot :  \simulsubst A \gamma \]
Then observe that $N$ and ${\textrm{Disc}(p^*)}$ are lax monoidal and
${\mathcal P \Sigma^*/\textrm{Disc}(\hat\Gamma)}$ is a topos so the
criteria for Hyland and Schalk's gluing construction apply, defining a
biclosed monoidal structure on $\gluedNL(p)$ as well as finite
(co-)products. Further, these constructions are all stable under
reindexing in $\gluedNL$, and so interpret the dependent linear typing
constructions for $I,\otimes,0,\oplus,\top,\amp,\lto,\tol$.

Concretely, an object of $\gluedNL(p)$ is equivalent to a pair of a
non-linear type $\Gamma \vdash A$ and a formal grammar $\hat A$
indexed by $\hat \Gamma$ with a projection function from $\hat A$ to
$N(A)$. We can think of this presheaf $\hat A$ as the presheaf of
canonical forms of $\simulsubst A \gamma$ for closing substitutions
$\gamma$ in contexts of the form $\lceil w \rceil$.

All that remains in this model is to define the semantics of the
non-standard connectives. We define them in an indexed style. First we
define the interpretation of a non-linear type $\Gamma \vdash X$ as a
family of sets $\hat X$ indexed by a closing substitution $\gamma :
S(\cdot,\Gamma)$, a semantic substitution $\hat \gamma : \hat
\Gamma(\gamma)$ and a closed term $\cdot \vdash e : \simulsubst A
\gamma$. Second, we define a linear type $\Gamma \vdash A$ as a family
$\hat A$ of grammars indexed by $\gamma,\hat\gamma$ as above along
with a morphism $\pi : \hat A \gamma \hat \gamma \to N(\simulsubst A
\gamma)$.

\begin{enumerate}
\item $\hat{c} \gamma \hat \gamma w = (w = c)$ and $\pi(*) = \lceil c \rceil = x_0:c \vdash x_0:c$
\item $\widehat{(\LinSigTy x X A)} \gamma \hat \gamma w = \Sigma(\cdot \vdash e : \simulsubst X \gamma)\Sigma(\hat x: \hat X \gamma \hat \gamma e) \hat{A}(\gamma,e)(\hat\gamma,\hat x) w$
\item $\sem{\LinPiTy x X A} \gamma w = \Sigma(x:\simulsubst X \gamma;\lceil w\rceil \vdash e:\simulsubst A \gamma)\Pi(\cdot \vdash e' : \simulsubst X \gamma)\Pi(\hat x:\hat{X}\gamma\hat\gamma x)\Sigma(\hat a : \hat A (\gamma,e')(\hat\gamma,\hat x) \times (\pi(\hat a) = \subst e {e'} x))$
  and $\pi(e,f) = \dlamb x.e$
\item $\widehat{G A} \gamma \hat \gamma e = \Sigma(\hat a: \hat{A} \gamma \hat\gamma \varepsilon) (p\gamma\hat\gamma \varepsilon(\hat a) = \mathsf{G}^{-1}e)$
\item $\sem{L_i}\gamma = \gluedNLUniv(\gamma)$
\end{enumerate}

Thus we have defined a second model $\gluedNL,\gluedL$ of the type
theory, with a morphism to the syntactic model. By the initiality of
the syntactic model, there is a morphism back to the glued model which
is a section of this projection. For purely positive types, we can
observe by induction on the type structure that the total space of the
family is isomorphic to the set-theoretic semantics. Further, the
projection above defines a section of the families. Finally, by
induction on formulae again we see that this projection is also a
section and therefore an isomorphism, establishing the canonicity
result as stated in Section\ref{section:canonicity}.
\end{document}
