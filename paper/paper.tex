\documentclass[sigconf,anonymous,review,screen]{acmart}
\usepackage{mathpartir}
\usepackage{tikz-cd}
\usepackage{enumitem}
\usepackage{wrapfig}
\usepackage{fancyvrb}
\usepackage{hyperref}
\usepackage{cleveref}
\usepackage{stmaryrd}

\newcommand{\sem}[1]{\llbracket{#1}\rrbracket}
\newcommand{\cat}[1]{\mathbf{#1}}
\newcommand{\lto}{\multimap}
\newcommand{\tol}{\mathrel{\rotatebox[origin=c]{180}{$\lto$}}}
\newcommand{\Set}{\mathbf{Set}}
\newcommand{\Gr}{\mathbf{Gr}}
\newcommand{\Type}{\mathbf{Type}}
\newcommand{\Prop}{\mathbf{Prop}}
\newcommand{\Bool}{\mathbf{Bool}}

\newcommand{\gluedNL}{{\mathcal G}_S}
\newcommand{\gluedNLUniv}{{\mathcal G}_{S,i}}
\newcommand{\gluedL}{{\mathcal G}_L}

\newcommand{\simulsubst}[2]{#1\{#2\}}
\newcommand{\subst}[3]{\simulsubst {#1} {#2/#3}}
\newcommand{\letin}[3]{\mathsf{let}\, #1 = #2 \, \mathsf{in}\, #3}
\newcommand{\lamb}[2]{\lambda #1.\, #2}
\newcommand{\lamblto}[2]{\lambda^{{\lto}} #1.\, #2}
\newcommand{\lambtol}[2]{\lambda^{{\tol}} #1.\, #2}
\newcommand{\dlamb}[2]{\overline{\lambda} #1.\, #2}
\newcommand{\app}[2]{#1 \, #2}
\newcommand{\applto}[2]{#1 \mathop{{}^{\lto}} #2}
\newcommand{\apptol}[2]{#1 \mathop{{}^{\tol}} #2}
\newcommand{\PiTy}[3]{\Pi #1 : #2.\, #3}
\newcommand{\SigTy}[3]{\Sigma #1 : #2.\, #3}
\newcommand{\LinPiTy}[3]{\widebar\Pi #1 : #2.\, #3}
\newcommand{\LinSigTy}[3]{\widebar\Sigma #1 : #2.\, #3}
\newcommand{\amp}{\mathrel{\&}}
\newcommand{\GrTy}{\mathsf{Gr}}

\newcommand{\ctxwff}[1]{#1 \,\, \mathsf{ok}}
\newcommand{\ctxwffjdg}[2]{#1 \vdash #2 \,\, \mathsf{type}}
\newcommand{\linctxwff}[2]{#1 \vdash #2 \,\, \mathsf{ok}}
\newcommand{\linctxwffjdg}[2]{#1 \vdash #2 \,\, \mathsf{linear}}

\newif\ifdraft
\drafttrue
\newcommand{\steven}[1]{\ifdraft{\color{orange}[{\bf Steven}: #1]}\fi}
\renewcommand{\max}[1]{\ifdraft{\color{blue}[{\bf Max}: #1]}\fi}
\newcommand{\pedro}[1]{\ifdraft{\color{red}[{\bf Pedro}: #1]}\fi}
\newcommand{\pipe}{\,|\,}

%% Rights management information.  This information is sent to you
%% when you complete the rights form.  These commands have SAMPLE
%% values in them; it is your responsibility as an author to replace
%% the commands and values with those provided to you when you
%% complete the rights form.
\setcopyright{acmcopyright}
\copyrightyear{2018}
\acmYear{2018}
\acmDOI{XXXXXXX.XXXXXXX}

%% These commands are for a PROCEEDINGS abstract or paper.
\acmConference[Woodstock '18]{Woodstock '18: ACM Symposium on Neural
  Gaze Detection}{June 03--05, 2018}{Woodstock, NY}
\acmBooktitle{Woodstock '18: ACM Symposium on Neural Gaze Detection,
  June 03--05, 2018, Woodstock, NY}
\acmPrice{15.00}
\acmISBN{978-1-4503-XXXX-X/18/06}


\begin{document}

\title{Formal Grammars as Functors and Formal Grammars as Types in Non-commutative Linear-Non-Linear Type Theory}
\author{Steven Schaefer}
\affiliation{
  \department{Electrical Engineering and Computer Science}
  \institution{University of Michigan}
  \country{USA}
}
\email{stschaef@umich.edu}

\author{Max S. New}
\affiliation{
  \department{Electrical Engineering and Computer Science}
  \institution{University of Michigan}
  \country{USA}
}
\email{maxsnew@umich.edu}

\author{Pedro H. Azevedo de Amorim}
\affiliation{
  \department{Department of Computer Science}
  \institution{University of Oxford}
  \country{UK}
}
\email{pedro.azevedo.de.amorim@cs.ox.ac.uk}

\begin{abstract}
  We propose a semantic framework for the study of formal
  grammars. First, we provide a syntax-independent notion of formal
  grammar as a function from strings to sets, generalizing the
  familiar notion of a language as a set of strings. The set of such
  grammars naturally form a very rich category whose notion of
  isomorphism corresponds to strong equivalence of grammars, and for
  which many language-theoretic constructs can be defined using
  universal properties.

  Based on these category-theoretic constructions we propose a new
  syntactic formalism for formal grammars: a version of dependent
  linear-non-linear type theory in which the tensor product is not
  commutative. The linear types can be interpreted as grammars, and
  linear terms as parse transformers. The non-dependent fragment of
  this type theory is already enough to express regular/context-free
  expressions as well as finite automata, with the isomorphism between
  regular languages and automata definable as linear terms. This
  generalizes the language theoretic formalisms such as Kleene
  algebras to incorporate ambiguity. By incorporating a dependency of
  linear types on non-linear types, we can further express indexed
  grammars, pushdown automata and Turing machines. This provides a new
  form of ``logical characterization'' of grammar classes based on a
  substructural logic rather than ordinary first-order logic. Further,
  the type theory can not only express the grammars as types, but
  equivalences between grammars and parsers as terms of the grammar.

  We give this type theory a semantics in the category of grammars and
  prove a canonicity theorem that shows that every term in a context
  corresponding to a fixed string is equal to a term in a canonical
  form encoding a parse tree of that string.
\end{abstract}

\maketitle

\section{A Syntax-Independent Notion of Syntax}

The theory of formal languages and parsing is one of the oldest and
most thoroughly developed areas of theoretical computer science. A
prominent topic in the 1950s and 60s led to a series of remarkable
developments: Chomsky's hierarchy of grammar formalisms, practical
algorithms for parsing of regular and context-free grammars and
variants, as well as implementations of practical tools for the
generation of efficient parsers.

A central notion is that of a \emph{formal language} $L$ over an
alphabet $\Sigma$ as simply a \emph{subset} of the set of strings $L
\subseteq \Sigma^*$. This definition is especially useful as it gives
a semantics to formal grammars that is completely independent of any
particular syntactic grammar formalism: any new notion of grammar can
be given a semantics in this common framework and it provides a
precise mathematical speicification for implementing a
\emph{recognizer} of a language and comparing the power of different
formalisms by demonstrating what languages can be formalized within
it. This also allows for completely \emph{language theoretic}
formulations of language classes: e.g., the regular \emph{languages}
can be characterized as those that have finitely many derivatives
\cite{brzozowski-or-myhill-nerode-idk}. This notion is remarkable
because it makes no reference to any specific syntactic formalism for
defining languages such as regular expressions or finite automata.

Formal language theory alone is not sufficient as a specification of a
parser: a language $L \subseteq \Sigma^*$ is equivalently defined as
its indicator function $\chi_L : \Sigma^* \to \Prop$ mapping each
string $w$ to the proposition that it is in the language $w \in
L$. This can then serve as the specification for a language
\emph{recognizer}: a program $r$ of type $\Sigma^* \to \Bool$ such
that $r(w) = \texttt{true}$ if and only if $\chi_L(w)$ holds, but
recognition is insufficient for most tasks for which the theory of
parsing was developed: the production of \emph{semantic} structures
from synactic descriptions in linguistics, compilation or
deserialization. The output of a parser is not just a boolean but a
\emph{parse tree}\footnote{such parse trees are usually not
materialized in memory, as the construction of the parse tree is fused
with application of semantic actions, which can be seen as a kind of
fold over the generated tree.}.

Due to this limitation, parsers are typically specified not by a
formal language, but by some \emph{formal grammar}, which specifies
not just \emph{which} strings are in the language but specifies what
are the \emph{parse trees} for the string. However unlike formal
language theory, there is no common universal notion of what
constitutes a formal grammar, but rather many syntactic systems such
as Chomskyan(?)  grammars, Thue systems, Montague grammars, etc which
are all syntactic presentations of the same underlying idea of an
abstract specification for parsing. The most common of these is the
Chomsky hierarchy of \emph{generative} grammars which specify parse
trees using \emph{derivations} of the string from a set of rewrite
rules, the parse tree encoding which rules were used.

Our first contribution is conceptual: we propose a simple,
syntax-independent definition of \emph{formal grammar}:
\begin{definition}
  A \emph{formal grammar} over an alphabet $\Sigma$ is a function
  $\Sigma^* \to \Set$.
\end{definition}
We say that a grammar $G$ associates to every string $w$ the set $Gw$
of \emph{parses} of that string. While we will work informally in this
paper, the collection $\Set$ can be formalized in many different
logical foundations: as a universe in type theory, a proper class in
set theory, etc. We argue that this is already a common intuition
latent in much prior work on grammars, especially when comparing
different formalisms for equivalence such as \cite{??}. This notion is
also completely natural in the context of constructive type theories
such as Agda which until recently did not include a universe of
propositions, and so for example in Elliott (\cite{??}) a formal
language was defined as a function to the universe $\Type$ with little
comment.

Every formal grammar $G$ induces a formal language, which can be seen
most easily by using the characteristic function formulation: we can
``squash'' any set into the proposition that it is inhabited, so
$\chi_G(w) = ||G(w)||$ which defines a subset $L_G = \{ w \in
\Sigma^*| G(w) \mathrm{inhabited}\}$.

Formal languages naturally arrange themselves into a partial order by
using the subset inclusion as the ordering. Formal grammars naturally
arrange themselves into a \emph{category} where a morphism $\alpha : G
\to H$ is given by a \emph{family} of functions
\[ \alpha^w : Gw \to Hw \]
for every string $w \in \Sigma^*$. The intuition is that a morphism of
grammars is a \emph{translation of parses}: a $G$-parse of $w$ gets
transformed to an $H$-parse of $w$. This then induces a notion of
\emph{isomorphism} of grammars: two grammars are equivalent if there
is a bijective translation of parses, precisely capturing the notion
of a \emph{strong equivalence} of grammars.

This category-theoretic framework can be further used to describe and
compare different notions of formal grammar. We can define a
\emph{notion of grammar} to be a category (or perhaps just groupoid)
paired with a functor into $\Set^{\Sigma^*}$. This naturally forms a
2-category the slice 2-category over $\Set^{\Sigma^*}$ and equivalence
in this 2-category defines a sensible notion of \emph{strong
equivalence of grammar formalisms} (TODO: check details).

The category of grammars is quite rich in structure, which can be most
easily observed from the fact that it is equivalently defined as the
category of functors $\Set^{\Sigma^*}$ where $\Sigma^*$ is here viewed
as a discrete category, that is, the objects are strings and only
morphisms are identity morphisms. Such functor categories, often
called presheaf categories, are incredibly rich in structure, which we
will exploit in section \ref{blah} to succinctly define the semantics
of regular and context-free grammars.

\section{Kleene Category}

Kleene algebras are an important tool in the theory of regular
languages. More broadly, they serve as a theoretical substrate to
studying various kinds of formal languages. Formally, they are a tuple
$(A, +, \cdot, (-)^*, 1, 0)$, where $A$ is a set, $+$ and $\cdot$
are binary operations over $A$, $(-)^*$ is a function over $A$, and
$1$ and $0$ are constants. These structures satisfy the axoims depicted
in Figure~\ref{fig:axioms}.

\begin{figure}
  \begin{align*}
    x + (y + z) &= (x + y) + z & x + y &= y + x\\
    x + 0 &= x & x + x &= x\\
    x(yz) &= (xy)z & x1 &= 1x = x\\
    x(y + z) &= xy + xz & (x + y)z &= xz + yz\\
    x0 &= 0x = x & & \\
    1 + aa^* &\leq a^* & 1 + a^*a &\leq a^*\\
     b + ax \leq x &\implies a^*b \leq x &  b + xa \leq x &\implies ba^* \leq x
  \end{align*}
  \label{fig:axioms}
  \caption{Kleene algebra axioms}
\end{figure}

The addition operation can be used to define the partial order
structure $a \leq b$ if $a + b = b$. In the theory of formal languages
this order structure can be used to model language containment. In this
section, we want to categorify the concept of Kleene algebra and
build on top of it in order to define an abstract theory of parsing.
We start by defining \emph{Kleene categories}.

\begin{definition}
  A Kleene category is a distributive monoidal category $\cat{K}$
  such that for every objects $A$ and $B$, the endofunctors $F_{A, B}
  = B + A \otimes X$ and $G_{A, B} = B + X \otimes A$ have initial
  algebras (denoted $\mu X.\, F_{A, B}(X)$) such that $B \otimes (\mu
  X.\, F_{A, 1}) \cong \mu X.\, F_{A, B}(X)$ and the analogue isomorphism
  for $G_{A,B}$ also holds.
\end{definition}

As a sanity check, note that Kleene algebras are indeed examples of
Kleene categories.

\begin{example}
  Every Kleene algebra, seen a posetal category, is a Kleene category.
\end{example}

An unexpected example comes from the theory of substructural logics.

\begin{example}
  The opposite category of every Kleene category is a model of a variant of
  conjunctive ordered logic, where the Kleene star plays the role of the ``of
  course'' modality from substructural logics which allows hypotheses to
  be discarded or duplicated.
\end{example}

The proposed axioms are a direct translation of the Kleene algebra
axioms to a categorical setting. Its most unusual aspect is the
axiomatization of the Kleene star as a family of initial algebras
satisfying certain isomorphisms. If the Kleene category $\cat{K}$ has
more structure, then these isomorphisms hold ``for free''.

\begin{theorem}
  \label{th:kleeneclosed}
  Let $\cat{K}$ be a Kleene category such that it is also monoidal
  closed.  Then, the initial algebras isomorphisms hold automatically.
\end{theorem}
\begin{proof}
  We prove this by the unicity (up-to isomorphism) of initial
  algebras. Let $[hd, tl]: 1 + (\mu X.\, F_{A, 1}(X)) \otimes A \to
  (\mu X.\, F_{A, 1}(X))$ be the initial algebra structure of $(\mu
  X.\, F_{A, 1}(X))$ and consider the map $[hd, tl] : B + B \otimes
  (\mu X.\, F_{A, 1}(X)) \otimes A \to B\otimes (\mu X.\, F_{A,
    1}(X))$.

  Now, let $[f,g] : B + A \otimes Y \to Y$ be an $F_{A,B}$-algebra and
  we want to show that there is a unique algebra morphism $h : B
  \otimes \mu X.\, F_{A,1} \to Y$. We can show existence and uniqueness
  by showing that the diagram on the left commute if, and only if,
  the diagram on the right commutes:

  This equivalence follows by using the adjunction structure given
  by the monoidal closed structure of $\cat{K}$. A completely analogous
  argument for $G_{A,B}$ also holds.
\end{proof}

This result feels similar in spirit to the definition of action
algebras, which are algebras where the product also has adjoint
operations which results in the Kleene star being more easily
axiomatized \cite{kozen1994}.

We are now ready to prove that our concept of formal grammars fits
nicely within our categorical framework. We start by presenting a
well-known construction from presheaf categories.

\begin{definition}
  Let $\cat{C}$ be a locally small monoidal category and $F$, $G$ be
  two functors $\cat{C} \to \Set$. Their Day convolution tensor
  product is defined as the following coend formula:
  \[
  (F \otimes_{Day} G)(x) = \int^{(y,z) \in \cat{C}\times\cat{C}}\cat{C}(y\otimes z, x) \times F(y) \times G(z) 
  \]
  Dually, its internal hom is given by the following end formula:
  \[
  (F \lto_{Day} G)(x) = \int_{y} \Set(F(y), G(x \otimes y))
  \]
\end{definition}

\begin{lemma}[\cite{day1970}]
  Under the assumptions above, the presheaf category $\Set^{\cat{C}}$ is
  monoidal closed.
\end{lemma}

%% \begin{theorem}
%%   Let $\cat{K}$ be a Kleene category and $A$ a discrete category.
%%   The functor category $[A, \cat{K}]$.
%%   (HOW GENERAL SHOULD THIS THEOREM BE? BY ASSUMING ENOUGH STRUCTURE,
%%   E.G. K = Set, THIS THEOREM BECOMES SIMPLE TO PROVE)
%% \end{theorem}
\begin{theorem}
  If $\cat{C}$ is a locally small monoidal category, then
  $\Set^{\cat{C}}$ is a Kleene category.
\end{theorem}
\begin{proof}

  By the lemma above, $\Set^{\cat{C}}$ is monoidal closed, and since it
  is a presheaf category, it has coproducts. Furthermore, the tensor
  is a left adjoint, i.e. it preserves colimits and, therefore, it is
  a distributive category.

  As for the Kleene star, since presheaf categories admit small colimits,
  the initial algebra of the functors $F_{A,B}$ and $G_{A,B}$ can be
  defined as the filtered colimit of the diagrams:

  From Theorem~\ref{th:kleeneclosed} it follows that these initial
  algebras satisfy the required isomorphisms and this concludes the
  proof.
\end{proof}

\begin{corollary}
  For every alphabet $\Sigma$, the presheaf category $\Set^{\cat{\Sigma^*}}$
  is a Kleene category.
\end{corollary}
\begin{proof}
  Note that string concatenation and the empty string make the
  discrete category $\Sigma^*$ a strict monoidal category.
\end{proof}

Much like in the posetal case, the abstract structure of a 2-Kleene
algebra is expressive enough to synthetically reason about formal
languages. A significant difference between them is that while Kleene
algebras can reason about language containment, 2-Kleene algebras can
reason about parsing as well.

For the rest of the paper we will use the presheaf category
$\Set^{\cat{\Sigma^*}}$ as a concrete model that will serve as our
guide when expanding abstract formalism presented in this section so
that it can handle more classes of languages beyond regular ones.

\paragraph{Beyond Simple Types}

\begin{definition}
  A model is a locally Cartesian category equipped with an internal Kleene category.
\end{definition}

\begin{theorem}
  The presheaf category $\Set^{\cat{\Sigma^*}}$ is a model.
\end{theorem}

\section{Related work}

\paragraph{Kleene Algebra}

Since the early works in the theory of formal languages, Kleene
algebras have played an important role in its development. They
generalize the operations known from regular languages by introducing
operations generalizing language composition, language union and the
Kleene star.  More generally, they are defined as inequational theory
where the inequality is meant to capture language containment. This
theory is extremely successful, having found applications in algebraic
path problems, theory of programming languages, compiler optimizations
and more.

A frequently fruitful research direction is exploring varying
extensions of Kleene algebras, Kleene algebra with tests (KAT) being
one of the most notable ones. Our approach is radically different from
most extensions, which usually aim at modifying or adding new
operations to Kleene algebras, but still keeping it as an inequational
theory. By adopting a category-theoretic treatment and allowing the
``order structure'' to encode more information than merely
inequalities, we were able to extend Kleene algebra to reason about
parsing as well.

\paragraph{TODO: more stuff}

%% That paper with Fritz Henglein about regexes as types

%% Vaughan Pratt on the residual operator


%% \section{Universal Constructions in the Category of Grammars}

%% Predicate BI: bicartesian closed, monoidal biclosed, omega-colimits
%% and therefore certain initial algebras



\section{Non-commutative Linear-Non-Linear Type Theory as a Syntax for Grammars}
\label{sec:typetheory}
\subsection{Syntax}
Judgments: $\Delta$ set ctx, $\Delta \vdash A$ set, $\Delta \vdash M :
A$ term, $\Delta \vdash \Gamma$ grammar ctx, $\Delta \vdash G$
grammar, $\Delta\pipe \Gamma \vdash p : G$ parse term.

\begin{figure}
  \label{fig:structjdg}
  \begin{mathpar}
    \inferrule{~}{\ctxwff \cdot}
    \and
    \inferrule{\ctxwff \Gamma \\ \ctxwffjdg \Gamma X}{\ctxwff {\Gamma, x : X}}
    
    \\

    \inferrule{~}{\linctxwff \Gamma \cdot}
    \and
    \inferrule{\linctxwff \Gamma \Delta \\ \linctxwffjdg \gamma A}{\linctxwff \Gamma {\Delta, a : A}}

    \\

    \inferrule{\Gamma \vdash X : U_i}{\ctxwffjdg \Gamma X}

    \and
    
    \inferrule{\Gamma \vdash A : L_i}{\linctxwffjdg \Gamma A}

    \\

    \inferrule{\Gamma \vdash X \equiv Y : U_i}{\ctxwffjdg \Gamma {X\equiv Y}}

    \and
    
    \inferrule{\Gamma \vdash A \equiv B : L_i}{\linctxwffjdg \Gamma {A \equiv B}}

  \end{mathpar}
  \caption{Structural judgements}
\end{figure}

\begin{figure}
  \label{fig:typewf}
  \begin{mathpar}
    \inferrule{~}{\Gamma \vdash U_i : U_{i+1}}
 %   
    \and
%
    \inferrule{~}{\Gamma \vdash L_i : U_{i+1}}
%
    \\
%
    \inferrule{\Gamma \vdash X : U_i \\ \hspace{-0.1cm} \Gamma, x : X \vdash Y : U_i}{\Gamma \vdash \PiTy x X Y : U_i }%
%
    \and
%
    \inferrule{\Gamma\vdash X : U_i \\ \hspace{-0.1cm} \Gamma, x : X \vdash Y : U_i}{\Gamma \vdash \SigTy x X Y : U_i}
%
    \\
%
    \inferrule{~}{\Gamma \vdash 1 : U_i}
%
    \and
%
    \inferrule{\Gamma \vdash A : L_i}{\Gamma \vdash G A : U_i}
%
    \\
%
    \inferrule{~}{\Gamma \vdash I : L_i}
 %   
    \and
%
    \inferrule{\Gamma \vdash A : L_i \\ \hspace{-0.1cm}\Gamma \vdash B : L_i}{\Gamma \vdash A \otimes B : L_i}
%
    \and
%
    \inferrule{\Gamma \vdash A : L_i \\ \hspace{-0.1cm}\Gamma \vdash B : L_i}{\Gamma \vdash A \lto B : L_i}
%
    \and
%
    \inferrule{\Gamma \vdash A : L_i \\ \hspace{-0.1cm}\Gamma \vdash B : L_i}{\Gamma \vdash B \tol A : L_i}
%
    \\
%
    \inferrule{\Gamma \vdash X : U_i \\ \Gamma, x : X \vdash A : L_i}{\Gamma \vdash \LinPiTy x X A : L_i}
%
    \and
%
    \inferrule{\Gamma \vdash X : U_i \\ \Gamma, x : X \vdash A : L_i}{\Gamma \vdash \LinSigTy x X A : L_i}
%
    \\
%
    \inferrule{~}{\Gamma \vdash \top : L_i}
%
    \and
%
    \inferrule{\Gamma \vdash A : L_i \\ \Gamma \vdash B : L_i}{\Gamma \vdash A \amp B : L_i}
%
    \\
%
    \inferrule{\Gamma, x : L_i \vdash A : L_i \and A \textrm{ strictly positive}}{\Gamma \vdash \mu x.\, A : L_i}
  \end{mathpar}
  \caption{Type well-formedness}
\end{figure}

\begin{figure}
  \label{fig:inttyping}
  \begin{mathpar}
  \inferrule{~}{\Gamma, x : X, \Gamma' \vdash x : X}
  %
  \and
  %
  \inferrule{\Gamma \vdash e : Y \quad \ctxwffjdg \Gamma {X \equiv Y}}{\Gamma \vdash e : X}
  %
  \\\
  %
  \inferrule{~}{\Gamma \vdash () : 1}
  %
  \and
  %
  \inferrule{\Gamma \vdash e : X \\ \Gamma \vdash e : \subst Y e x}{\Gamma \vdash (e, e') : \SigTy x X Y}
  %
  \\
%
  \inferrule{\Gamma \vdash e : \SigTy x X Y}{\Gamma \vdash \pi_1\, e : X}
  %
  \and
  %
  \inferrule{\Gamma \vdash e : \SigTy x X Y}{\Gamma \vdash \pi_2\, e : \subst Y {\pi_1\, e} x}
  \and
  \inferrule{\Gamma, x : X \vdash e : Y}{\Gamma \vdash \lamb x e : \PiTy x X Y}
  %
  \and
  %
  \inferrule{\Gamma \vdash e : \PiTy x X Y \\ \Gamma \vdash e' : X}{\Gamma \vdash \app e {e'} : \subst Y {e'} x}
  %
  \\
  %
  \inferrule{\Gamma \vdash e \equiv e' : X}{\Gamma \vdash \mathsf{refl} : e =_X e'}
  \and
  \inferrule{\Gamma \mid \cdot \vdash e : A}{\Gamma \vdash \mathsf G e : \mathsf G A}
  \end{mathpar}
  \caption{Intuitionistic typing}
\end{figure}

\begin{figure}
  \label{fig:linsyntax}
  \begin{mathpar}
    \inferrule{~}{\Gamma \mid a : A \vdash a : A}
    \and
    \inferrule{\Gamma \mid \Delta \vdash e : B \\ \linctxwffjdg \Gamma {A \equiv B}}{\Gamma \mid \Delta \vdash e : A}
    %
    \\
    %
    \inferrule{~}{\Gamma \mid \cdot \vdash () : I}
    \and
    \inferrule{\Gamma \mid \Delta \vdash e : I \\ \Gamma \mid \Delta_1',\Delta_2' \vdash e' : C}{\Gamma \mid \Delta_1',\Delta,\Delta_2' \vdash \letin {()} e {e'} : C}
    %
    \\
    %
    \inferrule{\Gamma \mid \Delta \vdash e : A \\ \Gamma \mid \Delta' \vdash e' : B}{\Gamma \mid \Delta, \Delta' \vdash e \otimes e' : A \otimes B}
    %
    \\
    %
    \inferrule{\Gamma \mid \Delta \vdash e : A \otimes B \\ \Gamma \mid \Delta'_1, a : A, b : B, \Delta'_2 \vdash e'}{\Gamma \mid  \Delta_1', \Delta, \Delta'_2 \vdash \letin {a \otimes b} e {e'}}
    \\
    %
    \inferrule{\Gamma \mid \Delta, a : A \vdash e : B}{\Gamma \mid \Delta \vdash \lamblto a e : A\lto B}
    \and
    \inferrule{\Gamma \mid \Delta' \vdash e' : A \\ \Gamma \mid \Delta \vdash e : A \lto B}{\Gamma \mid \Delta', \Delta \vdash \applto {e'} {e} : B}
    \\
    %
    \inferrule{\Gamma \mid a : A, \Delta \vdash e : B}{\Gamma \mid \Delta \vdash \lambtol a e : B\tol A}
    \and
    \inferrule{\Gamma \mid \Delta \vdash e : B \tol A \\ \Gamma \mid \Delta' \vdash e' : A}{\Gamma \mid \Delta, \Delta' \vdash \apptol e {e'} : B}
    %
    \\
    %
    \inferrule{\Gamma, x : X \mid \Delta  \vdash e : A}{\Gamma \mid \Delta \vdash \dlamb x e : \LinPiTy x X A}
    \and
    \inferrule{\Gamma \mid \Delta \vdash e : \LinPiTy x X A \\ \Gamma \vdash e' : X}{\Gamma \mid \Delta \vdash \app e {e'} : \subst A {e'} x}
    %
    \\
    %
    \inferrule{\Gamma \vdash e : X \quad \Gamma \mid \Delta \vdash e' : \subst A e x}{\Gamma \mid \Delta \vdash (e, e') : \LinSigTy x X A}
    %
    \\
    %
    \inferrule{\Gamma \mid \Delta \vdash e : \LinSigTy x X A \quad \Gamma, x : X \mid \Delta'_1, a : A, \Delta'_2 \vdash e' : C}{\Gamma\mid \Delta'_1, \Delta, \Delta'_2 \vdash \letin {(x, a)} e {e'}: C}
    %
    \\
    %
    \inferrule{~}{\Gamma \mid \Delta \vdash () : \top}
    %
    \\
    %
    \inferrule{\Gamma \mid \Delta \vdash e_1 : A_1 \quad \Gamma \mid \Delta \vdash e_2 : A_2}{\Gamma \mid \Delta \vdash (e_1, e_2) : A_1 \amp A_2}
    \and
    \inferrule{\Gamma \mid \Delta \vdash e : A_1 \amp A_2 }{\Gamma \mid \Delta \vdash \pi_i \, e : A_i}
    %
    \\
    %
    \inferrule{\Gamma \vdash e : \mathsf{G} A}{\Gamma \mid \cdot \vdash \mathsf{G}^{-1}\, e : A}
    %
    \\
    %
    \inferrule{\Gamma; \Delta \vdash e : \subst A {\mu x.\, A} x}{\Gamma; \Delta \vdash \mathsf{cons}\, e : \mu x.\, A}
    \and
    \inferrule{\Gamma;\Delta\vdash e' : \mu x.\,A \and \Gamma; a:\subst A B x \vdash e : B}{\Gamma;\Delta\vdash \mathsf{fold}(a.e)(e') : B}
  \end{mathpar}
  \caption{Linear typing}
\end{figure}

\pedro{Need to figure out the rules for inductive types and write down
  the judgemental equality rules}

\subsection{Semantics in Grammars}

For illustrative purposes, we give a brief overview of the concrete semantics in sets and grammars:
\begin{enumerate}
\item A non-linear context $\Gamma$ denotes a set $\sem{\Gamma}$
\item A non-linear type $\Gamma \vdash X : U_i$ denotes a family of sets $\sem{X} : \sem{\Gamma} \to \Set_i$
\item A non-linear term $\Gamma \vdash e : X$ denotes a section $\sem{e} : \Pi(\gamma:\sem{\Gamma})\,\sem{X}\,\gamma$
\item Linear contexts $\Gamma \vdash \Delta$ and types $\Gamma \vdash A : L_i$ both denote families of grammars $\sem{\Gamma}\to \Gr_i$
\item A linear term $\Gamma; \Delta \vdash e : A$ denotes a family of parse transformers $\sem{e} : \Pi(\gamma:\sem{\Gamma})\Pi(w:\Sigma^*)\,\sem{\Delta}\gamma w \Rightarrow \sem{A}\gamma w)$
\end{enumerate}
Most of the non-linear semantics is standard, we show just the $G$ type and the linear semantics:
\begin{enumerate}
\item $\sem{G A} \gamma = \sem{A} \gamma \varepsilon$
\item $\sem{\LinPiTy x X A} \gamma w = \Pi(x:\sem{X}\gamma) \sem{A}(\gamma,x) w$
\item $\sem{\LinSigTy x X A} \gamma w = \Sigma(x:\sem{X}\gamma) \sem{A}(\gamma,x) w$
\item $\sem{I} \gamma w = \{ () \pipe w = \epsilon \}$
\item $\sem{A \otimes B} \gamma w = \Sigma(w1,w2:\Sigma^*) w1w1 = w \wedge \sem{A} \gamma w_1 \times \sem{B} \gamma w_2$
\item $\sem{A \lto B} \gamma w = \Pi(w_a:\Sigma^*) A \gamma w_a \Rightarrow B\gamma (w_aw)$
\item $\sem{B \tol A} \gamma w = \Pi(w_a:\Sigma^*) A \gamma w_a \Rightarrow B\gamma (ww_a)$
\item $\sem{\mu x. A} \gamma = \mu (x:\Gr_i). \sem{A}(\gamma,x)$
\end{enumerate}

Propositional fragment: monoidal biclosed category that is also
bicartesian closed and has $\omega$-colimits. Call such a BIC.

Set theoretic fragment: category with families $\mathcal C$ + $\Pi,\Sigma,W$.

Combination: category with families $\mathcal C$ + a functor $L : C
\to \textrm{BIC}$.

Grammatical fragment: functor from $\mathcal C$ to the category of BI-models

Example: use $\Set$ as the CwF, use the functor $\Delta \mapsto
(\Set^{\Sigma^*})^\Delta$ as the model of the propositional fragment
using the pointwise-Day convolution monoidal structure.

\subsection{Canonicity}

Let $\cdot \vdash A$ be a closed linear type. Then there are
two obvious notions of what constitutes a ``parse'' of a string w
according to the grammar $A$:
\begin{enumerate}
\item On the one hand we have the set-theoretic semantics just
  defined, $\llbracket A \rrbracket \cdot w$
\item On the other hand, we can view the string $w = c_1c_2\cdots$ as
  a linear context $\lceil w \rceil = x_1:c_1,x_2:c_2,\ldots$ and
  define a parse to be a $\beta\eta$-equivalence class of linear terms $\cdot;
  \lceil w \rceil \vdash e : G$.
\end{enumerate}
It is not difficult to see that at least for the ``purely positive''
formulae (those featuring only the positive connectives
$0,+,I,\otimes,\mu, \overline\Sigma,c$) that
every element $t \in \llbracket A \rrbracket w$ is a kind of tree and
that the nodes of the tree correspond precisely to the introduction
forms of the type. However it is far less obvious that \emph{every}
linear term $\lceil w \rceil \vdash p : \phi$ is equal to some
sequence of introduction forms since proofs can include elimination
forms as well. To show that this is indeed the case we give a
\emph{canonicity} result for the calculus: that the parses for .

\begin{definition}
  A non-linear type $X$ is purely positive if it is built up using
  only finite sums, finite products and least fixed points.

  A linear type is purely positive if it is built up using only finite
  sums, tensor products, generators $c$, least fixed points and linear
  sigma types over purely positive non-linear types.
\end{definition}

\begin{definition}
  %% Let $X$ be a closed non-linear type. The closed elements $\textrm{Cl}(X)$ of $X$ are the definitional equivalence classes of terms $\cdot \vdash e : X$.

  Let $A$ be a closed linear type. The nerve $N(A)$ is a presheaf on
  strings that takes a string $w$ to the definitional equivalence
  classes of terms $\cdot; \lceil w\rceil \vdash e: N(A)$.
\end{definition}

\begin{theorem}[Canonicity]
  Let $A$ be a closed, purely positive linear type. Then there is an
  isomorphism between $\llbracket A\rrbracket$ and $N(A)$.
\end{theorem}
\begin{proof}
  We outline the proof here, more details are in the appendix. The
  proof proceeds first by a standard logical families construction
  that combines canonicity arguments for dependent type theory
  \cite{coquand,etc} with logical relations constructions for linear
  types \cite{hylandschalk}. It is easy to see by induction that the
  logical family for $A$, $\hat A$ is isomorphic to $\llbracket A
  \rrbracket$ and the fundamental lemma proves that the projection
  morphism $p : \hat A \to N(A)$ has a section, the canonicalization
  procedure. Then we establish again by induction that
  canonicalization is also a retraction by observing that introduction
  forms are taken to constructors.
\end{proof}


\begin{enumerate}
\item Every term $\lceil w \rceil \vdash p : G + H$ is equal to $\sigma_1q$ or $\sigma_2 r$ (but not both)
\item There are no terms $\lceil w \rceil \vdash p : 0$
\item If there is a term $\lceil w \rceil \vdash p : c$ then $w = c$ and $p = x$.
\item Every term $\lceil w \rceil \vdash p : G \otimes H$ is equal to $(q,r)$ for some $q,r$
\item Every term $\lceil w \rceil \vdash p : \epsilon$ is equal to $()$
\item Every term $\lceil w \rceil \vdash p : c$ is equal to $x:c$
\item Every term $\lceil w \rceil \vdash p : \mu X. G$ is equal to $\textrm{roll}(q)$ where $q : G(\mu X.G/X)$
\item Every term $\lceil w \rceil \vdash p : (x:A) \times G$ is equal
  to $(M,q)$ where $\cdot \vdash M : A$
\end{enumerate}

To prove this result we will use a logical families model. We give a
brief overview of this model concretely:
\begin{enumerate}
\item A context $\Gamma$ denotes a family of sets indexed by closing substitutions $\hat\Gamma : (\cdot \vdash \Gamma) \Rightarrow \Set_i$
\item A type $\Gamma \vdash X : U_i$ denotes a family of sets $\hat X : \Pi(\gamma:\cdot \vdash \Gamma) \hat\Gamma \Rightarrow (\cdot \vdash \simulsubst X \gamma) \Rightarrow \Set_i$
\item A term $\Gamma \vdash e : X$ denotes a section $\hat e : \Pi(\gamma)\Pi(\hat\gamma)\hat X \gamma \hat\gamma (\simulsubst e \gamma)$
\item A linear type $\Gamma \vdash A : L_i$ denotes a family of grammars $\hat A : \Pi(\gamma:\cdot\vdash\Gamma)\,\hat\Gamma \Rightarrow \Pi(w:\Sigma^*) (\cdot;\lceil w\rceil \vdash A[\gamma])\Rightarrow \Set_i$, and the denotation of a linear context $\Delta$ is similar.
\item A linear term $\Gamma;\Delta \vdash e : A$ denotes a function \[\hat e : \Pi(\gamma)\Pi(\hat\gamma)\Pi(w)\Pi(\delta : \lceil w \rceil \vdash \simulsubst \Delta \gamma) \hat\Delta \gamma \hat\gamma \delta \Rightarrow \hat A \gamma \hat\gamma w {(\simulsubst {\simulsubst e \gamma} \delta)}\]
\end{enumerate}
And some of the constructions:
\begin{enumerate}
\item $\widehat {(G A)} \gamma \hat\gamma e = \hat A \gamma \hat\gamma \varepsilon (G^{-1}e)$
\item $\widehat {(A \otimes B)} \gamma \hat\gamma w e = \Sigma(w_Aw_B = w)\Sigma(e_A)\Sigma(e_B) (e_A,e_B) = e \wedge \hat A \gamma \hat \gamma w_A e_A \times \hat B \gamma \hat \gamma w_B e_B$
\item $\widehat {(A \lto B)} \gamma \hat\gamma w e = \Pi(w_A)\Pi(e_A) \hat A \gamma\hat\gamma w_A e_A \Rightarrow \hat B \gamma\hat\gamma (ww_A) (\applto {e_A} e)$
\end{enumerate}

First, the category with families will be
the category of logical families over set contexts/types
$\Delta$/$A$. Then the propositional portion will be defined by
mapping a logical family $\hat \Gamma \to \Gamma$ 

First, let $L$ be the category of BI formulae and proofs (quotiented
by $\beta\eta$ equality). Define a functor $N : L \to \Set^{\Sigma^*}$ by
\[ N(\phi)(w) = L(w,\phi) \]

Then define the gluing category $\mathcal G$ as the comma category
$\Set^{\Sigma^*}/N$. That is, an object of this category is a pair of
a formula $\phi \in L$ and an object $S \in \mathcal
P(\Sigma^*)/N(\phi)$. We can then use the equivalence $\mathcal
P(\Sigma^*)/N(\phi) \cong \mathcal P(\int N(\phi))$ to get a simple
description of such an $S$: it is simply a family of sets indexed by
proofs $L(w,\phi)$:
\[ \prod_{w\in\Sigma^*} L(w,\phi) \to \Set \]
This category clearly comes with a projection functor $\pi : \mathcal
G \to \mathcal L$ and then our goal is to define a section by using
the universal property of $\mathcal L$.

To this end we define
\begin{enumerate}
\item $(\phi, S) \otimes (\psi, T) = (\phi \otimes \psi, S\otimes T)$ where
  \[ (S \otimes T)(w, p) = (w_1w_2 = w) \times (q_1,q_2 = p) \times S\,w_1\,q_1 \times T\,w_2\,q_2\]
\item $(\phi, S) \multimap (\psi, T) = (\phi \multimap \psi, S \multimap T)$ where
  \[ (S \multimap T)(w,p) = w' \to q \to S\,w'\,q \to T (ww') (p\,q) \]
\item $\mu X. ??$ ??
\end{enumerate}

\section{Type-theoretic Characterizations of Language Classes}
Classically, formal language theory is closely related to automata theory. In the styles of Chomsky and Sch\"utzenberger, there is a hierarchy of language classes and the types of automata that recognize them. The type theory of \cref{sec:typetheory} serves as a syntax to reason about formal grammars; moreover, we describe notions of automata into this formal system via syntactic forms that describe an automaton as a grammar. This method allows us to internalize many classic automata-theoretic results as proof terms in our type theory.

Note, a predicate logic over LNL with existential quantifiers is already enough to define
formulae that correspond to arbitrary recursively enumerable languages ---
since we can simulate the tape of a Turing machine using two
stacks. I.e., we can define a set $S$ of lists of tape symbols, and
then define a formula that first ``copies'' the input onto one stack, then executes

\[ \mu \textrm{Copy} : S \to *. \lambda s. (\epsilon \wedge T\,s\,[])\bigvee_{c \in \Sigma} c (S\,{c::s}) \]

We reflect on this expressive power to motivate why we put heavy syntactic restrictions on our presentation of certain classes of automata. For instance, when defining a pushdown automata, we could use the intuitionistic context to represent the stack, but we ought to be very careful about how this set is accessed and added to. Careless definitions may accidentally endow us with too much expressive power, launching us further up in the Chomsky hierarchy. To begin, let us focus only on language classes, and their associated automata, whose parses are definable without any mention to the inuitionistic context. When viewed as a machine, this refers to the class of automata definable without access to any memory --- i.e.\ finite state machines.

\subsection{Finite State Machines}
A finite state machine may be expressed via a particular syntactic form for grammars. A \emph{nondeterministic finite automaton} (NFA) is defined as a mutual\footnote{For convenience we refer to a mutual fixed point here, even though we only take single fixed points as primitive. As in Beki\'c's theorem, we may decompose our mutual fixed points into a series of single fixed points.} fixed point in the following manner,

\[
  NFA ::= \mu \begin{pmatrix}
        Y_{s} = \langle Q \rangle \\
        Y_{1} = \langle Q \rangle \\
        \vdots \\
        Y_{e} = I
        \end{pmatrix}~\textbf{in}~Y_{s}
\]
where
\[
  Q ::= c \otimes Y ~|~ Y ~|~ Q \oplus Q
\]
and $Y \in \{ Y_{s}, Y_{1}, \dots, Y_{N} \}$. Intuitively, we read each $Y_{j}$ as the label for a state, and the definitions in form $Q$ describe the transitions between states --- $c \otimes Y$ denotes a labeled transition, and $Y$ denotes an $\epsilon$-transition. Without loss of generality, $Y_{s}$ is reserved for a single distinguished start state, and $Y_{e}$ is reserved for a single distinguished accepting state. A disjunction\footnote{There is an admissible notion of coproduct making use of the primitive dependent pairing --- i.e. $A \oplus B := \LinPiTy b \Bool {\textbf{if}~b~A~B}$. We will utilize this without much comment to represent the usual notion of alternation of grammars.} of transitions in an NFA grammar corresponds to multiple outgoing edges of a given state in an automaton.

Let $\hat{Y}_{k}$ refer to the syntactic term on the right hand side of the definition of $Y_{k}$ inside of an NFA $N$. We say that $N$ is a \emph{deterministic finite automaton} (DFA) if there are no $\epsilon$-transitions and for every definition $\hat{Y}_{k}$ the $c_{j}$ are distinct guards for each produced state.
% \steven{Is this the right notion of determinism? Perhaps, it is is better to define determinism as something like: there is at most one derivation for all $j, k$, for all $a \in \Sigma$ we have $x : Y_{k}, y : a \vdash z : Y_{j}$.}

With this setting for finite automata, we can now internalize some classical theorems inside of our formal system. First, let us describe a version of Thompson's construction \cite{?} where we show that any regular expression can be recognized by some NFA\@. Moreover, we will show that this NFA is strongly equivalent to the original grammar.

\subsection{Equivalence Between Regular Grammars and Finite Automata}

The class of \emph{regular} grammars is given as follows:

\[
Reg ::= 0 ~|~ I ~|~ c ~|~ Reg \oplus Reg ~|~ Reg \otimes Reg ~|~ Reg^{*}
\]
where $c \in \mathcal{A}$ is a character literal drawn from a fixed alphabet, and the Kleene star $\cdot^{*}$ is derived as an admissible operation using the fixed point operator --- $g^{*} = \mu X.I \oplus (g \otimes X)$. \steven{This definition is maybe fair to assume. Also maybe better suited for a section earlier in the paper}

\begin{theorem}
  \label{thm:thompson}
  For $g$ a regular grammar, there is an NFA $N$ that recognizes the same language as $g$. Further, $N$ and $g$ are isomorphic.
\end{theorem}

\begin{proof}
  We construct the recognizing NFA $N$ inductively. To handle the atomic grammars --- literals and the empty grammar --- we define the following,

\[
  NFA(c) =
  \mu \begin{pmatrix}
        Y_{s} = c \otimes Y_{e} \\
        Y_{e} = I
  \end{pmatrix}~\textbf{in}~Y_{s}
\]
\[
  NFA(I) =
  \mu \begin{pmatrix}
        Y_{s} = Y_{e} \\
        Y_{e} = I
  \end{pmatrix}~\textbf{in}~Y_{s}
\]

The isomorphism between the grammars $c$ and $NFA(c)$ are witnessed by the following derivations,
\[
  \inferrule{p : c \vdash p : c \\ \cdot \vdash () : I}{p : c \vdash p \otimes () : Y_{s} = c \otimes I}
\]
and
\[
  \inferrule{q : Y_{s} = c \otimes I \vdash q : Y_{s} \\ p : c, e : I \vdash p : c }{q : Y_{s} \vdash \letin {p \otimes e} q p : c}
\]

Similar terms are used to show the equivalence between $I$ and $NFA(I)$.

We can now inductively define some NFA combinators and show that they preserve grammar equivalence. First begin with the disjunction of two NFAs. That is, assuming that $g \cong NFA(g)$ and $g' \cong NFA(g')$, we define an NFA that recognizes $g \oplus g'$. Let the states in $NFA(g)$ be represented by $X$'s and the states in $NFA(g')$ be represented by $Z$'s. Further let $\hat{X}_{k}$ refer to the syntactic term on the right hand side of the definition of $X_{k}$ inside of $NFA(g)$. Likewise define each $\hat{Z}_{k}$.

\[
  NFA(g \oplus g') =
  \mu \begin{pmatrix}
        Y_{s} =  X_{s}' \oplus Z_{s}' \\
        X_{j}' = \simulsubst {\hat{X}_{j}} {\delta_{X}}, j \neq e \\
        X'_{e} = Y_{e} \\
        Z_{j}' = \simulsubst {\hat{Z}_{j}} {\delta_{Z}}, j \neq e \\
        Z_{e}' = Y_{e} \\
        Y_{e} = I
  \end{pmatrix}~\textbf{in}~Y_{s}
\]
where the substitution $\{X_{s}'/X_{s}, X_{e}'/X_{e}, X_{1}'/X_{1}, \dots \}$ is given by $\delta_{X}$. Likewise define $\delta_{Z}$.
In the construction of $NFA(g \oplus g')$ we replicate each defintion $X_{j}$ in $NFA(g)$ with an analogous definition $X_{j}'$ inside of $NFA(g \oplus g')$, except we replace $X_{k}$ with $X_{k}'$ in each production. Lastly, $X_{e}'$ has an $\epsilon$-transition to the accepting state $Y_{e}$. Likewise, we build analogous defintions for the states $Z_{j}$ in $NFA(g')$, then we flatten all of these defintions into one mutally recursive list. This has the effect of disjunctively tying together $NFA(g)$ and $NFA(g')$ into one automaton.

To show the strong equivalence of $NFA(g \oplus g')$ and $g \oplus g'$, we will demonstrate an isomorphism between them with two proof terms in our logic that mutually invert each other. To define a map $NFA(g \oplus g') \to g \oplus g'$ we will use a multi-fold over all the mutually recursive defintions of $NFA(g \oplus g')$. Note, even though multiple folding is not taken as primitive, using Beki\'c's theorem we can define a family of multifold terms to serve as elimination forms for our admissible multiple fixed points. That is, rules of the form,

\[
  \inferrule{\Gamma ; \Delta \vdash e : \mu(X_{1} = A_{1}, \dots, X_{n} = A_{n})~\textbf{in}~X_{k} \\
             \Gamma ; x_{1} : \simulsubst {A_{1}}{\gamma} \vdash e_{1} : B_{1} \\
             \dots \\
             \Gamma ; x_{n} : \simulsubst {A_{n}}{\gamma} \vdash e_{n} : B_{n}
  }{\Gamma; \Delta \vdash \mathsf{mfold}(x_{1}.e_{1}, \dots, x_{n}.e_{n})(e) : B_{k}}
\]

are admissible, where the substitution $\{B_{1}/X_{1},\dots,B_{n}/X_{n} \}$ is given by $\gamma$. In this instance, we wish to simultaneously fold over the mutually recursive definitions comprising $NFA(g \oplus g')$. We may think of this as an eliminator that replaces each state inside of $NFA(g \oplus g')$ with a tail-recursive call that will ultimately return a term of type $g \oplus g'$. The structure of such a derivation is given below,

\[
  \inferrule{
    \Delta \vdash e : NFA(g \oplus g') \\
    y_{s} : \simulsubst {(X_{s}' \oplus Z'_{s})} \gamma \vdash p_{s} : g \oplus g' \\
    x_{s}' : \simulsubst {(\simulsubst {X_{s}} {\delta_{X}})}{\gamma} \vdash p_{x,s} : g \\
    x_{j}' : \simulsubst {(\simulsubst {X_{j}} {\delta_{X}})}{\gamma} \vdash p_{x,j} : X_{j} \text{ where } j \not \in \{s, e\} \\
    x_{e}' : \simulsubst {Y_{e}}{\gamma} \vdash p_{x, e} : X_{e} \\
    z_{s}' : \simulsubst {(\simulsubst {Z_{s}} {\delta_{Z}})}{\gamma} \vdash p_{z,s} : g' \\
    z_{j}' : \simulsubst {(\simulsubst {Z_{j}} {\delta_{Z}})}{\gamma} \vdash p_{z,j} : Z_{j} \text{ where } j \not \in \{s, e\} \\
    z_{e}' : \simulsubst {Y_{e}}{\gamma} \vdash p_{z,e} : Z_{e} \\
    y_{e} : \simulsubst I {\gamma} \vdash p_{e} : I
  }{\Delta \vdash \mathsf{mfold}(e) : g \oplus g'}
\]

For each of the variables above, we simplify the type and provide the actual proof term. For instance, $\simulsubst {(X_{s}' \oplus Z'_{s})} \gamma = g \oplus g'$, so the derivation for $p_{s}$ is just the identity. The rest of the terms are given below,
\begin{itemize}
  \item $y_{s}$ --- $\simulsubst {(X_{s}' \oplus Z'_{s})} \gamma = g \oplus g'$ and $p_{s} = y_{s} : g \oplus g'$
  \item $x_{s}'$ --- $\simulsubst {(\simulsubst {\hat{X}_{s}} {\delta_{X}})}{\gamma} = X_{s}$ and $p_{x,s} = \phi^{-1}(x_{s}') : g$
  \item $x_{j}', j \not \in \{s, e\}$ --- $\simulsubst {(\simulsubst {\hat{X}_{j}} {\delta_{X}})}{\gamma} = X_{j}$ and $p_{x, j} = x_{j}'$
  \item $x_{e}'$ --- $\simulsubst {Y_{e}}{\gamma} = I$ and $p_{x,e} = x_{e}' : X_{e} = I$
  \item $z_{s}'$ --- $\simulsubst {(\simulsubst {\hat{Z}_{s}} {\delta_{Z}})}{\gamma} = Z_{s}$ and $p_{z,s} = \psi^{-1}(z_{s}') : g'$
  \item $z_{j}', j \not \in \{s, e\}$ --- $\simulsubst {(\simulsubst {\hat{Z}_{j}} {\delta_{Z}})}{\gamma} = Z_{j}$ and $p_{z, j} = z_{j}'$
  \item $z_{e}'$ ---  $\simulsubst {Y_{e}}{\gamma} = I$ and $p_{z,e} = z_{e}' : Z_{e} = I$
  \item $y_{e}$ --- $\simulsubst {I} {\gamma} = I$ and $p_{e} = y_{e} : I$
\end{itemize}

where $\phi$ and $\psi$ are the witnesses to the isomorphisms $g \cong NFA(g)$ and $g' \cong NFA(g')$, respectively. This finshes the construction of a parse transformer $NFA(g \oplus g') \to g \oplus g'$ The map in the other direction is given the derivation,


\[
\inferrule{p_{1} : g \vdash \textbf{in}_{X_{s}'}(\phi(p_{1})) : NFA(g \oplus g') \\ p_{2} : g' \vdash \textbf{in}_{{Z_{s}'}}(\psi(p_{2})) : NFA(g \oplus g') \\ p : g \oplus g' \vdash p : g \oplus g'}{p : g \oplus g' \vdash \textbf{case}~p \{\textbf{in}_{g} p_{1} \mapsto \textbf{in}_{X_{s}'}(\phi(p_{1})), \\ \textbf{in}_{g'} p_{2} \mapsto \textbf{in}_{{Z_{s}'}}(\psi(p_{2})) \} : NFA(g \oplus g')}
\]

It is evident that the maps $NFA(g \oplus g') \to g \oplus g'$ and $g \oplus g' \to NFA(g \oplus g')$ form an isomorphism.

\begin{align*}
  \textbf{case}~\mathsf{mfold}(e) \{\textbf{in}_{g} p_{1} \mapsto \textbf{in}_{X_{s}'}(\phi(p_{1})), \\ \textbf{in}_{g'} p_{2} \mapsto \textbf{in}_{{Z_{s}'}}(\psi(p_{2})) \}
  & = e
\end{align*}

% \begin{align*}
%   \begin{aligned}
%     \textbf{case}~\mathsf{mfold}(e) \{\textbf{in}_{g} p_{1} \mapsto \textbf{in}_{X_{s}'}(\phi(p_{1})), \\ \textbf{in}_{g'} p_{2} \mapsto \textbf{in}_{{Z_{s}'}}(\psi(p_{2}))
%   \end{aligned}
%   & =
%   \begin{aligned}
%     \textbf{case}~(&  \textbf{case}~ e \{ \\
%     & \textbf{in}_{X_{s}'}u \mapsto \mathsf{mfold}(\textbf{in}_{X_{s}'}u), \\ & \textbf{in}_{Z_{s}'}v \mapsto \mathsf{mfold}(\textbf{in}_{Z_{s}'}v) \\ \}) \{ \\ & \textbf{in}_{g} p_{1} \mapsto \textbf{in}_{X_{s}'}(\phi(p_{1})), \\  & \textbf{in}_{g'} p_{2} \mapsto \textbf{in}_{{Z_{s}'}}(\psi(p_{2})) \}
%   \end{aligned} \\
%   & = \textbf{case}~e \{
%     \textbf{in}_{X_{s}'}u \mapsto \textbf{in}_{X_{s}'}(\phi(\textsf{mfold}(\textbf{in}_{{X_{s}'}}u))) \\
%     \textbf{in}_{Z_{s}'} v \mapsto
%     \}
    %   \end{align*}
% This doesn't seem worth it. But you argue formally that commuting the case statements as such is done with the eta rule. Then you get everything to cancel down to \phi(\phi^{-1}(...)) (or the same with \psi)

% \steven{I could type out the equality proof term, but it blows up in size and doesn't seem like a useful way to capture this iso. I have a commented out version of it above. Going to refactor tomorrow to something more sane, as these as are the crux of the proofs of equality} \steven{In any case, the NFA-to-grammar direction of these isos should all follow from some $\eta$-rule on the structure of the type for the start state, right?}
% \pedro{What I like doing in these cases is either presenting the high-level structure of the proof, e.g., ``by applying the rules <...> we can conclude'', or present the full proof in the appendix.
  % Since in this case it feels like it won't be a particularly illuminating proof, I suggest doing
% the former.}
    %

By breaking into a case distinction on $e$ and then applying the $\eta$-rule for coproducts, the above proof will simplify into two cases where $\phi$ will be inverted by $\phi^{{-1}}$ and $\psi$ will be inverted by $\psi^{-1}$, ultimately cancelling to give the identity map. The other direction below simplifies in a similar manner. \steven{Most concise list for this?}

% Without loss of generality assume that $e = \textbf{in}_{X_{s}'} u$. Then, $\mathsf{mfold}(e) = \textbf{in}_{g}(\phi^{-1}u)$ and passing this through the above case statement results in $\textbf{in}_{{X_{s}'}}(\phi(\phi^{{-1}}u)) = \textbf{in}_{{X_{s}'}}(u) = e$. Further,

\begin{align*}
  \mathsf{mfold}(\textbf{case}~p \{\textbf{in}_{g} p_{1} \mapsto \textbf{in}_{X_{s}'}(\phi(p_{1})), \\ \textbf{in}_{g'} p_{2} \mapsto \textbf{in}_{{Z_{s}'}}(\psi(p_{2})) \})
  & = p
\end{align*}

This concludes the disjunctive combination of two NFAs. Because they follow a very similar style of argumentation, we include the cases for sequencing and Kleene star in \cref{sec:appendixnfa}.


\end{proof}

% \subsubsection{NFA to Grammar}

% \steven{This section and following are basically todos for rn}

% We now seek to take an NFA and derive the regular expression recognized by it. To this end we weaken our syntactic form of an NFA to that a \emph{generalized nondeterministic finite automata} (GNFA) --- where guards on transition can be arbitrary regexps instead of single characters. We then repeatedly take away states from the NFA to give an increasingly more complete algebraic description of the regular expression inside of the guards. We repeat until there is only a single transition between start and accepting state, and the guard on the single transition denotes the regular expression that is equivalent to the NFA.

% \[
%   GNFA ::= \mu \begin{pmatrix}
%         Y_{s} = \langle Q \rangle \\
%         Y_{1} = \langle Q \rangle \\
%         \vdots \\
%         Y_{e} = I
%         \end{pmatrix}~\textbf{in}~Y_{s}
% \]
% where
% \[
%   Q ::= \langle RegExp \rangle \otimes Y ~|~ Y ~|~ Q \oplus Q
% \]
% \[
%   RegExp ::= c ~|~ I ~|~ RegExp \oplus RegExp ~|~ RegExp \otimes RegExp ~|~ RegExp^{*}
% \]

% This is shown inductively by showing that the equivalence of a GNFA to a copy of itself with a general state --- i.e.\ non-start, non-accepting --- removed. Consider GNFA $A$ and remove state $Y_{r}$ to achieve $A'$. Let $\delta_{Y}$ denote the substitution that replaces $Y_{k}'/Y_{k}$ for $k \neq r$, and $\gamma$ denote the substitution $\hat{Y}_{r}/Y_{r}$.

% \[
%   A = \mu \begin{pmatrix}
%             Y_{s} = \hat{Y}_{s} \\
%             Y_{r} = \hat{Y}_{r} \\
%             Y_{j} = \hat{Y}_{j}, j \not \in \{s, r, e\} \\
%             Y_{e} = I
%           \end{pmatrix}~\textbf{in}~Y_{s}
% \]
% \[
%   A' = \mu \begin{pmatrix}
%             Y_{s}' = \simulsubst {\hat{Y}_{s}} {\delta_{Y} \circ \gamma} \\
%             Y_{j}' = \simulsubst {\hat{Y}_{j}} {\delta_{Y} \circ \gamma}, j \not \in \{s, e\} \\
%             Y_{e}' = I
%           \end{pmatrix}~\textbf{in}~Y_{s}'
% \]

% We can pretty easily translate between parses of $A$ and $A'$. This can be seen by looking at the structure of the terms $\hat{Y}_{k}$ and $\simulsubst {\hat{Y}_{k}} {\delta_{Y} \circ \gamma}$. For terms $\hat{Y}_{k}$ that don't have $Y_{r}$ as a subterm, the action of $\gamma$ is trivial --- so $\simulsubst {\hat{Y}_{k}} {\delta_{Y} \circ \gamma} = \simulsubst {\hat{Y}_{k}} {\delta_{Y}}$. For these $Y_{k}$ it is trivial to define a fold from $A$ to $A'$, or vice versa as we are simply renaming terms. For $\hat{Y}_{k}$ containing $Y_{r}$ as a subterm, it is not as trivial. Instead, each occurence of $Y_{r}$ is replaced with an equivalent definition.

% Folding from $A$ to $A'$,

% \begin{itemize}
%   \item $y_{j}, j \not \in \{r, e\}$ and $Y_{r} \not \in \hat{Y}_{k}$ --- $\simulsubst {\hat{Y}_{k}} {\delta_{Y} \circ \Gamma} = \simulsubst {\hat{Y}_{k}} {\delta_{Y}}$ and $p_{s} = y_{j} : Y_{j}'$
%   \item $y_{j}, j \not \in \{r, e\}$ and $Y_{r} \in \hat{Y}_{k}$ ---
%   \item $y_{e}$ --- $\simulsubst I {\delta \circ \gamma} = I$ and $p_{e} = y_{e} : I$
% \end{itemize}




\steven{An aside on some axioms}

Something like $A \cong (I \& A) \oplus \sum_{c} c \otimes (c \lto A)$

For this taylor iso to hold, we need the map $c \otimes (c \lto A) \vdash (c \otimes \top) \amp A$ to be an iso. Specifically, this term needs an iso $\phi : c \otimes (c \lto A) \to (c \otimes \top) \amp A$ defined below,

\[
p : c \otimes (c \lto A) \vdash \phi(p) = \letin {p_{1} \otimes p_{2}} p {(p_{1} \otimes \top, p_{1}^{\lto}p_{2})} : (c \otimes \top) \amp A
\]

Also add the axiom that $\top \cong (\sum_{c \in \mathcal{A}} c)^{*}$

If we have the above, we can then show the following isos for some alphabet $\mathcal{A}$,
\begin{align*}
  A
  & \cong \top \amp A \\
  & \cong (\Sigma_{c \in \mathcal{A}})^{*}\amp A \\
  & \cong (I \oplus (\Sigma_{c} c \otimes \top)) \amp A \\
  & \cong (I \amp A) \oplus ((\Sigma_{c} c \otimes \top)) \amp A) \\
  & \cong (I \amp A) \oplus (\Sigma_{c }c \otimes (c \lto A)) & \text{using } \phi^{-1}
\end{align*}

From here we need to also show that derivatives of regular languages are regular (look at Brzozowksi's proof) and that regular languages have finitely many derivatives (potentially with some size constraint on the derivative?). Then the last iso in the chain above can be used to build a DFA equivalent to $A$ using Brzozowksi's algorithm.

Need to also add in axioms for $\amp$ distributing over $\oplus$ and $\lto$ distributing over $\oplus$. And that $c \amp c' \vdash 0$ for $c \neq c'$

\subsection{DFAs and NFAs}

All DFAs are NFAs, so NFAs are at least as powerful as DFAs. It is perhaps surprising that DFAs are as powerful as NFAs too! We can internalize the classic powerset construction to transform an NFA into a DFA that recognizes the same language. Note however that this loses some information. Throughout this process we may lose data such as the ambiguity of the grammar. Effectively we are forgetting the structure of our proof terms. Under the ``grammars as types'' lens, we are erasing the structure underlying the inhabitants of the type to simply mere existence --- this is a form of propostional truncation.

\steven{Worth spending some more time on the propositional truncation idea}

\steven{Put powerset construction here}

\subsection{TODO More Complicated Grammars}
Want to put PDA, CFGs, CSGs here. Basically, use this section to highlight the usefulness of the intuitionistic part of the contexts

\section{Future work}

\subsection{Implementation}



\subsection{Beyond Strings}

While parsing typically refers to the production of semantic objects
from their description as strings, many tasks in programming can be
viewed as parsing of more structured objects such as trees, trees with
binding structure or graphs. Fundamental to programming language
implementation is \emph{type checking}, analogous to language
recognition, or more generally \emph{typed elaboration}, analogous to
parsing, which produces a semantic object in addition to performing
some analysis. A type system could then be interpreted as a \emph{tree
grammar}. Our definition of string grammars as functors from
$\Sigma^*$ to $\Set$ can then be easily adapted to functors to $\Set$
from the set of \emph{trees} freely generated form some signature of
nodes. Then just as the set of string grammars inherits a monoidal
structure from the free monoid of strings using the Day convolution,
this set of tree grammars inherits a kind of weak algebraic structure
corresponding to the tree constructors. This suggests an even more
unusual form of bunched type theory, where the monoidal concatenation
and unit are replaced with context constructors corresponding to the
tree constructors.

\bibliographystyle{ACM-Reference-Format}
\bibliography{references}

\clearpage
\appendix

\section{Semantics in Sets and Grammars}

A model of Grammar Type Theory is a category with families equipped
with a functor $L : \mathcal C \to \textrm{GrLog}$ from the category
of contexts to the category of grammar logic models and strict morphisms.

First, we give the set theoretic model. We use the standard category
with families structure given by the category of sets and a cumulative
tower of universes.
%
Next, we define $L$ as follows:
\[ L(\Gamma) = \mathcal P \Sigma^*/\textrm{Disc} \Gamma \]
Where $\textrm{Disc} \Gamma$ is the presheaf that is constantly $\Gamma$.
Note that
\[ \mathcal P \Sigma^*/\textrm{Disc} \Gamma \cong (\Set/\Gamma)/\textrm{Disc} \Sigma^*\]
where $\textrm{Disc} \Sigma^*$ is the projection $\Gamma \times \Sigma^*
\to \Gamma$.  Then by the fundamental theorem of topos thoery, $\Set/\Gamma$ is
a topos. Since $\textrm{Disc} : \Set \to \Set/\Gamma$ is monoidal,
$\textrm{Disc} \Sigma^*$ carries a monoid structure in $\Set/\Gamma$ and
this induces a monoidal closed structure on $(\Set/\Gamma)/\textrm{Disc}
\Sigma^*$. Again by the fundamental theorem, it also has finite
products and coproducts and a natural numbers object, and so can
interpret least fixed points.\max{More details about constructing initial algebras?}

Then we can define the remaining type structure as follows. Unraveling
the above, a non-linear type $\Gamma \vdash X$ is interpreted as an
object of $\Set/\sem{\Gamma}$ and a linear type $\Gamma \vdash A$ is
interpreted as an object of $\mathcal P\Sigma^*/\textrm{Disc}\sem{\Gamma}$. We
define them below in indexed style:
\begin{enumerate}
\item $\sem{c} \gamma w = (w = c)$
\item $\sem{\LinPiTy x X A} \gamma w = \Pi(x:\sem{X}\gamma) \sem{A}(\gamma,x) w$
\item $\sem{\LinSigTy x X A} \gamma w = \Sigma(x:\sem{X}\gamma) \sem{A}(\gamma,x) w$
\item $\sem{G A} \gamma = \sem{A} \gamma \varepsilon$
\item $\sem{L_i}\gamma = (\mathcal P_i\Sigma^*/\textrm{Disc}\sem{\Gamma})_0$
\end{enumerate}

\section{Canonicity}

For the canonicity argument, we construct a second ``glued'' model
that contains a projection morphism to the syntactic model.

Let $S$ be the syntactic category with families for the type
theory. Define the glued category with families to be the comma category
% https://q.uiver.app/#q=WzAsNSxbMCwwLCJcXG1hdGhjYWwgRyJdLFswLDIsIlMiXSxbMiwyLCJcXFNldCJdLFsyLDAsIlxcU2V0Il0sWzEsMSwiXFxzd2Fycm93Il0sWzMsMl0sWzEsMl0sWzAsMV0sWzAsM11d
\[\begin{tikzcd}[ampersand replacement=\&]
	{\gluedNL} \&\& \Set \\
	\& \rotatebox[origin=c]{-45}{$\Downarrow$} \\
	S \&\& \Set
	\arrow[Rightarrow, no head, from=1-3, to=3-3]
	\arrow["{S(\cdot,-)}"', from=3-1, to=3-3]
	\arrow[from=1-1, to=3-1]
	\arrow[from=1-1, to=1-3]
\end{tikzcd}\]
This is a standard glued category for canonicity of dependent type
theory, and carries a category with families structure that we will
reuse from prior work for the (non-linear) universes,
$\Pi/\Sigma/\textrm{Id}$ \cite{coquand}. Concretely an object of
$\mathcal G$ is equivalent to a pair of a syntactic context $\Gamma$,
and a family $\hat \Gamma$ over (equivalence classes of) closing
substitutions for $\Gamma$.

Next we turn to interpreting the linear types and morphisms. For this
we define a functor $\gluedL : \gluedNL \to \textrm{MonCat}$ where
$\textrm{MonCat}$ is the category\footnote{in univalent foundations
this would be the category of \emph{strict} monoidal categories.} of
monoidal categories and lax monoidal functors.  We construct
$\gluedL(p : \hat \Gamma \to S(\cdot,\Gamma))$ as the following comma
category:

% https://q.uiver.app/#q=WzAsNixbMCwwLCJcXGdsdWVkTkwocCkiXSxbMCwyLCJMX1xcR2FtbWEiXSxbMSwyLCJcXG1hdGhjYWwgUCBcXFNpZ21hXiovXFx0ZXh0cm17RGlzY30oUyhcXGNkb3QsXFxHYW1tYSkpIl0sWzIsMiwiXFxtYXRoY2FsIFAgXFxTaWdtYV4qL1xcdGV4dHJte0Rpc2N9KFxcaGF0XFxHYW1tYSkiXSxbMiwwLCJcXG1hdGhjYWwgUCBcXFNpZ21hXiovXFx0ZXh0cm17RGlzY30oXFxoYXRcXEdhbW1hKSJdLFsxLDEsIlxccm90YXRlYm94W29yaWdpbj1jXXstNDV9e1xcRG93bmFycm93fSJdLFs0LDMsIiIsMCx7ImxldmVsIjoyLCJzdHlsZSI6eyJoZWFkIjp7Im5hbWUiOiJub25lIn19fV0sWzAsNF0sWzAsMV0sWzEsMiwiTiIsMl0sWzIsMywiXFx0ZXh0cm17RGlzY30ocF4qKSIsMl1d
\[\begin{tikzcd}[ampersand replacement=\&]
	{\gluedNL(p)} \&\& {\mathcal P \Sigma^*/\textrm{Disc}(\hat\Gamma)} \\
	\& {\rotatebox[origin=c]{-45}{$\Downarrow$}} \\
	{L_\Gamma} \& {\mathcal P \Sigma^*/\textrm{Disc}(S(\cdot,\Gamma))} \& {\mathcal P \Sigma^*/\textrm{Disc}(\hat\Gamma)}
	\arrow[Rightarrow, no head, from=1-3, to=3-3]
	\arrow[from=1-1, to=1-3]
	\arrow[from=1-1, to=3-1]
	\arrow["N"', from=3-1, to=3-2]
	\arrow["{\textrm{Disc}(p^*)}"', from=3-2, to=3-3]
\end{tikzcd}\]

Where $N : L_\Gamma \to {\mathcal P
  \Sigma^*/\textrm{Disc}(S(\cdot,\Gamma))}$ is the ``dependent nerve''
of the inclusion $\lceil\cdot\rceil : \Sigma^* \to L_\Gamma$ of
strings into linear contexts defined in indexed style as
\[ N(\Gamma \vdash A) \gamma w = \lceil w \rceil \vdash \cdot :  \simulsubst A \gamma \]
Then observe that $N$ and ${\textrm{Disc}(p^*)}$ are lax monoidal and
${\mathcal P \Sigma^*/\textrm{Disc}(\hat\Gamma)}$ is a topos so the
criteria for Hyland and Schalk's gluing construction apply, defining a
biclosed monoidal structure on $\gluedNL(p)$ as well as finite
(co-)products. Further, these constructions are all stable under
reindexing in $\gluedNL$, and so interpret the dependent linear typing
constructions for $I,\otimes,0,\oplus,\top,\amp,\lto,\tol$.

Concretely, an object of $\gluedNL(p)$ is equivalent to a pair of a
non-linear type $\Gamma \vdash A$ and a formal grammar $\hat A$
indexed by $\hat \Gamma$ with a projection function from $\hat A$ to
$N(A)$. We can think of this presheaf $\hat A$ as the presheaf of
canonical forms of $\simulsubst A \gamma$ for closing substitutions
$\gamma$ in contexts of the form $\lceil w \rceil$.

All that remains in this model is to define the semantics of the
non-standard connectives. We define them in an indexed style. First we
define the interpretation of a non-linear type $\Gamma \vdash X$ as a
family of sets $\hat X$ indexed by a closing substitution $\gamma :
S(\cdot,\Gamma)$, a semantic substitution $\hat \gamma : \hat
\Gamma(\gamma)$ and a closed term $\cdot \vdash e : \simulsubst A
\gamma$. Second, we define a linear type $\Gamma \vdash A$ as a family
$\hat A$ of grammars indexed by $\gamma,\hat\gamma$ as above along
with a morphism $\pi : \hat A \gamma \hat \gamma \to N(\simulsubst A
\gamma)$.

\begin{enumerate}
\item $\hat{c} \gamma \hat \gamma w = (w = c)$ and $\pi(*) = \lceil c \rceil = x_0:c \vdash x_0:c$
\item $\widehat{(\LinSigTy x X A)} \gamma \hat \gamma w = \Sigma(\cdot \vdash e : \simulsubst X \gamma)\Sigma(\hat x: \hat X \gamma \hat \gamma e) \hat{A}(\gamma,e)(\hat\gamma,\hat x) w$
\item $\sem{\LinPiTy x X A} \gamma w = \Sigma(x:\simulsubst X \gamma;\lceil w\rceil \vdash e:\simulsubst A \gamma)\Pi(\cdot \vdash e' : \simulsubst X \gamma)\Pi(\hat x:\hat{X}\gamma\hat\gamma x)\Sigma(\hat a : \hat A (\gamma,e')(\hat\gamma,\hat x) \times (\pi(\hat a) = \subst e {e'} x))$
  and $\pi(e,f) = \dlamb x.e$
\item $\widehat{G A} \gamma \hat \gamma e = \Sigma(\hat a: \hat{A} \gamma \hat\gamma \varepsilon) (p\gamma\hat\gamma \varepsilon(\hat a) = \mathsf{G}^{-1}e)$
\item $\sem{L_i}\gamma = \gluedNLUniv(\gamma)$
\end{enumerate}

Thus we have defined a second model $\gluedNL,\gluedL$ of the type
theory, with a morphism to the syntactic model. By the initiality of
the syntactic model, there is a morphism back to the glued model which
is a section of this projection. For purely positive types, we can
observe by induction on the type structure that the total space of the
family is isomorphic to the set-theoretic semantics. Further, the
projection above defines a section of the families. Finally, by
induction on formulae again we see that this projection is also a
section and therefore an isomorphism, establishing the canonicity
result as stated in Section\ref{section:canonicity}.

\section{NFA stuff}
\label{sec:appendixnfa}

We provide the final two cases of \cref{thm:thompson}.

\begin{proof}
We now construct a combinator for the sequencing of two NFAs.

\[
  NFA(g \otimes g') = \mu
  \begin{pmatrix}
    Y_{s} = X_{s}' \\
    X_{j}' = \simulsubst {\hat{X}_{j}} {\delta_{X}}, j \neq e \\
    X_{e}' = Z_{s}' \\
    Z_{j}' = \simulsubst {\hat{Z}_{j}} {\delta_{Z}}, j \neq e \\
    Z_{e}' = Y_{e} \\
    Y_{e} = I
  \end{pmatrix}
\]

Just as before, we use a multifold to define the map to $g \otimes g'$. Here the table of terms is as follows,

\begin{itemize}
  \item $y_{s}$ --- $\simulsubst {X_{s}'} \gamma = g \otimes g'$ and $p_{s} = y_{s} $
  \item $x_{s}'$ --- $\simulsubst {(\simulsubst {\hat{X}_{s}} {\delta_{X}})}{\gamma} = X_{s} \otimes g'$ and $p_{x,s} = \letin {u \otimes v} {x_{s}'} {\phi^{-1}(u) \otimes v} : g \otimes g'$
  \item $x_{j}', j \not \in \{s, e\}$ --- $\simulsubst {(\simulsubst {\hat{X}_{j}} {\delta_{X}})}{\gamma} = X_{j} \otimes g'$ and $p_{x, j} = x_{j}' : X_{j} \otimes g'$
  \item $x_{e}'$ --- $\simulsubst {Z_{s}'}{\gamma} = g'$ and $p_{x,e} = x_{e}' : g'$
  \item $z_{s}'$ --- $\simulsubst {(\simulsubst {\hat{Z}_{s}} {\delta_{Z}})}{\gamma} = Z_{s}$ and $p_{z,s} = \psi^{-1}(z_{s}') : g'$
  \item $z_{j}', j \not \in \{s, e\}$ --- $\simulsubst {(\simulsubst {\hat{Z}_{j}} {\delta_{Z}})}{\gamma} = Z_{j}$ and $p_{z, j} = z_{j}'$
  \item $z_{e}'$ ---  $\simulsubst {Y_{e}}{\gamma} = I$ and $p_{z,e} = z_{e}' : Z_{e} = I$
  \item $y_{e}$ --- $\simulsubst {I} {\gamma} = I$ and $p_{e} = y_{e} : I$
\end{itemize}

The map in the other direction is given as,

\[
  \inferrule{
    p : g \otimes g' \vdash p : g \otimes g' \\
    p_{1} : g, p_{2} : g' \vdash \phi(p_{1}) \otimes \psi(p_{2}) : g \otimes g'}
  {p : g \otimes g' \vdash \letin {p_{1} \otimes p_{2}} {p} {\phi(p_{1}) \otimes \psi(p_{2})} : NFA(g \otimes g')}
\]

Again, these are clearly isomorphic.

Finally, we build an automata combinator to represent the Kleene star\footnote{Which is another admissible definition via use of the fixed point constructor}.

\[
  NFA(g^{*}) = \mu
  \begin{pmatrix}
    Y_{s} = X_{s}' \oplus Y_{e} \\
    X_{j}' = \simulsubst {\hat{X}_{j}} {\delta_{X}}, j \neq e \\
    X_{e}' = X_{s}' \oplus Y_{e} \\
    Y_{e} = I
  \end{pmatrix}
\]

We fold over these defintions to get a parse of $g^{*}$,
\begin{itemize}
  \item $y_{s}$ --- $\simulsubst {(X_{s}' \oplus Y_{e})} \gamma = g^{*} \oplus I$ and $p_{s} = \textbf{case}~y_{s} \{ \textbf{in}_{g^{*}} u \mapsto u, \textbf{in}_{I} v \mapsto \mathsf{cons}_{*I} v \} : g^{*}$
  \item $x_{s}'$ --- $\simulsubst {(\simulsubst {\hat{X}_{s}} {\delta_{X}})}{\gamma} = X_{s} \otimes g^{*}$ and $p_{x,s} = \letin {u \otimes p_{*}} {x_{s}'} {\mathsf{cons}_{*}(\phi^{-1}(u), p_{*})} : g^{*} $ \steven{slight abuse of notation. Not sure how to best capture the Kleene star constructors}
  \item $x_{j}', j \not \in \{s, e\}$ --- $\simulsubst {(\simulsubst {\hat{X}_{j}} {\delta_{X}})}{\gamma} = X_{j} \otimes g^{*}$ and $p_{x, j} = : X_{j} \otimes g^{*}$
  \item $x_{e}'$ --- $\simulsubst {(X_{s}' \oplus Y_{e})}{\gamma} = g^{*} \oplus I$ and $p_{x,e} = \textbf{case}~x_{e}' \{ \textbf{in}_{g^{*}} u \mapsto u, \textbf{in}_{I} v \mapsto \mathsf{cons}_{*I} v \} : g^{*}$
  \item $y_{e}$ --- $\simulsubst {I} {\gamma} = I$ and $p_{e} = y_{e} : I$
\end{itemize}

and to derive a parse of $NFA(g^{*})$ from a parse of $g*$,

\[
  \inferrule{p : g^{*} \vdash p : g^{*} \\ \cdot \vdash \textbf{in}_{{Y_{e}}}(()) : NFA(g^{*}) \\ u : g, v : g^{*} \vdash \textbf{in}_{{X_{s}'}} (\phi(\mathsf{cons}_{*}(u, v))) : NFA(g^{*}) }{p : g^{*} \vdash \mathsf{fold}_{*}(\textbf{in}_{{Y_{e}}}(()), \textbf{in}_{{X_{s}'}} (\phi(\mathsf{cons}_{*}(u, v)))) : NFA(g^{*}) }
\]

This completes our inductive proof that $NFA(g) \cong g$ for all regular expressions $g$.


\end{proof}

\end{document}
