% -*- fill-column: 80; -*-
\documentclass[review,anonymous,screen,acmsmall,nonacm]{acmart}
\usepackage{mathpartir}
\usepackage{tikz-cd}
\usepackage{lipsum}
\usepackage{enumitem}
\usepackage{wrapfig}
\usepackage{fancyvrb}
\usepackage{xspace}

\usepackage[LGR,T1]{fontenc}
\DeclareMathAlphabet{\mathgtt}{LGR}{cmtt}{m}{n}

 %% \steven{TODO make the Agda logo clickable} %
\newcommand{\Agda}{\agdalogo}
\renewcommand{\Gamma}{\mathgtt{G}}
\renewcommand{\Delta}{\mathgtt{D}}
\renewcommand{\Sigma}{\mathgtt{S}}
\renewcommand{\mu}{\mathgtt{m}}

\renewcommand{\familydefault}{\ttdefault}
\usepackage{mathastext}
\renewcommand{\familydefault}{\rmdefault}

\definecolor{codegreen}{rgb}{0,0.6,0}
\definecolor{codegray}{rgb}{0.5,0.5,0.5}
\definecolor{codepurple}{rgb}{0.58,0,0.82}
\definecolor{backcolour}{rgb}{0.95,0.95,0.92}

\usepackage{listings}
\lstdefinelanguage{Agda}{
keywords={data, where, module, import, open, public,
record, field, let, in, if, then, else, case, of, with,
do, postulate, primitive, mutual, abstract, private,
forall, exists, cong, set, prop, sort, Level, Data, Type,
Renamer},
morekeywords=[2]{Set, Type, Prop, moduleFuncs, instance, Named},
sensitive=true,
comment=[l]{--},
morecomment=[s]{\{-}{-\}},
morestring=[b]",
mathescape=true,
escapeinside={(*@}{@*)}
}
\lstset{
language=Agda,
basicstyle=\ttfamily\small,
keywordstyle=\color{blue},
keywordstyle=[2]\color{teal},
identifierstyle=\color{black},
commentstyle=\color{gray}\textit,
stringstyle=\color{orange},
numbers=none,
numberstyle=\tiny\color{gray},
stepnumber=1,
numbersep=5pt,
showstringspaces=false,
tabsize=4,
captionpos=b,
breaklines=true,
breakatwhitespace=false,
rulecolor=\color{black},
texcl=true,
backgroundcolor=\color{backcolour},
frame=single
}

\usepackage{hyperref}
\usepackage[capitalise]{cleveref}
\usepackage{stmaryrd}
\usepackage{scalerel}
\usepackage{tikz}
\usetikzlibrary{automata, positioning, arrows, fit}

\usepackage{pdfpages}

\newcommand{\Subst}{\textrm{Subst}}
\newcommand{\SPF}{\mathsf{SPF}}
\newcommand{\Var}{\mathsf{Var}}
\newcommand{\K}{\mathsf{K}}
\newcommand{\map}{\mathsf{map}}
\newcommand{\roll}{\mathsf{roll}}
\newcommand{\fold}{\mathsf{fold}}
\newcommand{\inl}{\mathsf{inl}}
\newcommand{\inr}{\mathsf{inr}}
\newcommand{\sem}[1]{\llbracket{#1}\rrbracket}
\newcommand{\cat}[1]{\mathbf{#1}}
\newcommand{\lto}{\multimap}
\newcommand{\tol}{\mathrel{\rotatebox[origin=c]{180}{$\lto$}}}
\newcommand{\String}{\textbf{String}}
\newcommand{\Char}{\textbf{Char}}
\newcommand{\stringg}{\texttt{String}}
\newcommand{\charg}{\mathtt{Char}}
\newcommand{\Set}{\mathbf{Set}}
\newcommand{\Syn}{\mathbf{Synx}}
\newcommand{\SemAct}{\mathbf{SemAct}}
\newcommand{\Gr}{\mathbf{Gr}}
\newcommand{\Grammar}{\mathbf{Gr}}
\newcommand{\Type}{\mathbf{Type}}
\newcommand{\Prop}{\mathbf{Prop}}
\newcommand{\Bool}{\mathtt{Bool}}
\newcommand{\true}{\mathtt{true}}
\newcommand{\false}{\mathtt{false}}
\newcommand{\nat}{\mathbb{N}}
\newcommand{\theoryname}{Dependent Lambek Calculus\xspace}
\newcommand{\theoryabbv}{$\textrm{Lambek}^D$~}
\newcommand{\lnld}{$\textrm{LNL}_D$}

\newcommand{\isTy}{\textrm{ type}}
\newcommand{\isCtx}{\textrm{ ctx}}
\newcommand{\isSmall}{\textrm{ small}}
\newcommand{\isLinTy}{\textrm{ lin. type}}
\newcommand{\isLinCtx}{\textrm{ lin. ctx.}}

\newcommand{\quoteTy}[1]{\lceil{#1}\rceil}
\newcommand{\unquoteTy}[1]{\lfloor{#1}\rfloor}

\newcommand{\gluedNL}{{\mathcal G}_S}
\newcommand{\gluedNLUniv}{{\mathcal G}_{S,i}}
\newcommand{\gluedL}{{\mathcal G}_L}

\newcommand{\amp}{\mathrel{\&}}
\newcommand{\pair}{\amp}
\DeclareMathOperator*{\bigamp}{\scalerel*{\&}{\bigoplus}}
\DeclareMathOperator*{\bigwith}{\scalerel*{\&}{\bigoplus}}


\newcommand{\bang}{~\textbf{!}~}
% Tentatively using uparrow for linear to non-linear to be in line with
% Pfennings adjoint functional programming
\newcommand{\ltonl}[1]{~\uparrow #1}
\newcommand{\nil}{\texttt{nil}}
\newcommand{\ident}{\texttt{id}}
\newcommand{\cons}{\texttt{cons}}
\newcommand{\epscons}{\varepsilon\texttt{cons}}
\newcommand{\data}{\mathsf{data}}
\newcommand{\where}{\mathsf{where}}
\newcommand{\Trace}{\texttt{Trace}}
\newcommand{\literal}[1]{\texttt{\textquotesingle#1\textquotesingle}}
\newcommand{\stringquote}[1]{\texttt{\textquotedbl#1\textquotedbl}}
\newcommand{\internalize}[1]{\lceil#1\rceil}

\newcommand{\oplusinj}[2]{\sigma\,#1\,#2}
\newcommand{\withprj}[2]{\pi\,#1\,#2}

\newcommand{\simulsubst}[2]{#1\{#2\}}
\newcommand{\subst}[3]{\simulsubst {#1} {#2/#3}}
\newcommand{\el}{\mathsf{el}}
\newcommand{\letin}[3]{\mathsf{let}\, #1 = #2 \, \mathsf{in}\, #3}
\newcommand{\lamb}[2]{\lambda #1.\, #2}
\newcommand{\lamblto}[2]{\lambda^{{\lto}} #1.\, #2}
\newcommand{\lambtol}[2]{\lambda^{{\tol}} #1.\, #2}
\newcommand{\dlamb}[2]{{\lambda}^{{\&}} #1.\, #2}
\newcommand{\withlamb}[2]{{\lambda}^{{\&}} #1.\, #2}
\newcommand{\app}[2]{#1 \, #2}
\newcommand{\applto}[2]{#1 \, #2}
\newcommand{\apptol}[2]{#1 \mathop{{}^{\tol}} #2}
\newcommand{\PiTy}[3]{\textstyle\prod (#1 : #2). #3}
\newcommand{\SigTy}[3]{\textstyle\sum (#1 : #2). #3}
\newcommand{\LinPiTy}[3]{\textstyle\bigamp (#1 : #2). #3}
\newcommand{\LinPiTyLimit}[3]{\bigwith\limits_{#1 : #2} #3}
\newcommand{\LinSigTy}[3]{\textstyle\bigoplus (#1 : #2). #3}
\newcommand{\LinSigTyLimit}[3]{\bigoplus\limits_{#1 : #2} #3}
%% \newcommand{\DepWith}[2]{{\textstyle\bigamp}\limits_{#1}{#2}
%% \newcommand{\DepPlus}[2]{{\textstyle\bigoplus}\limits_{#1}{#2}
\newcommand{\GrTy}{\mathsf{Gr}}

\newcommand{\equalizer}[3]{\{#1\,|\,\applto {#2}{#1} = \applto{#3}{#1} \}}
\newcommand{\equalizerin}[1]{\langle #1 \rangle}
\newcommand{\equalizerpi}[1]{#1.\pi}

\newcommand{\ctxwff}[1]{#1 \isCtx}
\newcommand{\ctxwffjdg}[2]{#1 \vdash #2 \isTy}
\newcommand{\linctxwff}[2]{#1 \vdash #2 \isLinCtx}
\newcommand{\linctxwffjdg}[2]{#1 \vdash #2 \isLinTy}

\newsavebox{\logoagdabox}
\sbox{\logoagdabox}{%
  %
  \raisebox{-2pt}{\includegraphics[height=1em]{logo-agda.pdf}}%
}
\newcommand{\agdalogo}{%
  \usebox{\logoagdabox}}%

\newif\ifdraft
\drafttrue
\newcommand{\steven}[1]{\ifdraft{\color{orange}[{\bf Steven says}: #1]}\fi}
\renewcommand{\max}[1]{\ifdraft{\color{blue}[{\bf Max says}: #1]}\fi}
\newcommand{\pedro}[1]{\ifdraft{\color{red}[{\bf Pedro says}: #1]}\fi}
\newcommand{\nathan}[1]{\ifdraft{\color{green}[{\bf Nathan says}: #1]}\fi}
\newcommand{\pipe}{\,|\,}

\begin{document}

\pagestyle{plain}

\pagebreak

\title{Intrinsic Verification of Parsers and Formal Grammar Theory in Dependent Lambek Calculus}

\author{Steven Schaefer}
\affiliation{\department{Electrical Engineering and Computer Science}
  \institution{University of Michigan}
  \country{USA}
}
\email{stschaef@umich.edu}

\author{Nathan Varner}
\affiliation{\department{Electrical Engineering and Computer Science}
  \institution{University of Michigan}
  \country{USA}
}
\email{nvarner@umich.edu}

\author{Pedro H. Azevedo de Amorim}
\affiliation{
  \department{Department of Computer Science}
  \institution{University of Oxford}
  \country{UK}
}
\email{pedro.azevedo.de.amorim@cs.ox.ac.uk}

\author{Max S. New}
\affiliation{
  \department{Electrical Engineering and Computer Science}
  \institution{University of Michigan}
  \country{USA}
}
\email{maxsnew@umich.edu}

\makeatletter
\let\@authorsaddresses\@empty
\makeatother

\begin{abstract}
  We present \theoryname~(\theoryabbv), a domain-specific dependent
  type theory for verified parsing and formal grammar theory. In
  \theoryabbv, linear types are used as a syntax for formal grammars,
  and parsers can be written as linear terms. The linear typing
  restriction provides a form of intrinsic verification that a parser
  yields only valid parse trees for the input string. We demonstrate
  the expressivity of this system by showing that the combination of
  inductive linear types and dependency on non-linear data can be used
  to encode commonly used grammar formalisms such as regular and
  context-free grammars as well as traces of various types of
  automata. Using these encodings, we define parsers for regular
  expressions using deterministic automata, as well as providing
  examples parsers for LL(1) and LR(1) grammars.

  We present a denotational semantics of our type theory that
  interprets the types as a mathematical notion of formal
  grammars. Based on this denotational semantics, we have made a
  prototype implementation of \theoryabbv using a shallow embedding in
  the Agda proof assistant. All of our examples parsers have been
  implemented in this prototype implementation.
\end{abstract}

\maketitle

\section{Introduction}
\label{sec:intro}
Parsing structured data from untrusted input is a ubiquitous task in
computing. Any formally verified software system that interacts with
the outside world must contain some parsing component. For example, in
an extensive experiment finding bugs in C compilers
\cite{yangFindingUnderstandingBugs}, an early version of the formally
verified CompCert C compiler only contained bugs in the then
unverified parsing component \cite{leroy_formal_2009}. Bugs in parsers
undermine the overall correctness theorem for a verified system: an
incorrectly parsed C program will be compiled correctly but this is
not very useful if it did not correctly correspond to the actual
source program. Eventually, a correct parser was implemented using an
automaton that is formally verified to implement an LR grammar
\cite{jourdanValidatingLRParsers2012}.

It is entirely understandable from an engineering perspective
\emph{why} verified parsing was not part of the initial releases of
CompCert: parsing algorithms and formal grammars are a complex area,
featuring a variety of domain-specific formalisms such as context-free
grammars and various automata. These formalisms have little relation
to the main components of a verified compiler. For this reason, it is
advantageous for verified parsers to be implemented using a reusable
verified library, just as parser generators and regular expression
matchers have done for many decades in unverified software.

Prior approaches to verified parsing focus on verification of a
particular grammar formalism such as \max{Steven write down what the
  related work uses
  here}\cite{lasserCoStarVerifiedALL2021,firsovCertifiedNormalizationContextFree2015,danielssonTotalParserCombinators2010}.
Each new grammar formalism must then be extended with its own verified
implementation.

In this work, we present the design of \theoryname (\theoryabbv), a
domain-specific language for formal verification of parsers. A key
property is that \theoryabbv is an \emph{extensible} framework for
verification of parsers in that it supports the definition of grammar
formalisms of unrestricted complexity. That is, \theoryabbv is not a
system for verifying \emph{one} type of grammar formalism, but instead
is a domain-specific language in which many grammar formalisms and
their verified parsers can be implemented. For example, \theoryabbv is
not a verified parser generator that compiles regular expressions to
deterministic finite automata, but is instead is a domain specific
language \emph{for writing} such a verified parser generator.

\max{Steven, is this accurate comparison?} The design of \theoryabbv
is an extension of Joachim Lambek's \emph{categorial grammars}
\cite{lambek}. Categorial grammars are a grammar formalism equivalent
in expressive power to context-free grammars that in modern
terminology would be considered a kind of \emph{non-commutative linear
logic}, a version of linear logic where the tensor product is not
commutative, reflecting the obvious property that the relative
ordering of characters is significant in parsing problems. We extend
non-commutative linear logic with two key components that increase its
power to support arbitrarily powerful grammar formalisms: inductive
linear types, as well as dependency of linear types on non-linear
data. The resulting system has two kinds of types: non-linear types
which model sets and linear types which model formal
grammars. Crucially, the non-linear types and linear types are allowed
to be dependent on non-linear types, but not on linear types. This
combination has been used previously in the ``linear-non-linear
dependent'' type theory with \emph{commutative} linear logic to model
imperative programming \cite{krishnaswami_integrating_2015}.

The substructural nature of \theoryabbv is well-aligned with the
requirements intrinsic to parsing and to the theory of formal
languages, where strings constitute a very clear notion of resource
that cannot be duplicated, reordered, or dropped. Moreover, the constructive
aspect of \theoryabbv ensures that verification of parsers written in the
calculus are \emph{correct-by-construction}. The type system is powerful enough
that derivations of a term type checking carry intrinsic proofs of correctness.
Parsers written in \theoryabbv take on a linear functional style, which makes
them familiar to write and amenable to compositional verification techniques.

To show the feasibility of our design, we have implemented \theoryabbv
as a shallowly embedded domain-specific langauge in the Cubical Agda
proof assistant \cite{agda,cubical}. We have implemented many example
grammars and parsers in our system including regular expressions,
non-deterministic and deterministic automata, as well as some example
context-free grammars and parsers based on LL(1) and LR(1)
automata. Throughout this paper, we will use \agdalogo~ to mark
results that are mechanized in our Agda development.

Our Agda prototype is based on a \emph{denotational semantics} of
\theoryabbv. The core idea of the denotational semantics is
\cite{elliottSymbolicAutomaticDifferentiation2021}, Elliott describes
a formal grammar as type-level predicates on strings that prove
language membership. That is, a \emph{formal grammar} $A$ is a
function $\String \to \Set$ such that for a string $w$, $A~w$ is the
set of ``proofs'' showing that $w$ belongs to the language recognized
by $A$. We show that all linear types in \theoryabbv can be so
interpreted as an abstract formal grammar in this sense, and that
linear terms are a kind of \emph{parse transformer}, a function that
takes a parse tree from one grammar to a parse tree in a different
grammar but over the same underlying string.

%% Formal verification encompasses a broad suite of techniques one may use to
%% mathematically guarantee the correctness of software systems. We can only
%% reap the benefits of
%% any proof effort if its underlying assumptions hold. For instance, verifying the
%% functional correctness of an algorithm is no use if it is run on buggy hardware.
%% Likewise, the promised guarantees of a program may be jeopardized if it is
%% parsed incorrectly.

Our contributions are then:\max{This contribution section needs to be punchier}
%
\begin{itemize}
  \item The design of \theoryname (\theoryabbv): A dependent
    linear-non-linear type theory for building verified parsers, which
    extends prior work on dependent linear-non-linear type theory to
    support inductive linear types
  \item Demonstration of how to encode many common grammar formalisms
    (regular expressions, (non-)deterministic automata, context-free
    grammars) and parser formalisms within our type theory.
  \item A prototype implementation of \theoryabbv in Agda with all
    examples mechanized.
  \item A denotational semantics for \theoryabbv that shows that the
    parsers are in fact verified to be correct and soundness of the
    equational theory.
\end{itemize}

This paper begins in \cref{sec:type-theory-examples} by studying small
example programs from \theoryabbv to build intuition.  From there, in
\cref{sec:tt} we provide the syntax, typing and equational theory of
\theoryname. In \cref{sec:applications} we demonstrate the
applicability of \theoryabbv for relating familiar grammar and
automata formalisms as well as building concrete parsers.  Then in
\cref{sec:semantics-and-metatheory}, we give a denotational semantics
that makes precise the connection between \theoryabbv syntax and
formal grammars. Finally in \Cref{sec:discussion} we discuss related
and future work.

\section{\theoryname by Example}
\label{sec:type-theory-examples}
To gain intuition for working in \theoryabbv, we begin with some
illustrative examples drawn from the theory of formal languages. Each
of our examples will be defined for strings over the three character
alphabet $\Sigma = \{ \texttt{a} , \texttt{b}, \texttt{c} \}$.

\newcommand{\A}{\texttt{A}}
\newcommand{\B}{\texttt{B}}
\newcommand{\I}{\texttt{I}}
\newcommand{\f}{\texttt{f}}
\newcommand{\g}{\texttt{g}}
\renewcommand{\L}{\texttt{L}}
\renewcommand{\a}{\texttt{a}}
\renewcommand{\b}{\texttt{b}}
\renewcommand{\c}{\texttt{c}}
\newcommand{\w}{\texttt{w}}

\paragraph{Finite Grammars}
First consider finite grammars --- those built from base types via disjunctions and
concatenations. The base types comprise characters drawn from the alphabet, the
empty string, and the empty grammar.
For each character $a$ in the alphabet we have a type $\literal a$ which
has a single parse tree for the string
\stringquote{a} and no parse trees at any other strings. The grammar $\I$ has a single
parse tree for the empty string $\epsilon = \stringquote{}$ and no parses for any other strings.
The final base type, the empty grammar $0$, has no parses for any string. We
use type-theoretic syntax to represent disjunction $\oplus$ and concatenation
$\otimes$ of
grammars. Over an input string $\w$, a parse of the disjunction $\A \oplus \B$ is either
a parse of $\A$ over the string $\w$ or a
parse of $\B$ over the string $\w$. Similarly, $\w$ matches $\A \otimes \B$ if
$\w$ can be split into two strings $\w_{\A}$ and $\w_{\B}$ that match $\A$
and $\B$, respectively.

For a type $\A$, a parse tree of a string $\w$ is represented as a
term of type $\A$ in the context $\internalize \w$, where
$\internalize \w$ is a context with one variable for each character of
$\w$. For example, to define a parse tree for \stringquote{ab}, we use
the context $\internalize{\stringquote{ab}} = a : \literal a , b :
\literal b$. On the left side of \cref{fig:fingram}, we give a typing
derivation that shows that $\inl(a \otimes b)$ defines a valid parse
tree for the finite grammar $(\literal a \otimes \literal b) \oplus
\literal c$. Underneath the derivation in \cref{fig:fingram}, we give
a linear $\lambda$-term that implements the program given in the
derivation.\max{reverse the order of these?}

\begin{figure}
\begin{mathpar}
  \inferrule
  {
    \inferrule
    {
      \inferrule
      {~}
      {a : \literal a \vdash a : \literal a}
      \\
      \inferrule
      {~}
      {b : \literal b \vdash b : \literal b}
    }
    {a : \literal a , b : \literal b \vdash a \otimes b : \literal a \otimes
      \literal b}
  }
  {a : \literal a , b : \literal b \vdash \texttt{f} := \inl(a \otimes b) :
    (\literal a \otimes \literal b) \oplus \literal c}
\end{mathpar}
\begin{lstlisting}
f : (*@$\uparrow$@*)('a' (*@$\otimes$@*) 'b' (*@$\lto$@*) ('a' (*@$\otimes$@*) 'b') (*@$\oplus$@*) 'c')
f (a , b) = inl (a (*@$\otimes$@*) b)
\end{lstlisting}
\caption{\stringquote{ac} matches
  $(\literal a \oplus \literal b) \otimes \literal c$}
\label{fig:fingram}
\end{figure}
For this interpretation of parse trees as terms to make sense, our
calculus cannot allow for \emph{any} of the usual structural rules of
type theory: weakening, contraction and exchange. Weakening allows
for variables to go unused, while contraction allows for the same
variable to be used twice, but in a parse tree, every character must
be accounted for exactly once. That is, we want to prevent the following
erroneous derivations,
\[
  a : \literal a , b : \literal b \not \vdash a : \literal a \qquad a : \literal a , b : \literal b \not \vdash (a, a) : \literal a \otimes \literal a
\]

%
Finally, the ordering of characters in a string cannot be ignored while
parsing, so we omit the exchange rule because it would allow
for variables in the context to be reordered,
\[
  a : \literal a , b : \literal b \not \vdash (b , a) : \literal b \otimes \literal a
\]
% otherwise we would have that $\inl(x\otimes y)$ is a valid
% term in context $y:b,x:a$, but there should be no parses of $ba$ in
% this grammar.

\paragraph{Regular Expressions}
Regular expressions can be encoded as types generated by base types,
$\oplus$, and $\otimes$, and the Kleene star $(\cdot)^{\ast}$.  For a
grammar $\A$, we define the Kleene star $\A^{*}$ as a particular
\emph{inductive linear type} of linear lists, as shown in
\cref{fig:kleenestarinductive}. Here $\A^{*} : \L$ means we are defining
a \emph{linear} type. $\A^{*}$ has two constructors: $\nil$, which
builds a parse of type $\A^{*}$ from nothing; and $\cons$, which
linearly consumes a parse of $\A$ and a parse of $\A^{*}$ and builds a
parse of $\A^{*}$. This linear consumption is defined by the linear
function type $\lto$. The linear function type $\A \lto \B$ defines functions that
take in parses of $\A$ as input, \emph{consume} the input, and return a parse of
$\B$ as output. The arrow, $\uparrow$, wrapping these
constructors means that the constructors by themselves are not consumed
upon usage, and so are \emph{non-linear} values themselves. That is, the
names $\nil$ and $\cons$ are function symbols that may be reused as many times
as we wish.

\begin{figure}
\begin{lstlisting}
data A(*@$^*$@*) : L where
  nil : (*@$\uparrow$@*)(A(*@$^*$@*))
  cons : (*@$\uparrow$@*)(A (*@$\lto$@*) A(*@$^*$@*) (*@$\lto$@*) A(*@$^*$@*))
\end{lstlisting}
% \begin{align*}
% \data &~A^{*} : L~\where\\
%       & \nil : \ltonl {A^{*}} \\
%       & \cons : \ltonl {(A \lto A^{*} \lto A^{*})}
% \end{align*}
\caption{Kleene Star as an inductive type}
\label{fig:kleenestarinductive}
\end{figure}

Through repeated application of the Kleene star constructors,
\cref{fig:kleenestarderivation} gives a derivation that shows
\stringquote{ab} matches the regular expression $({\literal a}^{*}
\otimes \literal b) \oplus \literal c$. The leaves of the proof tree
that mention the arrow $\uparrow$ describe a cast from a non-linear
type to a linear type.  For instance, the premise of the leaf
involving $\nil$ views $\nil : \ltonl {({\literal a}^{*})}$ as the
name of a constructor, and a constructor should be nonlinearly valued
because we may call it several times (or not at all). However, the
conclusion of this leaf views $\nil : {\literal a}^{*}$ as a linear
value, which in our syntax is an implicit coercion from a nonlinear
value to a linear value. After we call the constructor it ``returns''
a value that may only be used a single time.

\steven{TODO fig 3 sizing}
\begin{figure}
\begin{mathpar}
\footnotesize
\inferrule
{
  \inferrule
  {
    \inferrule
    {
      \inferrule
      {
        \inferrule
        {
          \inferrule
          {~}
          {\cdot \vdash \cons : \ltonl {({\literal a} \lto {\literal a}^* \lto {\literal a}^*)}}
        }
        {\cdot \vdash \cons : {\literal a} \lto {\literal a}^* \lto {\literal a}^*}
        \\
        \inferrule
        {~}
        {a : {\literal a} \vdash a : {\literal a}}
      }
      {a : a \vdash \cons~a : {\literal a}^* \lto {\literal a}^*}
      \\
      \inferrule
      {
        \inferrule
        {~}
        {\cdot \vdash \nil : \ltonl {({\literal a}^*)}}
      }
      {\cdot \vdash \nil : {\literal a}^*}
    }
    {a : {\literal a} \vdash \cons~a~\nil : {\literal a}^{*}}
    \\
    \inferrule
    {~}
    {b : {\literal b} \vdash b : {\literal b}}
  }
  {a : {\literal a} , b : {\literal b} \vdash (\cons~a~\nil) \otimes b : {\literal a}^{*} \otimes {\literal b}}
}
{a : \literal a , b : \literal b \vdash \texttt{g} := \inl ((\cons~a~\nil)
  \otimes b) : ({\literal a}^{*} \otimes \literal b) \oplus \literal c}
\end{mathpar}
\begin{lstlisting}
g : (*@$\uparrow$@*)(('a' (*@$\otimes$@*) 'b') (*@$\lto$@*) ('a'(*@$^*$@*) (*@$\otimes$@*) 'b') (*@$\oplus$@*) 'c')
g (a , b) = inl (cons a' nil (*@$\otimes$@*) b')
\end{lstlisting}
\caption{\stringquote{ab} matches
  $({\literal a}^{*} \otimes \literal b) \oplus \literal c$}
\label{fig:kleenestarderivation}
\end{figure}

We may also have derivations where the term in context is not simply a
string of literals. In \cref{fig:kleeneabstractproof} we show that every parse
of the grammar $(A \otimes A)^{*}$ induces a parse of $A^{*}$ for an
arbitrary grammar $A$. The context $(A \otimes A)^{*}$ does not correspond directly to a string, so it is
not quite appropriate
to think of a linear term here as a parse \textit{trees}.
The
context $a : (A \otimes A)^{*}$ does not contain concrete data to be parsed; rather, there may be many choices of string underlying the parse tree captured
by the variable $a$. Thus, the term $\texttt{h}$ from
\cref{fig:kleeneabstractproof} is not a parse of a string, and
it is more
appropriate to think of it as a parse \textit{transformer} --- a function from
parses of $(\A \otimes \A)^{*}$ to parse of $\A^{*}$.

We define $\texttt{h}$ by recursion on terms of type $(\A \otimes \A)^{*}$.
This recursion is expressed in the derivation tree by invoking the
elimination principle for Kleene star, written
as $\fold$. The parse transformer $\texttt{h}$ is more compactly presented in the pseudocode
of \cref{fig:kleeneabstractproof} by pattern matching on the input and making an
explicit recursive call in the body of its definition.

\begin{figure}
\begin{mathpar}
  \inferrule
  {
    \inferrule
    {~}
    {\cdot \vdash \nil : \A^*}
    \\
    \inferrule
    {
      \inferrule
      {
        \inferrule
        {~}
        {a_1 : \A, a_2 : \A , a_{*} : \A^* \vdash \cons (a_1 , \cons(a_2 , a_{*})) : \A^*}
      }
      {a' : \A \otimes \A , a_{*} : \A^* \vdash g := \letin {a_1 \otimes a_2} {a'} {\cons (a_1 , \cons(a_2 , a_{*})) : \A^*}}
    }
    {\cdot \vdash f := \lamblto a {\lamblto {a_{*}} {\app {\app g a} {a_{*}} }} : (\A \otimes \A) \lto \A^* \lto \A^*}
  }
  {a : (\A \otimes \A)^* \vdash \texttt{h} := \fold(\nil , f)(a) : \A^* }
\end{mathpar}
\begin{lstlisting}
h : (*@$\uparrow$@*)((A (*@$\otimes$@*) A)(*@$^*$@*) (*@$\lto$@*) A(*@$^*$@*))
h nil = nil
h (cons (a1 (*@$\otimes$@*) a2) as) = cons a1 (cons a2 (h as))
\end{lstlisting}
\caption{A parse transformer for abstract grammars}
\label{fig:kleeneabstractproof}
\end{figure}

\paragraph{Non-deterministic Finite Automata}
\newcommand{\s}{\texttt{s}}
\newcommand{\0}{\texttt{0}}
\newcommand{\1}{\texttt{1}}
\newcommand{\2}{\texttt{2}}
Regular expressions are a compact formalism for defining a formal grammar,
but an expression such as
$({\literal a}^{*} \otimes \literal b) \oplus \literal c$ does not give a very operational perspective of how to parse
it. For this reason, most parsers are based on compiling a grammar to a corresponding notion of automaton, which is readily implemented. To impleemnt
these algorithms in \theoryabbv, we need a way to represent automata as types in the same way we can represent regular expressions.

\max{combine these three figures into one?}
Finite automata are precisely the class of machines that recognize regular
expressions. \cref{fig:exampleNFA} shows a non-deterministic
finite automaton (NFA) for the regular expression
$({\literal a}^{*} \otimes \literal b) \oplus \literal c$, along with a type $\Trace$, an \emph{indexed} inductive linear type of traces through this automaton. Defining an indexed inductive type can be thought
of as defining a family of mutually recursive inductive types, one for each element of the indexing type. Here $\Trace$ is 
uses an index $\s : \texttt{Fin 3}$ which picks out which
state in the automaton a trace begins at --- where $\texttt{Fin 3}$ is the
finite type containing inhabitants $\{\0 , \1 , \2\}$. We can think of this as defining three mutually recursive inductive types $\Trace~\0$,
$\Trace~\1$, and $\Trace~\2$.
%
There are three kinds of constructors for $\Trace$: (1) those that
terminate traces, (2) those that correspond to transitions labeled by
a character, and (3) those that correspond to transitions labeled by
the empty string $\epsilon$. The constructor $\texttt{stop}$
terminates a trace in the accepting state $\2$. The constructors
$\texttt{1to1}$, $\texttt{1to2}$, $\texttt{0to2}$ each define a
labeled transition through the NFA, and each of these consumes a parse
of the label's character and a trace beginning at the destination of a
transition to produce a trace beginning at the source of a
transition. The constructor $\texttt{0to1}$ behaves similarly, except
its transition is labeled with the empty string $\epsilon$. Therefore,
$\texttt{0to1}$ takes in a trace beginning a state $\1$ and returns a
trace beginning at state $\0$ corresponding to the same underlying
string.

Just as \cref{fig:kleenestarderivation} shows the \stringquote{ab} matches
$({\literal a}^{*} \otimes \literal b) \oplus \literal c$, \cref{fig:nfaderivation} demonstrates that \stringquote{ab} is
accepted by the NFA representation of this regular expression. In particular, we
show that \stringquote{ab} matches the grammar $\Trace~0$ since $\0$ is the
initial state of the NFA.\ If we construct parse transformers from
$({\literal a}^{*} \otimes \literal b) \oplus \literal c$ to $\Trace~\0$, and vice versa, we would demonstrate that the regular expression
and the automaton capture the same language --- i.e.\ they recognize the same
set of strings. As we will see in \cref{sec:applications}, we can indeed define parse
transformers between regular expressions and their associated automaton.
Moreover, we prove that these parse transformers define a bijective
correspondence of parses over every input string. That is, in a strong sense the
parse of a regular expression and the traces of their associated automata
capture precisely the same data, i.e., that they are \emph{strongly equivalent} as grammar formalisms.

\begin{figure}
  \begin{tikzpicture}[node distance = 25mm ]
    \node[state, initial] (0) {$\0$};
    \node[state, below left of=0] (1) {$\1$};
    \node[state, right of=1, accepting] (2) {$\2$};

    \path[->] (0) edge[above] node{$\epsilon$} (1)
              (0) edge[right] node{$\stringquote{c}$} (2)
              (1) edge[loop left] node{$\stringquote{a}$} (1)
              (1) edge[below] node{$\stringquote{b}$} (2);
  \end{tikzpicture}
  \begin{lstlisting}
data Trace : (s : Fin 3) (*@$\to$@*) L where
  stop : (*@$\uparrow$@*)(Trace 2)
  1to1 : (*@$\uparrow$@*)('a' (*@$\lto$@*) Trace 1 (*@$\lto$@*) Trace 1)
  1to2 : (*@$\uparrow$@*)('b' (*@$\lto$@*) Trace 2 (*@$\lto$@*) Trace 1)
  0to2 : (*@$\uparrow$@*)('c' (*@$\lto$@*) Trace 2 (*@$\lto$@*) Trace 0)
  0to1 : (*@$\uparrow$@*)(Trace 1 (*@$\lto$@*) Trace 0)
\end{lstlisting}
  \caption{NFA for $(\a^{*} \otimes \b) \oplus \c$ and its corresponding type}
  \label{fig:exampleNFA}
\end{figure}

\begin{figure}
\begin{lstlisting}
k : (*@$\uparrow$@*)(('a' (*@$\otimes$@*) 'b') (*@$\lto$@*)  Trace 0)
k (a , b) = 0to1 (1to1 a (1to2 b stop))
\end{lstlisting}
\caption{\stringquote{ab} is accepted by the NFA for
  $({\literal a}^{*} \otimes {\literal b }) \oplus \literal c$}
\label{fig:nfaderivation}
\end{figure}

%% 3. Third example: NFA traces to show the indexed inductive types

\section{Syntax and Typing for \theoryname}
\label{sec:tt}

%% OUTLINE
%%
%% 1. Overview of syntax: what are the judgments
%% 2. Cover the linear types except for inductives
%% 3. Cover non-linear types, mainly focus on the universes
%% 4. Cover indexed inductive linear types
%% 5. Cover the "axioms" we add

The design of \theoryabbv is based the dependent
linear-non-linear calculus (\lnld) and Lambek calculus, also known as
non-commutative linear logic.
\cite{krishnaswami_integrating_2015,lambekCalculus}. As in \lnld,
\theoryabbv includes both non-linear dependent types, as well as
linear types, which are allowed to depend on the non-linear types, but
not on other linear types.
%
The main point of departure from \lnld's design is that, as in Lambek calculus \cite{lambekCalculus}, the linear
typing is \emph{non-commutative}, i.e., that exchange is not an
admissible structural rule. Furthermore, we add a general-purpose
indexed inductive linear type connective, as well as an
\emph{equalizer} type, which we will show allows us to perform
inductive proofs of equalities between linear terms.
%
Finally, while \lnld was enhanced with special connectives
inspired by separation logic to model imperative programming, we
instead add base types and axioms to the system specifically to model
formal grammars and parsing.

The formation rules for the judgments of \theoryabbv are shown in
\Cref{fig:formation}. $\Gamma$ stand for non-linear contexts, $X,Y,Z$
stand for non-linear types, $M,N$ stand for non-linear terms, these
act as in an ordinary dependent type theory. $\Delta$ stands for
linear contexts, $A,B,C$ for linear types and $e,f,g$ for linear
terms. These contexts, types and terms are allowed to depend on an
ambient non-linear context $\Gamma$, but note that linear types $A$
cannot depend on any \emph{linear} variables in $\Delta$. We include
definitional equality judgments for both kinds of type and term
judgments as well. Additionally, we have a judgment $\Gamma\vdash X
\isSmall$ which is used in the definition of universe types.

\begin{figure}
  \begin{mathpar}
    \inferrule{~}{\ctxwff \Gamma}

    \inferrule{\ctxwff \Gamma}{\ctxwffjdg \Gamma X}

    \inferrule{\ctxwffjdg \Gamma X}{\Gamma \vdash X \isSmall}

    \inferrule{\ctxwffjdg \Gamma X \and \ctxwffjdg \Gamma Y}{\ctxwffjdg \Gamma {X \equiv Y}}

    \inferrule{\ctxwffjdg \Gamma X}{\Gamma \vdash M : X}

    \inferrule{\Gamma \vdash M : X \and \Gamma \vdash N : X}{\Gamma \vdash M \equiv N : X}

    \inferrule{\ctxwff \Gamma}{\linctxwff \Gamma \Delta}

    \inferrule{\ctxwff \Gamma}{\Gamma \vdash A \isLinTy}

    \inferrule{\Gamma \vdash A \isLinTy \and \Gamma \vdash B \isLinTy}{\Gamma \vdash A \equiv B}

    \inferrule{\Gamma \vdash \Delta \isLinCtx \and \Gamma \vdash A \isLinTy}{\Gamma;\Delta\vdash e : A}

    \inferrule{\Gamma; \Delta \vdash e : A \and \Gamma;\Delta \vdash f : A}{\Gamma;\Delta\vdash e \equiv f : A}
  \end{mathpar}
  \caption{Formation Rules}
  \label{fig:formation}
\end{figure}

\subsection{Non-linear Typing}
The non-linear types of \theoryabbv include the standard dependent
types for $\Pi,\Sigma$, extensional equality, natural numbers,
booleans, unit and empty types, and we assume function
extensionality\cite{standard-type-theory-reference}. We present the
other non-linear type constructions in
\Cref{fig:non-linear-stuff}. First, we include universe types $U$ of
small non-linear types and $L$ of linear types. These are defined as
universes ``ala Coquand'' in that we define a judgment saying when a
non-linear type is small and define the universe to internalize
precisely this judgment \cite{universesAlaCoquand}. These universe
types are needed so that we can define types by recursion on natural
numbers. Next, we include a non-linear type $\ltonl A$ where $A$ is a
linear type. The intuition for this type is that its elements are the
linear terms that are ``resource free'': its introduction rule says we
can construct an $\ltonl A$ when we have a linear term of type $A$
with no free linear variables. This type is used extensively in our
examples, playing a similar role to the $!$ modality of ordinary
linear logic or the persistence modality $\square$ of separation logic
\cite{linearlogic,persistencemodalityseparationlogic}.

%% \begin{figure}
%%   \[
%%   \begin{array}{rrcl}
%%     \textrm{non-linear contexts} & \Gamma & ::= & \cdot \pipe \Gamma,x:X \\
%%     \textrm{non-linear types} & X , Y , Z & ::= & \mathsf{Nat} \pipe
%%                                                   \mathsf{Bool} \pipe
%%                                                   \mathsf{Empty} \pipe
%%                                                   \mathsf{Unit} \pipe
%%                                                   \mathsf{SPF}\,X\pipe U \pipe L
%%                                                   \pipe \ltonl A \pipe \sum\limits_{x:X} Y
%%                                                   \pipe \prod\limits_{x:X} Y\pipe M =_A
%%                                                   N
%%   \end{array}
%%   \]
%%   \caption{Non-linear Types}
%% \end{figure}
\begin{figure}
  \begin{mathpar}
    \inferrule{}{\Gamma \vdash U \isTy}\and
    \inferrule
    {\Gamma \vdash M : U}
    {\Gamma \vdash \unquoteTy M \isTy}\and
    \inferrule
    {\Gamma \vdash X \isTy}
    {\Gamma \vdash \quoteTy X : U}\and
    \unquoteTy{\quoteTy{X}} \equiv X \and \inferrule{\Gamma \vdash M : U}{\Gamma\vdash \quoteTy{\unquoteTy{M}}\equiv M : U}

    \inferrule{}{\Gamma \vdash L \isTy}\and
    \inferrule
    {\Gamma \vdash M : L}
    {\Gamma \vdash \unquoteTy M \isLinTy}\and
    \inferrule
    {\Gamma \vdash A \isLinTy}
    {\Gamma \vdash \quoteTy A : L}\and
    \unquoteTy{\quoteTy{A}} \equiv A \and \inferrule{\Gamma \vdash M : U}{\Gamma\vdash \quoteTy{\unquoteTy{M}}\equiv M : U}

    \inferrule
    {\Gamma \vdash A \isLinTy}
    {\Gamma \vdash \ltonl { A } \isTy}
    \and
    \inferrule{\Gamma ; \cdot \vdash e : A}{\Gamma \vdash e : \ltonl A}\and
    \inferrule{\Gamma \vdash M : \ltonl A}{\Gamma ; \cdot \vdash M : A} \and
  \end{mathpar}
  \caption{Non-linear types (selection)}
  \label{fig:non-linear-types}
\end{figure}
\max{todo: decide if we shoudl keep all of these rules or not}

\subsection{Linear Typing}

We give an overview of the linear types in \Cref{fig:linear-types} and
terms in \Cref{fig:linear-terms}. The equational theory for these
types is straightforward and included in the appendix. First, the
linear variable rule says that a linear variable can be used if it is
the \emph{only} variable in the context.

First, we cover the ``multiplicative'' connectives of non-commutative
linear logic. The linear unit ($\I$) and tensor product ($\otimes$)
are standard for a non-commutative linear logic: when we construct a
linear unit we cannot use any variables and when we construct a tensor
product, the two sides must use disjoint variables, and the variables
the left side of the product uses must be to the left in the context
of the variables used by the right side of the tensor product. The
elimination rules for unit and tensor are given by pattern
matching. The pattern matching rules split the linear context into
three pieces $\Delta_1,\Delta_2,\Delta_3$: the middle $\Delta_2$ is
used by the scrutinee of the pattern match, and in the continuation
this context is replaced by the variables brought into scope by the
pattern match. This ensures that pattern matches maintain the proper
ordering of resource usage.

Because we are non-commutative, there are two function types: $A \lto
B$ and $B \tol A$, which have similar $\lambda$ introduction forms and
application elimination forms. The difference between these is that
the introduction rule for $A \lto B$ adds a variable to the right side
of the context, whereas the introduction rule (elided) for $B \tol A$ adds a
variable to the left side of the context. In our experience, because
by convention parsing algorithms parse from left-to-right, we rarely
need to use the $B \tol A$ connective. As we have already seen, the
$\lto$ connective is frequently used in conjunction with the
$\uparrow$ connective so that we can abstract non-linearly over linear
functions.

Next, we cover the ``additive'' connectives. First, we use the
non-linear types to define \emph{indexed} versions of the additive
disjunction $\oplus$ and additive conjunction $\&$ of linear logic,
which can be thought of as linear versions of the $\Sigma$ and $\Pi$
connectives of ordinary dependent type theory, respectively. The
indexed $\&$ is defined by a $\lambda$ that brings a \emph{non-linear}
variable into scope and eliminated using projection where the index
specified is given by a non-linear term. The rules for indexed
$\oplus$ are analogous to a \emph{weak} $\Sigma$ type: it has an
injection introduction rule $\sigma$, but its elimination rule is
given by \emph{pattern matching} rather than projections
\cite{weakSigmatype}. We can define the more typical nullary and
binary versions of these connectives by using indexing over the empty
and boolean type respectively. We will freely use $0$ to refer to this
empty disjunction and $\top$ to refer to the empty conjunction, and
use infix $\oplus/\&$ for binary disjunction/conjunction.
%
In addition to the introduction and elimination rules for
$\bigamp,\bigoplus$, our development requires an additional axiom that
is not proveable from these rules: that additive conjunction
\emph{distributes} over additive disjunction, e.g., in the finitary
case that $0 \& A \cong 0$ and $(A + B) \& C \cong (A \& C) + (B \&
C)$. In its most general form, the axiom says that the definable function
${\bigoplus\limits_{f : \prod_{x:X}
    Y(x)}\bigamp\limits_{x:X}A\,x\,(f\,x) \lto
  \bigamp\limits_{x:X}\bigoplus\limits_{y:Y(x)}A\,x\,y}$ has an
inverse.

Lastly, we include a type $\equalizer {a}{f}{g}$ that we call the
\emph{equalizer} of linear functions $f$ and $g$. We think of this
type as the ``subtype'' of elements of $A$ that satisfy the equation
$f\,a\equiv g\,a$. Note that it is important here that $f, g$
themselves are non-linearly used functions, as linear values cannot be
used in a type.  Equalizer types are not needed for non-linear types
since they can be constructed using the equality type as $\sum_{x:X}
f\,x=_Y g\,x$, but this construction can't be used for linear types
because it uses a \emph{dependent} version of the equality type, which
we cannot define as a linear type. While the equalizer type is not
used directly in defining any of our parsers or formal grammars, it is
used for several proofs, allowing for inductive arguments about our
indexed inductive types.

\begin{figure}
  \begin{mathpar}
    \inferrule{}{\Gamma \vdash \I \isLinTy}\and
    \inferrule{\Gamma \vdash A \isLinTy \and \Gamma \vdash B \isLinTy}{\Gamma \vdash A \otimes B \isLinTy}\and
    \inferrule{\Gamma \vdash A \isLinTy \and \Gamma \vdash B \isLinTy}{\Gamma \vdash A \lto B \isLinTy}\and
    \inferrule{\Gamma \vdash A \isLinTy \and \Gamma \vdash B \isLinTy}{\Gamma \vdash A \tol B \isLinTy}\and
    \inferrule{\Gamma,x:X \vdash A \isLinTy}{\Gamma \vdash \bigoplus\limits_{x:X} A}\and
    \inferrule{\Gamma,x:X \vdash A \isLinTy}{\Gamma \vdash \bigamp\limits_{x:X} A}\and
    \inferrule
    {\Gamma \vdash f : \ltonl {(A \lto B)} \and \Gamma \vdash g : \ltonl { (A \lto B) }}
    {\Gamma \vdash \equalizer {a}{f}{g} \isLinTy}
  \end{mathpar}
  \caption{Linear Types (selection)}
\end{figure}

\begin{figure}
  \begin{mathpar}
    \inferrule{~}{\Gamma ; a : A \vdash a : A}
    \and
    \inferrule{\Gamma ; \Delta \vdash e : B \\ \linctxwffjdg \Gamma {A \equiv B}}{\Gamma ; \Delta \vdash e : A}
    %
    \\
    %
    \inferrule{~}{\Gamma ; \cdot \vdash () : I}
    \and
    \inferrule{\Gamma ; \Delta_2 \vdash e : I \\ \Gamma ; \Delta_1,\Delta_3 \vdash e' : C}{\Gamma ; \Delta_1,\Delta_2,\Delta_3 \vdash \letin {()} e {e'} : C}
    %
    \\
    %
    \inferrule{\Gamma ; \Delta \vdash e : A \\ \Gamma ; \Delta' \vdash e' : B}{\Gamma ; \Delta, \Delta' \vdash (e , e') : A \otimes B}
    %
    \and
    %
    \inferrule{\Gamma ; \Delta_2 \vdash e : A \otimes B \\ \Gamma ; \Delta_1, a : A, b : B, \Delta_2 \vdash e' : C}{\Gamma ;  \Delta_1, \Delta_2, \Delta_3 \vdash \letin {(a , b)} e {e'} : C}
    \\
    %
    \inferrule{\Gamma ; \Delta , a : A \vdash e : B}{\Gamma ; \Delta \vdash \lamblto a e : A\lto B}
    \and
    \inferrule{\Gamma ; \Delta' \vdash e' : A \\ \Gamma ; \Delta \vdash e : A \lto B}{\Gamma ; \Delta, \Delta' \vdash \applto {e'} {e} : B}
    \\
    %
    \inferrule{\Gamma, x : X ; \Delta  \vdash e : A}
              {\Gamma ; \Delta \vdash \dlamb x e : \LinPiTy x X A}
    \and
    \inferrule{\Gamma ; \Delta \vdash e : \LinPiTy x X A \\ \Gamma \vdash M : X}{\Gamma ; \Delta \vdash e\,.\pi\,M : \subst A {M} x}
    %
    \\
    %
    \inferrule{\Gamma \vdash M : X \quad \Gamma ; \Delta \vdash e : \subst A M x}{\Gamma ; \Delta \vdash \sigma\,M\,e : \bigoplus\limits_{x:X} A}
    %
    \and
    %
    \inferrule{\Gamma ; \Delta_2 \vdash e : \bigoplus\limits_{x:X} A \quad \Gamma, x : X ; \Delta_1, a : A, \Delta_3 \vdash e' : C}{\Gamma; \Delta_1, \Delta_2, \Delta_3 \vdash \letin {\sigma\,x\,a} e {e'}: C}
    %
    \and
    %
    \inferrule
    { \Gamma ; \Delta \vdash e : A \\
      \Gamma ; \Delta \vdash \applto {f}{e} \equiv \applto {g}{e}}
    {\Gamma ; \Delta \vdash \equalizerin{e} : \equalizer {a}{f}{g}}
    %
    \and
    %
    \inferrule
    {\Gamma ; \Delta \vdash e : \equalizer{a}{f}{g}}
    {\Gamma ; \Delta \vdash \equalizerpi {e} : A}
  \end{mathpar}
  \caption{Linear terms (selection)}
  \label{fig:linsyntax}
\end{figure}

\subsection{Indexed Inductive Linear Types}

Next, we introduce the most complex and important linear type
constructors of our development, \emph{indexed inductive linear
types}. We encode these by adding a mechanism for constructing initial
algebras of strictly positive functorial type expressions, following
prior work on inductive types
\cite{quantitativePolynomialFunctors,indexedContainers}. The syntax is
given in \Cref{fig:iilt}. First, we add a non-linear type $\SPF\,X$ of
\emph{strictly positive functorial} linear type expressions indexed by
a non-linear type $X$. We think of the elements of this type as
syntactic descriptions of linear types that are parameterized by
$X$-many variables standing for linear types that are only used in
strictly positive positions. Accordingly, the $\SPF\,X$ type supports
an operation $\el$ that interprets it as such a type constructor, as
well as an operator $\map$ that defines a functorial action on parse
transformers. The $\SPF\,X$ type supports constructors for a reference
$\Var~x$ to one of the linear type variables, a constant expression
that doesn't mention any type variables $K$, as well as tensor
products and additive conjunction and disjunction of type expressions.
We additionally add equations in the appendix that say that the
$\el$/$\map$ operations correspond to these descriptions of the
constructors.

\begin{figure}
  \begin{footnotesize}
    \begin{mathpar}
      \inferrule{\Gamma \vdash X\isTy}{\Gamma \vdash \SPF\,X \isTy}\and
      \inferrule{\Gamma \vdash X\isSmall}{\Gamma \vdash \SPF\,X \isSmall}\and
    \el : \prod_{X:U}\SPF\,X \to (X \to L) \to L\and
    \map : \prod_{X:U}\prod_{F : \SPF\,X}\prod_{A,B:X\to L}{\left(\prod_{x:X}\uparrow(\unquoteTy{A\,x}\lto \unquoteTy{B\,x})\right)} \to {\uparrow(\unquoteTy{\el(F)(A)} \lto \unquoteTy{\el(F)(B)})}\and
    \mathsf{Var} : \prod_{X:U} X \to \SPF\,X\and
    \mathsf{K} : \prod_{X:U} L \to \SPF\,X\and
    \mathsf{\bigoplus} : \prod_{X:U}\prod_{Y:U}(Y \to \SPF\,X) \to \SPF\,X\and
    \mathsf{\bigamp} : \prod_{X:U}\prod_{Y:U}(Y \to \SPF\,X) \to \SPF\,X\and
    \mathsf{\otimes} : \prod_{X:U}\SPF\,X \to \SPF\,X \to \SPF\,X\and
    \roll : \prod_{X:U}\prod_{F:X \to \SPF\,X}\prod_{x:X}\ltonl{(\el(F\,x)(\mu\,F))}\and
    \fold : \prod_{X:U}\prod_{F:X\to\SPF\,X}\prod_{A:X \to L}
    \left(\prod_{x:X}\ltonl{(\unquoteTy{\el(F\,x)(\unquoteTy{A})} \lto \unquoteTy{A\,x})}\right)
    \to \prod_{x:X}\ltonl{(\mu F\,x \lto A\,x)}\and

    \inferrule*[right=Ind$\beta$]
    {\Gamma \vdash f : \prod_{x:X}\ltonl{(\el(F\,x)(A) \lto A\,x)} \and \Gamma; \Delta \vdash e : \el(F\,x)(\mu\,F)}
    {\Gamma;\Delta \vdash \fold\,F\,f\,x\,(\roll e) \equiv f\,x\,(\map(F\,x)\,(\fold\,F\,f)) : A\,x}

    \inferrule*[right=Ind$\eta$]
    {\Gamma \vdash f : \prod_{x:X}\ltonl{(\el(F\,x)(A) \lto A\,x)}
     \and \Gamma \vdash e : \prod_{x:X}\ltonl{(\mu F\,x \lto A\,x)}\\\\
      \Gamma,x:X;a:\el(F\,x)(\mu F) \vdash e\,x\,(\roll a) \equiv f\,x\,(\map(F\,x)\,e) : A\,x }
    {\fold\,F\,f\equiv e' : \prod_{x:X}\ltonl{(\mu F\,x \lto A\,x)}}
  \end{mathpar}
  \end{footnotesize}
  %% \begin{footnotesize}
  %%   \begin{align*}
  %%   \mathsf{Var} &: \prod_{X:U} X \to \SPF\,X\\
  %%   \mathsf{K} &: \prod_{X:U} L \to \SPF\,X\\
  %%   \mathsf{\bigoplus} &: \prod_{X:U}\prod_{Y:U}(Y \to \SPF\,X) \to \SPF\,X\\
  %%   \mathsf{\bigamp} &: \prod_{X:U}\prod_{Y:U}(Y \to \SPF\,X) \to \SPF\,X\\
  %%   \mathsf{\otimes} &: \prod_{X:U}\SPF\,X \to \SPF\,X \to \SPF\,X\\
  %%   \el &: \prod_{X:U}\SPF\,X \to (X \to A) \to L\\
  %%   \map &: \prod_{X:U}\prod_{F : \SPF\,X}\prod_{A,B:X\to L}{\left(\prod_{x:X}\uparrow(\unquoteTy{A\,x}\lto \unquoteTy{B\,x})\right)} \to {\uparrow(\unquoteTy{\el(F)(A)} \lto \unquoteTy{\el(F)(B)})}\\\\
  %%   \roll &: \prod_{X:U}\prod_{F:X \to \SPF\,X}\prod_{x:X}\ltonl{(\el(F\,x)(\mu\,F))}\\
  %%   \fold &: \prod_{X:U}\prod_{F:X\to\SPF\,X}\prod_{A:X \to L}
  %%   \left(\prod_{x:X}\ltonl{(\el(F\,x)(A) \lto A\,x)}\right)
  %%   \to \prod_{x:X}\ltonl{(\mu F\,x \lto A\,x)}
  %% \end{align*}
  %% \end{footnotesize}
  \caption{Strictly Positive Functors and Indexed Inductive Linear Types}
  \label{fig:iilt}
\end{figure}

Next, given a family of $X$-many strictly positive linear type
expressions $F : X \to \SPF\,X$, we define a family $\mu F : X \to L$
of $X$-many mutually recursive inductive types. The introduction rule
for this is $\roll$, which constructs an element of $\mu F\,x$ from
the one-level of the $x$th type expression. The elimination principle
is defined by a mutual $\fold$ operation: given a family of output
types $A$ indexed by $X$, we can define a family of functions from
$\mu F\,x \multimap A\,x$ if you specify how to interpret all of the
constructors as operations on $A$ values. We add $\beta\eta$ equations
that specify that this makes the family $\mu F$ into an \emph{initial
algebra} for the functor $\el(F)$. That is the $\beta$ rule says that
a $\mathsf{fold}$ applied to a $\mathsf{roll}$ is equivalent to
$\mathsf{map}$ping the $\mathsf{fold}$ over all the sub-expressions,
which means that $\mathsf{fold}$ interprets all of the constructors
homomorphically using the provided interpretation $\mathsf{f}$. Then
the $\eta$ rule says that $\mathsf{fold}$ is the \emph{unique} such
homomorphism, i.e. anything that satisfies the recurrence equation of
the $\mathsf{fold}$ is equal to it.

This definition as an initial algebra is well-understood semantically
but the $\eta$ principle in particular is somewhat cumbersome to use
directly in proofs. In dependent type theory, we would have a
dependent \emph{elimination} principle, which can be used to
implemetns functions by recursion as well as proofs by
induction. However since linear types do not support dependency on
linear types, we cannot directly adapt this approach. However, if we
are trying to prove that two morphisms out of a mutually recursive
type are equal, we can use the \emph{equalizer} type to prove their
equality by induction. That is, if our goal is to prove two functions
$f,g : \uparrow(\mu F\,x \lto A\,x)$ are equal, it suffices to
implement a function $\mathsf{ind} : \uparrow(\mu F\,x \lto \equalizer
{a} f g)$ such that $\textrm{ind}(a) \equiv a$. Then an
inductive-style proof can be implemented by constructing
$\mathsf{ind}$ using a $\fold$. This can all be justified using only
the $\beta\eta$ principles for equalizers and inductive types, and
this is how our most complex inductive proofs are implemented in Agda
formalization.

\subsection{Grammar-specific Additions}

So far, our calculus is a somewhat generic combination of dependent
types with non-commutative linear types. In order to carry out formal
grammar theory and define parsers, we need only add a few
grammar-specific constructions. First for each character in our
alphabet, we add a corresponding linear type $c$. We can then define a
non-linear type $\Char$ as the disjunction of all of these characters,
and define a type $\String$ as the Kleene star of $\Char$, i.e. as an
inductive linear types. Then we add a function $\mathsf{read} :
\ltonl{(\top\lto \String)}$ that intuitively ``reads'' the input
string from the input and makes it available. It is important that the
input type of $\mathsf{read}$ is $\top$, which can control any amount
of resources, and not $\I$ which controls no resources. Further, we
add an axiom that $\lambda s. \mathsf{read}(\top) \equiv \lambda
s. s$, i.e., that if we have a string, but then throw it away and read
it from the input, then in fact that is equivalent to the string we
were given originally. This ensures that the elements of the $\String$
type always stand for the actual input string in our reasoning. In the
next section, we will show how these basic principles are enough to
provide a basis for verified parsing and formal grammar theory.

% \steven{TODO the intro to this section is to be condensed based on what is
%   included in the examples section as motivation and intuition building}

% Omission of the structural rules of a deductive system, such as in
% linear logic \cite{GIRARD19871}, offers precise control over how a value is used
% in a derivation. Linear logic omits the weakening and contraction rules
% to ensure that every value in context is used exactly once. This control enables
% \emph{resource-sensitive} reasoning, where we may treat a resource as
% \emph{consumed} after usage. This viewpoint is amenable to parsing applications,
% as we may treat characters of a string as finite resources that are consumed at
% parse-time. That is, when reading a string, the occurrence of any character is read once. Freely duplicating or dropping characters from a string changes the meaning of that string. \max{what about a backtracking parser?}. One may then envision a linear type system where the types comprise
% formal grammars generated over some alphabet $\Sigma$, and the type constructors
% correspond precisely to inductive constructions on grammars --- such as
% conjunction, disjunction, concatenation, etc.

% Programming in a purely linear term language is limiting due to the variable
% usage restrictions. Code in such a language can
% become unnecessarily verbose when ensuring the linearity invariant.
% To alleviate this
% concern, in 1995 Benton et al.\ proposed an alternative categorical
% semantics of linear logic based on adjoint interactions between linear
% and non-linear logics \cite{bentonMixedLinearNonlinear1995} ---
% appropriately referred to as a \emph{linear-non-linear} (LNL)
% system. This work is simply typed, so the boundary between linear and
% non-linear subtheories is entirely characterized via a monoidal
% adjunction between linear terms and non-linear terms.

% \steven{Describe this monoidal adjunction with $\ltonl A$ and the derivable
%   version of $F$}

% Inside of an LNL system, linearity may be thought of as an option that users can
% choose to deliberately invoke at deliberate points in their developments in an
% otherwise intuitionistic system. However, if we are wishing to treat parsers as
% linear terms over input strings, the non-linear fragment of an LNL theory does
% not really assist in the development of parsers. It is instead the case that
% parsers may benefit from a \emph{dependence} on non-linear terms.
% Through the approach described by Krishnaswami et al.\ in
% \cite{krishnaswami_integrating_2015},
% we may define a restricted form of dependent types.

% In particular, dependence
% on linear terms
% is disallowed; however, through dependence of a linear term on a non-linear
% index, we can recover the definition of Aho's indexed grammars \cite{AhoIndexed}
% internal to \theoryabbv. Although, we can define strictly more things, as Aho's
% indexed grammars provide restricted access to the index whereas we can do
% whatever with it.

% \steven{elaborate on indexed grammars in a more articulate way in the last sentence}
% \steven{Put the aho stuff in the subtheory section}
%

% \paragraph{Regular Expressions}
% We may realize regular expression as a subtheory
% of our language by restricting the type constructors to the
% linear unit, characters, $\otimes$, $\oplus$, and Kleene star. Classically, the
% languages recognized by these are exactly the regular languages --- the lowest
% on the Chomsky hierarchy.

% \paragraph{$\mu$-Regular Expressions} Similarly, we may restrict the connectives
% to be linear unit, characters, $\otimes$, $\oplus$, and arbitrary recursion via
% left-fixed point rather than only Kleene star.
% Instead of regular expressions these correspond to Lei{\ss}'s $\mu$-regular
% expressions, which are known to be equivalent to context-free grammars
% \cite{leiss,krishnaswami_typed_2019}.

% \paragraph{Beyond Context-Free Grammars} The previous two subtheories induce as semantics
% regular grammars and context-free grammars, respectively; however, by including
% the LNL dependent types we may actually express the entirety of the Chomsky
% hierarchy. Through use of the LNL dependent types, we may encode indexed
% grammars, which are known to be properly between context-free and
% context-sensitive grammars within the Chomsky hierarchy \cite{AhoIndexed}. We
% will further see in \cref{subsubsec:tm} how to use dependence to encode Turing machines internal to our
% calculus, which of course induces a semantics of unrestricted grammars.

% \paragraph{Indexed Grammars}
% The LNL dependent types can be used to encode Aho's indexed grammars
% \cite{AhoIndexed} internal to \theoryabbv. Although, we can define strictly more
% things with this non-linear index, as we have unrestricted access to the index
% whereas Aho's grammars can only manipulate it in a particular way.

% \steven{Elaborate on these, and note that they are bw context free and context
%   sensitive on the Chomsky hierarchy}

\section{Formal Grammar Theory in \theoryname}
\label{sec:applications}
This section explores the applications of \theoryabbv to conducting formal
grammar theory. We demonstrate that several classical notions and constructions
integral to the theory of formal languages are faithfully represented
in \theoryname. By encoding well-established formal grammar concepts, we ensure
that our framework remains grounded in the
foundational principles of formal language theory while opening the door to
compositional formal verification of parsers.

\paragraph{Weak and Strong Equivalence}
In the theory of formal grammars, there are two different notions of
equivalence: weak generative capacity, meaning just which strings are
accepted by the grammar, and \emph{strong} generative capacity, when
the parse trees of the two grammars are isomorphic
\cite{chom1963}. Using linear types as grammars, we can define both of
these notions of equivalence in \theoryabbv.

\begin{definition}
  \label{def:weakequiv}
  Grammars $\A$ and $\B$ are \emph{weakly equivalent} if there exists
  parse transformers $\f : \ltonl{(\A \lto \B)}$ and $\g : \ltonl {(\B
    \lto \A)}$. $\A$ is a \emph{retract} of $\B$ if $\A$ and $\B$ are
  weakly equivalent and $\lambda a. g(f(a)) \equiv \lambda a.a$. They
  are \emph{strongly equivalent} if further the other composition is
  the identity, i.e., $\lambda b. f(g(b)) \equiv \lambda b.b$.
\end{definition}

That is, grammars are weakly equivalent if we can construct a parse
tree of $B$ from a parse tree of $A$ and vice-versa, whereas they are
strongly equivalent if these processes are inverse to each other. The
definition of retract is a useful intermediate notion. In retrospect,
our axiom about $\String$ says that $\String$ is a retract of $\top$.

%% \newcommand{\lang}{\texttt{Lang}}
%% A parse transformer $\f : \ltonl{(\A \lto \B)}$ takes as input parses of $\A$ for string
%% $\w$ and returns a parse of $\B$ for $\w$. Therefore, $\f$ demonstrates an
%% inclusion of languages $\lang(\A) \subset \lang(\B)$. Similarly, the existence of
%% $\g$ shows that $\lang(\B) \subset \lang(\A)$, and thus $\lang(\A) = \lang(\B)$.

%% \steven{Need better notation for language that is cohesive throughout the paper}

%% \begin{definition}
%%   \label{def:strongequiv}
%%   Grammars $\A$ and $\B$ are \emph{strongly equivalent} if there exists parse
%%   transformers $\f : \ltonl{(\A \lto \B)}$ and $\g : \ltonl {(\B \lto \A)}$ such
%%   that $\f \circ \g \equiv \ident$ and $\g \circ \f \equiv \ident$.
%% \end{definition}

%% For a string $\w$, \cref{def:strongequiv} induces a bijection between the
%% $\A$-parses of $\w$ and the $\B$-parses of $\w$, thus demonstrating that $\A$
%% and $\B$ agree in their strong generative capacity.

\paragraph{Ambiguity}
A formal grammar $\A$ is ambiguous if there are multiple parse trees
for the \emph{same} string $\w$.  For example, $\a \oplus \a$ is
ambiguous because there are two parses of $\stringquote{a}$,
constructed using $\inl$ and $\inr$. On the other hand, a formal
grammar is unambiguous when there is at most one parse tree for any
input string. We can capture this notion as a type in \theoryabbv in a
clever way:
\begin{definition}
  \label{def:unambig}
  A grammar $\A$ is \emph{unambiguous} if for every linear type $\B$,
  $\f : \ltonl{(\B \lto \A)}$, and $\g : \ltonl{(\B \lto \A)}$
  then $\f \equiv \g$
\end{definition}
\cref{def:unambig} can be read more intuitively as stating that $\A$
is unambiguous if there is at most one way to transform parses of any
other grammar $\B$ into parse of $\A$. This notion of an unambiguous
type is the analog for linear types of the definition of a (homotopy)
\emph{proposition} in the terminology of homotopy type
theory\cite{hott}. The most basic unambiguous types are $\top$ and
$0$, and in a system of classical logic all unambiguous types would
have to be equivalent to one of these, but with our axioms we can show
also that $\I$ and literals $\literal c$ are unambiguous. To see this, first, we
establish two useful properties of unambiguity.
\begin{lemma}[\agdalogo]
  If $\B$ is unambiguous and $\A$ is a retract of $\B$ then $\A$ is unambiguous.
  If a disjunction $\bigoplus\limits_{x:X} A(x)$ is unambiguous then
  each $A(x)$ is unambiguous.
\end{lemma}
From the first principle, we can prove that $\String$ is unambiguous,
since it is a retract of $\top$. In fact, observe that if $\A$ is a
retract of $\B$ and $\B$ is unambiguous, then in fact $\A$ and $\B$
are strongly equivalent, as the equation $\lambda b. f(g(b)) \equiv
\lambda b. b$ follows because $\B$ is unambiguous. Therefore also
$\String$ is strongly equivalent to $\top$. Next, since $\String$ is
defined as a Kleene star, we can easily show that $\String \cong \I
\oplus \Char \oplus (\Char \otimes \Char \otimes \String)$. Then by
the second principle we have that $\I$ and $\Char$ and each literal
$\literal c$ are unambiguous as well.

% \paragraph{A Grammar of Strings}
% Define the character grammar $\charg$ to be the disjunction over characters in the alphabet $\Sigma$,
% $\charg = \LinSigTy {c} {\Sigma} c$. Then define a linear type of strings as a
% linear list of characters,
% $\stringg = \charg^{*}$.

% In the grammars model, we can build a
% strong equivalence between $\top$ and $\stringg$. hen building parsers, we may wish to enforce this
% syntactically by including $\top \cong \stringg$ as an axiom.

% Through inclusion of this axiom, $! : \stringg \to \top$ is an isomorphism, and
% thus also a monomorphism. That is, $\stringg$ is unambiguous.
% \max{we should have a section of the previous section which gives the axioms, this one and the distributivity axiom}

% \paragraph{Parseability}
% We say $A$ is \emph{parseable} if there exists a grammar $B$ and a map
% $\top \to A \oplus B$. Taking $\top \cong \stringg$, a grammar $A$ is parseable
% exactly when we may read an arbitrary string into a parse of $A$ or a $B$-valued
% refutation.

% \paragraph{Decidability}
% In the case that $A$ is parseable with respect to $\neg A$ --- i.e.\
% there's a map $\top \to A \oplus \neg A$ --- then call $A$ \emph{decidable}.

\paragraph{Partial and Total Parsers}

We now turn to our main task, which is using our linear type system to
implement verified parsers. Given a grammar defined as a linear type
$A$, a first attempt at defining a parser would be to implement a
function $\uparrow{(\String \lto A)}$. But since our linear functions
must be total, this means that we can construct an $A$ parse for
\emph{every} input string, which is impossible for most grammars of
interest. Instead we might try to write a partial function as a
$\ltonl{(\String \lto (A \oplus \top))}$ using the ``option'' monad
\cite{maybeMonad}. This allows for the possibility that the input
string doesn't parse, but is far too weak as a specification: we can
trivially implement a parser for any type by always returning
$\inr$. The correct notion of a parser should be one that allows for
failure, but only in the case that a parse cannot be constructed.
\begin{definition}
  A parser for a linear type $A$ is a function $\uparrow{(\String \lto
    A \oplus A_{\neg})}$ where $A_{\neg}$ is a linear type that is
  \emph{disjoint} from $A$ in that we can implement a function
  $\uparrow{(A \& A_{\neg} \lto 0)}$
\end{definition}
Here we replace $\top$ in our partial parser type with a type
$A_{\neg}$ that we can think of as a negation of $A$. The function
$\uparrow{(A \& A_{\neg} \lto 0)}$ ensures that it is impossible for
$A$ and $A_{\neg}$ to match the same input string. This means that in
defining a parser, we will need to define a kind of negative grammar
for strings that do not parse. Fortunately, we will see that
deterministic automata naturally support such a notion with no
additional effort: the negative grammar is simply the grammar for
traces that end in a rejecting state.
\begin{lemma}[\agdalogo]
  If $A \oplus A_{\neg}$ is unambiguous, then $A$ and $A_{\neg}$ are
  disjoint.
\end{lemma}

Writing a parser as a linear term in this way is an intrinsic
verification of the \emph{soundness} of the parser completely for free
from the typing: any $\inl$ parse that we return \emph{must}
correspond to a parse tree of the input string. Further if we verify
the disjointness property we then also get the \emph{completeness} of
the parser as well, that it never fails to generate an $A$ parse when
it is possible.

Our main method for constructing verified parsers is to show that a
grammar $A$ is weakly equivalent to a grammar for a deterministic
automaton. Parsers for deterministic automata are simple to implement
by stepping through the states of the automaton, with the rejecting
traces serving as the negative grammar. This is sufficient due to the
following:
\begin{lemma}[\agdalogo]
  \label{lem:wk-eqv-parse}
  If $\A$ is weakly equivalent to $\B$ then any parser for $\A$ can be
  extended to a parser for $\B$.
\end{lemma}
Here we need both directions of the weak equivalence. We need $A \lto
B$ to extend the parser from $\String \lto A \oplus A_{\neg}$ to
$\String \lto B \oplus A_{\neg}$ but then we also need $B \lto A$ to
establish that $A_{\neg}$ is disjoint from $B$.

\subsection{Regular Expressions and Finite Automata}
%% 2. Regex, NFA, DFA

In this section, we describe how to construct an intrinsically
verified parser for regular expressions by compiling it to an NFA and
then a DFA. That is, for each regular expression $A$, we construct an
NFA $N(A)$ and a corresponding DFA $D(A)$ such that $A$ is strongly
equivalent to the traces of $N(A)$ and weakly equivalent to the
accepting traces of $D(A)$. Then we can easily construct a parser for
traces of $D(A)$ and combine this using \Cref{lem:wk-eqv-parse} to get
a verified regex parser.

\newcommand{\states}{\mathtt{states}}
\newcommand{\labelt}{label}
\newcommand{\transitions}{\texttt{transitions}}
\newcommand{\epstransitions}{\epsilon\texttt{transitions}}
\newcommand{\isAcc}{\mathtt{isAcc}}
\newcommand{\init}{\mathtt{init}}
\newcommand{\src}{\mathtt{src}}
\newcommand{\dst}{\mathtt{dst}}
\newcommand{\parse}{\mathtt{parse}}
\newcommand{\print}{\mathtt{print}}
\newcommand{\epssrc}{\epsilon\mathtt{src}}
\newcommand{\epsdst}{\epsilon\mathtt{dst}}

\newcommand{\N}{\texttt{N}}

A regular expressions in \theoryabbv is a linear type constructed
using only the connectives $\literal c$, $0$, $\oplus$, $I$,
$\otimes$, and Kleene star.  In \cref{sec:type-theory-examples}, we
saw one particular NFA corresponding and its type of traces. More
generally we define the linear type of traces as in
\Cref{fig:nfatrace}

%% The definition of the linear type of traces
%% through this
%% NFA, as
%% presented in \cref{fig:nfainductive}, inspects the concrete structure of the
%% NFA.\ When generalizing from this example to an arbitrary NFA, we do not have the
%% luxury of inspecting a concrete definition. Therefore, we abstract over the
%% specification of an arbitrary NFA and define traces with respect to that
%% specification.

%% Let the data of an NFA $\N$ be given by the following nonlinear data,
%% \begin{itemize}
%% \item $\N.\states$ --- a finite set of states
%% \item $\N.\init : \N.\states$ --- the initial state
%% \item $\N.\isAcc : \N.\states \to \Bool$ --- a mapping marking which states are
%%         accepting
%% \item $\N.\transitions$ --- a finite set of labeled transitions
%% \item $\N.\labelt : \N.\transitions \to \Sigma$ --- a mapping from $N.\transitions$ to the character labeling the transition
%% \item $\N.\src , \N.\dst : \N.\transitions \to \N.\states$ --- mappings from
%%         $\N.\transitions$ to the source and target states, respectively, of a labeled transition
%% \item $\N.\epstransitions$ --- a finite set of $\varepsilon$-transitions
%% \item $\N.\epssrc, \N.\epsdst : \N.\epstransitions \to \N.\states$ --- mappings
%%         from $\N.\epstransitions$ to the source and target states of an $\epsilon$-transition
%% \end{itemize}

In \cref{fig:nfatrace} we define a linear type of traces through an
NFA $\N$. $\Trace_{\N}$ is an inductive type indexed by the starting
state of the trace $\s : \N.\states$. Traces in $\N$ may be built
through one of three constructors. We may terminate a trace at an
accepting state with the constructor $\nil$. Here we use an Agda-style
unicode syntax for $\bigamp$, as well as using the function arrow to
mean a non-dependent version of $\bigamp$. If we had a trace beginning
at the destination state of a transition, then we may use the $\cons$
constructor to linearly combine that trace with a parse of the label
of the transition to build a trace beginning at the source of the
transition.  Finally, if we had a trace beginning at the destination
of an $\epsilon$-transition then we may use $\epscons$ to pull it back
along the $\epsilon$-transition and construct a trace beginning at the
source of the $\epsilon$-transition. As a shorthand, write $Parse_{N}$
for the accepting traces out of $N.init$.
\newcommand{\D}{\texttt{D}}
\begin{figure}
\begin{lstlisting}
data Trace(*@$_{\N}$@*) : (s : N.states) (*@$\to$@*) L where
  nil : (*@$\uparrow$@*)(&[ s : N.states ] N.isAcc (*@$\to$@*) Trace(*@$_{\N}$@*) s)
  cons : (*@$\uparrow$@*)(&[ t : N.transitions ]
    ('N.label t' (*@$\lto$@*) Trace(*@$_{\N}$@*) (N.dst t) (*@$\lto$@*) Trace(*@$_{\N}$@*) (N.src t)))
  (*@$\epsilon$@*)cons : (*@$\uparrow$@*)(&[ t : N.(*@$\epsilon$@*)transitions ]
    (Trace(*@$_{\N}$@*) (N.(*@$\epsilon$@*)dst t) (*@$\lto$@*) Trace(*@$_{\N}$@*)(N.(*@$\epsilon$@*)src t)))

data Trace(*@$_{\D}$@*) : (s : D.states) (b : Bool) (*@$\to$@*) L where
  nil : (*@$\uparrow$@*)(&[ s : D.states ] Trace(*@$_{\D}$@*) s D.isAcc s)
  cons : (*@$\uparrow$@*)(&[ c : Char ] &[ s : D.states ] &[ b : Bool ]
  ('c' (*@$\lto$@*) Trace(*@$_{\D}$@*) (D.(*@$\delta$@*) c s) b (*@$\lto$@*) Trace(*@$_{\D}$@*) s b))
\end{lstlisting}
\caption{Traces of an NFA $N$ and a DFA $D$}
\label{fig:nfatrace}
\end{figure}

%% \paragraph{Deterministic Finite Automata}
%% Just as the above defines an NFA, we provide a specification for an arbitrary
%% deterministic finite automaton.\ A DFA $D$ is
%% given by the following data,

%% \begin{itemize}
%% \item $\D.\states$ --- a finite set of states
%% \item $\D.\init : \D.\states$ --- the initial state
%%   \item $\D.\isAcc : \D.\states \to \Bool$ --- a mapping marking which states
%%         are accepting
%% \item $\D.\delta : \Sigma \to \D.\states \to \D.\states$ --- a deterministic transition function. Given a character $\c$ and a state
%%         $\s$, $\D.\delta~\c~\s$ is the state for which there is a transition $\s \overset{\stringquote{c}}{\to} (\D.\delta~\c~\s)$
%% \end{itemize}

%% \begin{figure}

%% \caption{Traces through the DFA $D$ as an indexed inductive type}
%% \label{fig:dfatrace}
%% \end{figure}

$\Trace_{\D}$, the linear type of traces through $\D$, is given
next. Unlike traces for an NFA, we parameterize this type additionally
by a boolean which says whether the trace is accepting or
rejecting. These traces may be terminated in an accepting state $\s$
with the $\nil$ constructor.  The $\cons$ constructor builds a trace
out of state $\s$ by linearly combining a parse of some character $\c$
with a trace out of the state $\D.\delta~\c~\s$. The trace built with
$\cons$ is accepting if and only if the trace out of $D.\delta~c~s$ is
accepting.

Because DFAs are deterministic, we are able to prove that their type
of traces are unambiguous and define a parser for them. In particular
we show that for any start state $s$, ${\LinSigTy {b} {\Bool}
  \Trace_{D}~s~b}$ is a retract of $\stringg$. That is, first we
construct a function $\parse_{D}$ that is a parser for
$\Trace_D~{s}~{true}$, with $\Trace_D~{s}~{false}$ being the disjoint
type used. The fact that these types are disjoint follows by showing
that this function is part of a retraction, i.e., that there is only
one way to trace through a deterministic automaton.

The parser,
$\parse_{D}$,
is defined by recursion on strings in \cref{fig:printdfa}. If this string is empty, then $\parse_{D}$
defines a linear function that terminates a trace at the input state $s$. If the
string is nonempty, then $\parse_{D}$ walks forward in $D$ from the input state $s$ by
the character at the head of the string.
%
The inverse, $\print_{D}$ is defined by recursion on traces. If the trace is defined
via $\nil$, then $\print_{D}$ returns the empty string. Otherwise, if the trace
is defined by $\cons$ then $\parse_{D}$ appends the character from the most
recent transition to the output
string and recurses.
%
We prove this is a retraction by induction on traces.

\begin{theorem}[\agdalogo]
  $\parse_D s$ is a parser for $\Trace_D~s~\true$.
\end{theorem}

\begin{figure}
\begin{lstlisting}
parse(*@$_D$@*) : (*@$\uparrow$@*)(String (*@$\lto$@*) &[ s : D.states ] (*@$\oplus$@*)[ b : Bool ] Trace(*@$_{\D}$@*) s b)
parse(*@$_D$@*) String.nil s = (*@$\sigma$@*) (D.isAcc s) (Trace(*@$_{\D}$@*).nil s)
parse(*@$_D$@*) (String.cons ((*@$\sigma$@*) c a) w) s =
   let (*@$\sigma$@*) b t = parse w (D.(*@$\delta$@*) c s) in
   (*@$\sigma$@*) b (Trace(*@$_{\D}$@*).cons c s b a t)

print(*@$_D$@*) : (s : D.states) (*@$\to$@*) (*@$\uparrow$@*)(((*@$\oplus$@*)[ b : Bool ] Trace(*@$_{\D}$@*) s b) (*@$\lto$@*) String)
print(*@$_D$@*) s ((*@$\sigma$@*) b (Trace(*@$_{\D}$@*).nil .s)) = String.nil
print(*@$_D$@*) s ((*@$\sigma$@*) b (Trace(*@$_{\D}$@*).cons c (D.(*@$\delta$@*) c .s) b a trace)) =
  String.cons ((*@$\sigma$@*) c a) (print(*@$_D$@*) (D.(*@$\delta$@*) c s) ((*@$\sigma$@*) b trace))
\end{lstlisting}
\caption{Parser/Printer for DFA traces}
\label{fig:printdfa}
\end{figure}

Working backwards, we can then show the traces of an NFA are weakly
equivalent to the traces of a DFA implementing a variant of Rabin and
Scott's classic powerset construction
\cite{rabinFiniteAutomataTheir1959}. Here we note that this is
\emph{only} a weak equivalence and not a strong equivalence, as the
DFA is unambiguous even if the NFA is not.
\begin{theorem}[Determinization, \Agda]
  \label{thm:determinization}
  Given an NFA $N$, there exists a DFA $D$ such that
  $Parse_{N}$ is weakly equivalent to $Parse_{D}$.
\end{theorem}
\newcommand{\X}{\texttt{X}}
\newcommand{\Y}{\texttt{Y}}
\newcommand{\x}{\texttt{x}}
\newcommand{\y}{\texttt{y}}
\begin{proof}
  Define the states of $\D$ to be the $\mathbb{P}_{\varepsilon}(\N.\states)$ --- the type
  of $\epsilon$-closed\footnote{A subset of states $\X$ is $\epsilon$-closed if
    for every $\s \in \X$ and $\epsilon$-transition $\s \overset{\epsilon}{\to} \s'$ we have $\s' \in \X$.} subsets of
  $\N.\states$. A subset is accepting in $\D$ if it contains an accepting state
  from $\N$. Construct the initial state of $\D$ as the
  $\epsilon$-closure of $\N.\init$. Lastly define the transition function
  of $\D$ to send the subset $X$ under the character $\c$ to the
  $\epsilon$-closure of all the states reachable from $X$ via a transition
  labelled with the character $\c$.

  We demonstrate the weak equivalence between $Parse_{N}$ and
  $Parse_{D}$ by constructing parse transformers between the two
  grammars. To build the parse transformer
  $\ltonl{(Parse_{N} \lto Parse_{D})}$, we strengthen our inductive hypothesis
  and build a term
\newcommand{\NtoD}{\texttt{NtoD}}
\newcommand{\DtoN}{\texttt{DtoN}}
  \[
    \NtoD : \ltonl {\left(\Trace_{\N}~\s~\true \lto \bigwith\limits_{\X : \D.\states} \bigwith\limits_{\texttt{sInX} : \X \ni \s} \Trace_{\D}~\X~\true \right)}
  \]
  that maps a trace in $\N$ from an arbitrary state $\s$ to a trace in $\D$
  that may begin at any subset of states $\X$ that contains $\s$. $\NtoD$ may then be instantiated at
  $\s = \N.\init$ and $\X = D.\mathsf{init}$ to get the desired
  parse transformer.
  Vice-versa, 

%% \begin{figure}
%% \begin{lstlisting}
%% NtoD : (s : N.states) (*@$\to$@*)
%%        (*@$\uparrow$@*)(Trace(*@$_{\N}$@*) s true (*@$\lto$@*)
%%            &[ X : D.states ] &[ sInX : (X (*@$\ni$@*) s) ] Trace(*@$_{\D}$@*) X true)
%% NtoD .s (nil s) X sInX = nil s
%% NtoD .(N.src t) (N.cons trans .true a dstTrace) X sInX =
%%   D.cons (N.label t) X true a
%%     (((NtoD s dstTrace).(*@$\pi$@*) (D.(*@$\delta$@*) (N.label t) X)).(*@$\pi$@*) (N.dst t))
%% NtoD .(N.(*@$\epsilon$@*)src t) ((*@$\epsilon$@*)cons (*@$\epsilon$@*)trans .true (*@$\epsilon$@*)DstTrace) X sInX =
%%   ((NtoD s (*@$\epsilon$@*)DstTrace).(*@$\pi$@*) X).(*@$\pi$@*) (N.(*@$\epsilon$@*)dst t)
%% \end{lstlisting}
%% \caption{Parse transformer from NFA traces to Powerset DFA traces}
%% \label{fig:NtoD}
%% \end{figure}

%% Because the state $\s$ is now arbitrary, we can define $\NtoD$ via
%% recursion on the trace through the NFA. Code for this is given in \cref{fig:NtoD}.
%% If the NFA trace is terminating at the accepting state $\s$ and $\s \in \X$,
%% then we return a trace in $\D$ that terminates at
%% $\X$, which is accepting because it contains the accepting state $\s$.

%% If the NFA trace is defined via $\cons$ for a labeled transition
%% $t : N.transitions$ such that $N.src~t \in \X$, then
%% we first decompose the trace into a parse of $N.label~t$ and a
%% trace starting from $N.dst~t$. We then recursively call $\NtoD$
%% on this trace which returns a family of traces parameterized by $\epsilon$-closed
%% subsets containing $N.\dst~t$. We instantiate this family at the
%% set of states
%% $\D.\delta~(N.label~t)~X$. Because $\N.\dst~t$ is
%% reachable from $\N.\src~\texttt{trans}$ by a single transition labeled by
%% $N.label~t$ and $N.src~t \in X$ we have that
%% $N.dst~t \in \D.\delta~(N.label~t)~X$. Therefore
%% we can linearly combine the parse of the label and the trace built from the
%% recursive call to get a trace in $\D$ rooted at $X$.

%% Lastly, if the NFA trace is defined via $\epsilon\cons$ for an $\epsilon$-transition
%% $t : \N.\epstransitions$ such that
%% $\N.\epssrc~t \in \X$, then the construction is quite
%% similar. In this case we again recursively call $\NtoD$, except afterwards we
%% instantiate the family of grammars at the set $X$ itself. We may do this because
%% all states in $\D$ are taken to be $\epsilon$-closed, therefore
%% $\N.\epsdst~t \in \X$ because
%% $\N.\epssrc~t$ is in $X$. This concludes the construction of $\NtoD$.

To construct a term from DFA traces to NFA traces, we similarly strengthen our
induction hypothesis and build a parse transformer
\[
  \DtoN : \ltonl {\left( \Trace_{\D}~\X~\true \lto \bigoplus\limits_{s : \N.\mathsf{states}}\bigoplus\limits_{\texttt{sInX} : \X \ni \s} \Trace_{\N}~s~\true \right)}
\]

%% For a moment, let us turn our attention to the content behind the traces in $\D$.
%% Traces from subset $\X$ to subset $\Y$ describes the \emph{existence} of some trace
%% in the $\N$ from some $\x \in \X$ to some $\y \in \Y$. In general it is not clear how
%% to extract out a choice of trace in $\N$, which is what $\DtoN$ is asking of us.
%% However, we may further take the states and transitions of $\N$ to be \emph{ordered}.
%% This allows us to
%% induce an order on traces in $\D$ from $\X$ to $\Y$. From these orderings we can then
%% define the necessary choice functions on each type by taking the smallest with
%% respect to the ordering.

%% We define $\DtoN$ by recursion on traces through $\D$. First if the trace stops at
%% some accepting $\epsilon$-closed subset $\X$, then we may choose some state
%% $\s \in X$ such that $\s$ is accepting and terminate the trace in $\N$ at $\s$.

%% Next if the trace in $\D$ is defined via transition
%% $\X \overset{\stringquote{c}}{\to} (\D.\delta~\c~\X)$ then we may choose some state in
%% $\s \in \D.\delta~\c~\X$. Recursively, $\DtoN$ sends the trace rooted at
%% $\D.\delta~\c~\X$ to one rooted at $\s$. By definition of $\D.\delta~\c~\X$,
%% there exists some $\s' \in \X$ such that $\s'$ transitions to $\s$ by some number of
%% $\epsilon$-transitions followed by a transition labeled by $\c$.
%% We order all
%% such traces of this form and choose the smallest one, which is then linearly
%% combined with the parse of $\c$ to build an $\N$ trace. This concludes the
%% definition of $\DtoN$, thus showing that $Parse_{N}$ and $Parse_{D}$ are
%% weakly equivalent.
\end{proof}

Finally, given any regular expression we can construct a
\emph{strongly} equivalent NFA. While only weak equivalence is
required to construct a parser, proving the strong equivalence shows
that other aspects of formal grammar theory are also verifiable in
\theoryabbv.
\newcommand{\R}{\texttt{R}}
\begin{theorem}[Thompson's Construction, \Agda]
  Given a regular expression $\R$, there exists an NFA $\N$ such that $\R$ is
  strongly equivalent to $\Trace_{N}(\N.\init)$.
\end{theorem}
\begin{proof}
We use a variant of Thompson's construction
\cite{thompsonProgrammingTechniquesRegular1968}, showing that NFAs are, up to strong equivalence, closed under each type operation for regular expressions.

%% In this paper we will walk through
%% some of the cases of this proof. The remaining cases
%% follow
%% similar reasoning and the details may be found in the formalized artifact.

%% \begin{figure}
%%   \begin{tikzpicture}[node distance = 17mm ]
%%     \node[state, initial] (0) {$\0$};
%%     \node[state, right of=0, accepting] (1) {$\1$};

%%     \path[->] (0) edge[above] node{$\stringquote{c}$} (1);
%%   \end{tikzpicture}
%%   \caption{NFA $\N_{\c}$ for a single character $\c$}
%%   \label{fig:litnfa}
%% \end{figure}

%% First handle the case where $R$ is a literal character $\literal c$. The NFA $N_{c}$ pictured in
%% \cref{fig:litnfa} is strongly equivalent to the grammar $c$. To demonstrate this
%% strong equivalence, we build functions between $\literal c$ and $Parse_{N_{c}}$
%% and show they are mutually inverse.

%% \begin{figure}
%% \begin{lstlisting}
%% A : (s : Fin 2) (*@$\to$@*) L
%% A 0 = 'c'
%% A 1 = I

%% toN(*@$_c$@*) : (s : Fin 2) (*@$\to$@*) (*@$\uparrow$@*)(A s (*@$\lto$@*) Trace(*@$_{N_c}$@*) s true)
%% toN(*@$_c$@*) 0 c = cons t true c (nil 1)
%% toN(*@$_c$@*) 1 () = nil 1

%% fromN(*@$_c$@*) : (s : Fin 2) (*@$\to$@*) (*@$\uparrow$@*)(Trace(*@$_{N_c}$@*) s true (*@$\lto$@*) A s)
%% fromN(*@$_c$@*) 0 (cons t .true c tr) = let a (*@$\otimes$@*) () = c (*@$\otimes$@*) fromN(*@$_c$@*) 1 tr in a
%% fromN(*@$_c$@*) 1 (nil .1) = ()
%% \end{lstlisting}
%% \caption{Parse transformer out of $Trace_{N_{c}}~s~true$}
%% \label{fig:litNFAterms}
%% \end{figure}

%% In \cref{fig:litNFAterms} we define a family of grammars $A$ that serve as
%% interpretations for each state in $N_{c}$. We then prove that each $A~s$ is
%% equivalent to the accepting traces through $N_{c}$ out of state $s$.

%% When
%% defining $fromN_{c}$ we pattern match on the accepting traces beginning in each
%% state. Out of state $0$ the only accepting traces are built via $cons$ over the
%% single transition $t$, and out
%% of state $1$ the only accepting trace is built via $nil$.

%% Each $A~s$ is unambiguous, so $(fromN_{c}~s) \circ (toN_{c}~s)$ is equal to the
%% identity on each $A~s$. To complete the proof of strong equivalence it remains
%% to show that $(toN_{c}~s) \circ (fromN_{c}~s)$ is equal to the identity on
%% $Trace~s~true$, which we show by using the equalizer type.

%% \begin{figure}
%% \begin{lstlisting}
%% eqHom(*@$_{N_c}$@*) : (s : N(*@$_c$@*).state) (*@$\to$@*)
%%       (*@$\uparrow$@*)(Trace(*@$_{N_c}$@*) s true (*@$\lto$@*) {tr | (toN(*@$_c$@*) s (*@$\circ$@*) fromN(*@$_{c}$@*) s) tr = id tr})
%% \end{lstlisting}
%% \caption{Signature for the equalizer term}
%% \label{fig:litequalizer}
%% \end{figure}

%% The type signature for the family of functions, $eqHom_{N_{c}}$, into the equalizer type is given in
%% \cref{fig:litequalizer}. By defining an $Trace_{N_{c}}$-algebra homomorphism
%% into the equalizer, we enable a
%% proof strategy for equality by induction on traces through $N_{c}$. First consider the traces
%% out of state $1$. The only case to handle here is $nil 1$, and by definition
%% $toN_{c}~1$ and $fromN_{c}~1$ invert each other.
%% When defining $eqHom_{N_{c}}$ for traces out of state $0$, we must leverage an
%% inductive hypothesis,

%% \begin{align*}
%%   (toN_{c}~0 \circ fromN_{c}~0) (cons~t~true~c~tr)
%%   & \equiv (toN_{c}~0) (let~a~\otimes~()~=~c~\otimes~fromN_{c}~1~tr~in~a) \\
%%   & \equiv cons~t~true~(let~a~\otimes~()~=~c~\otimes~fromN_{c}~1~tr~in~a)~(nil~1) \\
%%   & \equiv cons~t~true~c~(nil~1) \\
%%   & \equiv cons~t~true~c~tr \\
%% \end{align*}

%% The first two equalities are by defintion. The thrid equality holds because
%% $\literal c$ is unambiguous and
%% $let~a~\otimes~()~=~c~\otimes~fromN_{c}~1~tr~in~a$ and $c$ have type
%% $\literal c$. The final equality holds because our inductive hypothesis states
%% that $nil~1 \equiv tr$. Thus, each $A s$ and $Trace_{N_{c}}~s~true$ are strongly
%% equivalent --- in particular, $\literal c$ and $Parse_{N_{c}}$.

%% \steven{Rewrite the disjunction case with equalizers}
%% Now consider the case where $R$ is the disjunction of two regular expressions
%% $R_{1}$ and $R_{2}$. Inductively, we may build NFAs $N_{1}$ and $N_{2}$ that are
%% strongly equivalent to $R_{1}$ and $R_{2}$, respectively. We define $N_{\oplus}$
%% as a new NFA built as the disjunction of $N_{1}$ and $N_{2}$.
%% $N_{\oplus}.\states$ comprises the states from $N_{1}$, the states from
%% $N_{2}$, and a new marked initial state. The accepting states are precisely
%% those that were accepting in the subautomata. The initial state has an
%% $\varepsilon$-transition to each of the initial states of the subautomata. Once in
%% $N_{1}.\init$, the downstream transitions are a copy of those in $N_{1}$,
%% likewise for $N_{2}.\init$.  That is, $N_{\oplus}$ contains a copy of each
%% $N_{1}$ and $N_{2}$, and we choose which copy to inhabit by deciding which
%% $\varepsilon$-transition out of the initial state to take. A schematic
%% representation of this automaton is given in \cref{fig:disjunctionnfa}.

%% \begin{figure}
%%   \begin{tikzpicture}[node distance = 17mm ]
%%     \node[state, initial] (init) {$\init$};
%%     \node[state, above right of=init] (init1) {$N_{1}.\init$};
%%     \node[state, below right of=init] (init2) {$N_{2}.\init$};
%%     \node[right of=init1] (dots1) {$\dots$};
%%     \node[right of=init2] (dots2) {$\dots$};
%%     \node[state, accepting, right of=dots1] (acc1) {$s_{1}$};
%%     \node[state, accepting, right of=dots2] (acc2) {$s_{2}$};

%%     \path[->] (init) edge[below] node{$\varepsilon$} (init1)
%%               (init) edge[above] node{$\varepsilon$} (init2)
%%               (init1) edge[above] node{} (dots1)
%%               (init2) edge[above] node{} (dots2)
%%               (dots1) edge[above] node{} (acc1)
%%               (dots2) edge[above] node{} (acc2);
%%   \end{tikzpicture}
%%   \caption{NFA $N_{\oplus}$ as a disjunction of $N_{1}$ and $N_{2}$}
%%   \label{fig:disjunctionnfa}
%% \end{figure}

%% Because $N_{1}$ and $N_{2}$ are each strongly equivalent to $R_{1}$ and $R_{2}$,
%% it suffices to show that
%% $Parse_{N_{1}} \oplus Parse_{N_{2}}$ is strongly
%% equivalent to $Parse_{N_{\oplus}}$.

%% Just like the case of a single character, when building the maps in either
%% direction, we strengthen the induction hypothesis by mapping out of traces
%% beginning with an arbitrary state then proceed by induction on traces.

%% First, define the
%% mapping out of $\Trace_{N_{\oplus}}(s)$. If $s$ is a state from $N_{1}$ then
%% send the trace to a trace in $N_{1}$ beginning at $s$. Likewise, if $s$ is from
%% $N_{2}$, then we map to a trace in $N_{2}$ beginning at $s$. If $s$ is
%% $N_{\oplus}.\init$, then we map the trace to a disjunction of traces
%% $\Trace_{N_{1}}(N_{1}.\init) \oplus \Trace_{N_{2}}(N_{2}.\init)$.

%% In the other direction, we need to map a disjunction of traces through $N_{1}$
%% or $N_{2}$ to a trace in $N_{\oplus}$. Without loss of generality, assume that
%% the trace came from $N_{1}$. That is, assume we currently have a term of type
%% $\Trace_{{N_{1}}}(N_{1}.\init)$ and we aim to construct a term of type
%% $\Trace_{N_{\oplus}}(N_{\oplus}.\init)$. We induct on the trace $N$ trace an map
%% it over to a trace beginning at the copy of $N_{1}.\init$ in $N_{\oplus}$, and
%% then we conclude by using the $\epscons$ constructor to turn this trace into one
%% beginning at $N_{\oplus}.\init$.

%% We show that these maps mutually invert each other by the same sort of argument
%% we used with the single character NFA.\ That is, we prove that the each map is a
%% homomorphism of algebras and then appeal to the initiality of each of
%% $\Trace_{N_{1}}$, $\Trace_{N_{2}}$, and $\Trace_{N_{\oplus}}$ to prove that the
%% relevant maps compose to the identity.

%% In this sketch, we elide the proof that these maps do indeed form homomorphisms.
%% Further, we are eliding the needed NFAs for the cases where the regular
%% expression is $I$, $0$, a Kleene star, or a concatenation. All of these
%% elided details may be found in the Agda formalization.
\end{proof}

\subsection{Context-free grammars}

Next, we give two examples for parsing context-free
grammars. Context-free grammars (CFG) can be encoded in our type
theory in a similar way to regular expressions, as CFGs are equivalent
to the formalism of \emph{$\mu$}-regular expressions, where the Kleene
star is replaced by an arbitrary fixed point
operation\cite{muregular}.

\newcommand{\balanced}{\mathtt{balanced}}
\newcommand{\Dyck}{\mathtt{Dyck}}
A simple example of a CFG is the Dyck grammar of balanced
parentheses, which we define in \Cref{fig:dyckinductive}
$\Dyck$ is a grammar over the alphabet $\{ \stringquote{(}, \stringquote{)} \}$. The $\nil$ constructor shows that
the empty string is balanced, and the $bal$ constructor builds a balanced
parse by wrapping an already balanced parse in an additional set of parentheses
then following it with another balanced parse. We construct a parser for $\Dyck$ by building a deterministic automaton $M$ such
that $Parse_{M}$ is strongly equivalent to $\Dyck$.
\begin{figure}
\begin{lstlisting}
data Dyck : L where
  nil : (*@$\uparrow$@*) Dyck
  bal : (*@$\uparrow$@*)('(' (*@$\lto$@*) Dyck (*@$\lto$@*) ')' (*@$\lto$@*) Dyck (*@$\lto$@*) Dyck)
\end{lstlisting}
\caption{The Dyck grammar as an inductive linear type}
\label{fig:dyckinductive}
\end{figure}

The Dyck language is an example of an $\textrm{LL}(0)$ language, one
that can be parsed top-down with no lookahead\cite{ll0}. This means we
can implement it simply as an \emph{infinite} state deterministic
automaton, in \Cref{fig:DyckAutomaton}. Here the state is a ``stack''
counting how many open parentheses have been seen so far. Functions
$parse_{M}$ and $print_{M}$ for this automaton can be defined
analogously to \cref{fig:dfaparser,fig:printdfa}, and so $\LinSigTy
{s} {M.states} {\LinSigTy {b}{Bool} {Trace_{M}~s~b}}$ is likewise
unambiguous.

\begin{theorem}[\Agda]
  \label{thm:dyck}
  $Dyck$ and $Parse_{M}$ are strongly equivalent. And therefore we can construct a parser for $\Dyck$.
\end{theorem}

%% We can build a $Dyck$ parser by utilizing
%% this artifact, and the
%% runtime of the $Dyck$ parser is about 30 seconds for input that is 25,000
%% characters long.
%% \pedro{It might be a good idea to mention explicitly in the introduction
%%   that we are not claiming that our verified parsers are competitive with
%%   the state of the art.}

\begin{figure}
  \begin{tikzpicture}[node distance = 17mm ]
    \node[state, initial, accepting] (0) {$0$};
    \node[state, right of=0] (1) {$1$};
    \node[state, right of=1] (2) {$2$};
    \node[right of=2] (3) {$\dots$};
    \node[state, below of=0] (fail) {$fail$};

    \path[->] (0) edge[above, bend left] node{\stringquote{(}} (1)
              (1) edge[below, bend left] node{\stringquote{)}} (0)
              (1) edge[above, bend left] node{\stringquote{(}} (2)
              (2) edge[below, bend left] node{\stringquote{)}} (1)
              (2) edge[above, bend left] node{\stringquote{(}} (3)
              (3) edge[below, bend left] node{\stringquote{)}} (2)
              (fail) edge[loop right] node{\stringquote{(}, \stringquote{)}} (fail)
              (0) edge[left] node{\stringquote{)}} (fail);
  \end{tikzpicture}
  \caption{Automaton $M$ for the Dyck grammar}
  \label{fig:DyckAutomaton}
\end{figure}

Our final example is of a simple grammar of arithmetic expressions
with an associative operation. Here we take the alphabet to be $\{
\stringquote{(}, \stringquote{)}, \stringquote{+} \stringquote{NUM}
\}$. In \Cref{fig:binop-inductive} we define it using two mutually
recursive types, corresponding to the two non-terminals we would use
in a CFG syntax. The syntactic structure encodes that the binary
operation is right associative. In the same figure, we define the
traces of an automaton with one token of lookahead. The automaton has
four different ``states'', each with access to a natural number
``stack''. The ``opening'' state $O$ expects either an open paren, in
which case it increments the stack and stays in the opening state, or
sees a number and proceeds to the $D$ state. The ``done opening''
state $D$ is where lookahead is used: if the next token will be a
right paren, then we proceed to $C$, otherwise we proceed to $M$. Here
$\mathsf{NotStartsWithRP}$ is defined as $\I \oplus (('(' \oplus '+'
\oplus \mathsf{NUM}) \otimes \top)$. In the ``closing'' state $C$ if
we observe a close paren we decrement the count and continue to the
$D$ state. In the ``multiplying'' state $M$, we succeed if the string
ends and the count is $0$, and if we see a plus we continue to the $O$
state. Additionally, since the automaton need also parse all of the
incorrect strings, we add additional failing cases. It is
straightforward to implement a parser for this lookahead automaton,
generalizing the approach for deterministic automata.

\begin{theorem}[\agdalogo]
  We construct a parser for $\mathsf{Exp}$ by showing it is weakly
  equivalent to $\mathsf{O}~0~\true$ 
\end{theorem}
\begin{figure}
\begin{lstlisting}
data Exp : L where
  done : (*@$\uparrow$@*)(Atom (*@$\lto$@*) Exp)
  add : (*@$\uparrow$@*)(Atom (*@$\lto$@*) '+' (*@$\lto$@*) Exp (*@$\lto$@*) Exp)
data Atom : L where
  num : (*@$\uparrow$@*)(NUM (*@$\lto$@*) Atom)
  parens : (*@$\uparrow$@*)('(' (*@$\lto$@*) Exp (*@$\lto$@*) ')' (*@$\lto$@*) Atom)

data O : Nat (*@$\to$@*) Bool (*@$\to$@*) L where
  left : (*@$\uparrow$@*)(&[ n ] &[ b ] '(' (*@$\lto$@*) O (n + 1) b (*@$\lto$@*) O n b)
  num : (*@$\uparrow$@*)(&[ n ] &[ b ] NUM (*@$\lto$@*) DO n b (*@$\lto$@*) O n b)
  done : (*@$\uparrow$@*)(&[ n ] O n false)
  unexpected : (*@$\uparrow$@*)(&[ n ] (')' (*@$\oplus$@*) '+') (*@$\lto$@*) (*@$\top$@*) (*@$\lto$@*) O n false)
data D : Nat (*@$\to$@*) Bool (*@$\to$@*) L where
  lookAheadRP  : (*@$\uparrow$@*)(&[ n ] &[ b ] ((')' (*@$\otimes$@*) (*@$\top$@*)) (*@$\&$@*) C n b) (*@$\lto$@*) D n b)
  lookAheadNot  : (*@$\uparrow$@*)(&[ n ] &[ b ] (NotStartsWithRP (*@$\&$@*) M n b) (*@$\lto$@*) D n b)
data C : Nat (*@$\to$@*) Bool (*@$\to$@*) L where
  closeGood : (*@$\uparrow$@*)(&[ n ] &[ b ] ')' (*@$\lto$@*) D n b (*@$\lto$@*) C (n + 1) b)
  closeBad : (*@$\uparrow$@*)(&[ n ] ')' (*@$\lto$@*) C 0 b)
  done : (*@$\uparrow$@*)(&[ n ] C n false)
  unexpected : (*@$\uparrow$@*)(&[ n ] ('(' (*@$\oplus$@*) '+' (*@$\oplus$@*) NUM) (*@$\lto$@*) (*@$\top$@*) (*@$\lto$@*) C n false)
data M : Nat (*@$\to$@*) Bool (*@$\to$@*) L where
  doneGood : (*@$\uparrow$@*)(M 0 true)
  doneBad : (*@$\uparrow$@*)(&[ n ] M (n + 1) false)
  add : (*@$\uparrow$@*)(&[ n ] &[ b ] '(' (*@$\lto$@*) O n b (*@$\lto$@*) M n b)
  unexpected : (*@$\uparrow$@*)(&[ n ] ('(' (*@$\oplus$@*) ')'(*@$\oplus$@*) NUM) (*@$\lto$@*) (*@$\top$@*) (*@$\lto$@*) M n false)
\end{lstlisting}
\caption{Associative arithmetic expressions and a corresponding lookahead automaton}
\label{fig:binop-inductive}
\end{figure}
% \subsection{LL(1)}
% %% 4. LL(1) parser

% \steven{TODO impute LL(1) example, mention that its weak equivalence is mechanized}

\subsection{Unrestricted Grammars}
\newcommand{\stringNL}{String'}

While we have shown only examples for context-free grammars, in fact
arbitrarily complex grammars are encodable in \theoryabbv. To
demonstrate this, we show that for any \emph{non-linear} function $P :
\stringNL \to U$, where here $\stringNL$ is the \emph{non-linear} type
of strings over the alphabet, we can construct a grammar whose parses
correspond to $P$.
%% Define $\stringNL$ to be the non-linear list type over the alphabet $\Sigma$ and
%% let  be a mapping from the set of strings to the universe of
%% nonlinear types. The function $P$ describes a family of nonlinear types
%% dependent on $\stringNL$.
%% We view the image of $P$ as a set of parse trees for each input string, and thus
%% $P$ carries the structure of a formal grammar. However, even though $P$ describes this grammatical data, it is not
%% a first-class linear type in \theoryabbv. 
\[
Reify~P = \LinSigTyLimit {w} {\stringNL} {\LinSigTyLimit {x} {P~w} {~\lceil w \rceil}}
\]
%% \steven{Probably put this ceiling business around the discussion of the linear
%%   type of strings. Is this in section 3?}
where $\lceil \stringquote{} \rceil = I$ and
$\lceil c :: w \rceil = \literal c \otimes \lceil w \rceil$.

This reification operation on functions $\stringNL \to U$ is incredibly expressive,
as it allows to sidestep our linear typing connectives and utilize the whole of nonlinear
dependent type theory to define a grammar. For example, given a
Turing machine $Tur$ one may
define a non-linear predicate $accepts : \stringNL \to U$ such that $accept~w$ is equal to $\top$ if
$T$ accepts $w$ and equal to $0$ otherwise. Then, $Reify~accepts$ is a linear
type that captures precisely the strings accepted by $Tur$. That is,
$Lang(Reify~accepts)$ is recursively enumerable --- the most general class of
languages in the Chomsky hierarchy.
\begin{theorem}[\Agda]
  For any Turing machine, we can construct a grammar in \theoryabbv
  that accepts the same language as the Turing machine.
\end{theorem}

\section{Denotational Semantics and Implementation}
\label{sec:semantics-and-metatheory}



To justify our assertion that \theoryabbv is a syntax for formal
grammars and parse transformers, we will now define a
\emph{denotational semantics} that makes this mathematically
precise. To interpret our types of codes we assume a universe of
\emph{small} sets.

\subsection{Formal Grammars and Parse Transformers}

First, we need to establish a precise definition of a formal grammar
that accounts for the possibility of ambiguity. The most common
definition of a formal grammar is as generative grammars, defined by
a set of non-terminals, with a specified start symbol and set of
production rules. We instead use a more abstract formulation that is
closer in spirit to the standard definition of a formal language:

\begin{definition}
  A \emph{formal language} $L$ is a function from strings to propositions.
  A \emph{formal grammar} $A$ is a function from strings to small sets.
\end{definition}

We think of the grammar $A$ as taking a string to the set of all parse
trees for that string. However since $A$ could be any function
whatsoever there is no requirement that an element of $A(w)$ be a
``tree'' in the usual sense. This definition provides a simple,
syntax-independent definition of a grammar that can be used for any
formalism: generative grammars, categorial grammars, or our own
type-theoretic grammars. Note that the definition of a formal grammar
is a generalization of the usual notion of formal language since a
proposition can be equivalently defined as a subset of a one-element
set. Then the difference between a formal grammar and a formal
language is that formal grammars can be \emph{ambiguous} in that there
can be more than one parse of the same string. Even for unambiguous
grammars, we care not just about \emph{whether} a string has a parse
tree, but \emph{which} parse tree it has, i.e., what the structure of
the element of $A(w)$ is.

The linear types of \theoryabbv can all be interpreted as formal
grammars, but what about the linear terms? These can be interpreted as
\emph{parse transformers}, which we now define.
\begin{definition}
  Let $L_1,L_2$ be formal languages. Then we say $L_1\subseteq L_2$ if
  for every string $w$, $L_1(w)\Rightarrow L_2(w)$.

  Let $A_1,A_2$ be formal grammars. Then a parse transformer $f$ from
  $A_1$ to $A_2$ is a function assigning to each string $w$ a function
  $f_w : A_1(w) \to A_2(w)$.
\end{definition}
\pedro{Do we actually need a definition of formal language morphism?
In case we do, it should probably be its own definition.}
First, note that parse transformers generalize language inclusion: if
$A_1(w), A_2(w)$ are all subsets of a one-element set, then a parse
transformer is equivalent to showing that $A_1(w) \subseteq A_2(w)$.
In our denotational semantics, linear terms will be interpreted as
such parse transformers, and the notions of unambiguous grammar,
parseability, etc, introduced in \Cref{sec:concepts} can be verified
to correspond to their intended meanings under this interpretation.

Parse transformers can be composed: given two parse transformers $f$ and $g$,
their composition is defined pointwise, i.e. $(f\circ g)_w = f_w \circ g_w$.
Furthemore, given a formal grammar $A$, its identity transformer is $id_w =
id_{A(w)}$, where $id_{A(w)}$ is the identity function on the set $A(w)$. This
defines a \emph{category}.
\begin{definition}
  Define $\Grammar$ to be the category whose objects are formal
  grammars and morphisms are parse transformers.
\end{definition}
This category is equivalent to the slice category $\Set/\Sigma^*$ and
as such is very well-behaved. It is complete, co-complete, Cartesian
closed and carries a monoidal biclosed structure. We will use these
structures to model the linear types, terms and equalities in
\theoryabbv.
%% \begin{theorem}
%%   The category $\Grammar$ is complete, co-complete and cartesian
%%   closed. Furthermore, it is monoidal biclosed with the monoidal
%%   product given by
%%   \[ (A \otimes B) w = \{ (w_1,w_2,a,b) \pipe w_1w_2 = w \wedge a \in A w_1 \wedge b \in B w_2\} \]
%%   and unit by
%%   \[ I w = \{ () \pipe w = \varepsilon \} \]
%% \end{theorem}

%% \max{todo: need to decide if we need this section here or if it's in the intro already}.
% The linear typing judgment in our syntax takes on the following schematic form
% $\Gamma ; \Delta \vdash a : A$. First, $A$ represents a \emph{linear type} in
% our syntax. The intended semantics of these linear types are formal grammars.
% That is, the linear typing system is designed to syntactically reflect the
% behavior of formal grammars. For this reason, we may often interchangeably use
% the terms ``linear type'' and ``grammar''.

% The term $a$ is an inhabitant of type $A$, which is thought of as a parse tree of
% the grammar $A$. The core idea of this entire paper follows precisely from this
% single correspondence: grammars are types, and the inhabitants of these types
% are parse trees for the grammars.

% $\Gamma$ represents a non-linear context, while $\Delta$ represents a
% \emph{linear context} dependent on $\Gamma$. These linear contexts behave
% substructurally. As stated earlier, they are linear --- they do not obey
% weakening or contraction --- because a character is exhausted once read by a
% parsing procedure. Moreover, the characters in strings appear in the order in
% which we read them. We do not have the freedom to freely permute characters,
% therefore any type theory that is used to reason about formal grammars ought to
% omit the structural rule of exchange as well. This means that every variable
% within $\Delta$ must be used \emph{exactly once} and \emph{in order of
%   occurrence}. Thus, we can think of the linear contexts as an ordered list of
% limited resources. Once a resource is consumed, we cannot make reference to it
% again. Variables in a linear context then act like building
% blocks for constructing patterns over strings.

% We give the base types and type constructors for linear terms. As the
% interpretation of types as grammars in $\Grammar$ serves as our intended
% semantics, we simultaneously give the interpretation $\sem \cdot$ of the
% semantics as grammars.

% \pedro{The paragraphs above are a bit circular, what about the following alternative?}
% \steven{I am in favor of cutting what's above, but we haven't introduced the
%   syntax in the main body of the paper yet, even if the figures appeared
%   earlier. So what you write needs to take that into account}
% \pedro{In which case we should introduce it in the previous section. I suggest
%   adding this right after ``and the intuitionistic typing rules in fig 3''}
\subsection{Semantics}

We can now define our denotational semantics.
\begin{definition}[Grammar Semantics]
  We define the following interpretations by mutual recursion on the
  judgments of \theoryabbv:
  \begin{enumerate}
  \item For each non-linear context $\Gamma \isCtx$, we define a set $\sem \Gamma$.
  \item For each non-linear type $\Gamma \vdash X \isTy$, and element
    $\gamma \in \sem\Gamma$, we define a set $\sem X \gamma$.
  \item For each linear type $\Gamma A \isLinTy$ and element $\gamma
    \in \sem\Gamma$, we define a formal grammar $\sem{A}\gamma$. We
    similarly define a formal grammar $\sem\Delta\gamma$ for each
    linear context $\Gamma \Delta\isLinCtx$.
  \item For each non-linear term $\Gamma \vdash M : X$ and $\gamma \in \sem{\Gamma}$, we define an element $\sem{M}\gamma \in \sem{X}\gamma$.
  \item For each linear term $\Gamma; \Delta \vdash e : A$ and $\gamma \in \sem{\Gamma}$ we define a parse transformer from $\sem{\Delta}\gamma$ to $\sem{A}\gamma$.
  \end{enumerate}
  And we verify the following conditions:
  \begin{enumerate}
  \item If $\Gamma \vdash X \isSmall$, then $\sem X \gamma$ is a small set.
  \item If $\Gamma \vdash X \cong X'$ then for every $\gamma$, $\sem{X}\gamma = \sem{X'}\gamma$
  \item If $\Gamma \vdash A \cong A'$ then for every $\gamma$, $\sem{A}\gamma = \sem{A'}\gamma$
  \item If $\Gamma \vdash M \cong M' : X$ then for every $\gamma$, $\sem{M}\gamma = \sem{M'}\gamma$
  \item If $\Gamma;\Delta \vdash e \cong e' : A$ then for every $\gamma$, $\sem{e}\gamma = \sem{e'}\gamma$
  \end{enumerate}
\end{definition}

For reasons of brevity, we present only the cases related to the
linear types in \Cref{fig:semantics}. The full details are in the
appendix.\max{TODO: put the full details in the appendix}.  The
grammar for a literal $c$ has a single parse precisely when the input
string consists of the single character. The grammar for the unit
similarly has a single parse for the empty string.  A parse of the
tensor product $A \otimes B$ consists of a \emph{splitting} of the
empty string into a prefix $w_1$ and suffix $w_2$ along with an $A$
parse of $w_1$ and $B$ parse of $w_2$. A parse of $\bigoplus_{x:X} A$
is a pair of an element of the set $X$ and a parse of $A(x)$, while
dually a parse of $\bigwith_{x:X} A$ is a \emph{function} taking any
$x:X$ to a parse of $A(x)$. A $w$-parse of $A \lto B$ is a function
that takes an $A$ parse of some other string $w'$ to a $B$ parse of
$ww'$, and $B \tol A$ is the same except the $B$ parse is for the
reversed concatenation $w'w$.
%
The set $\uparrow A$ is the set of parse for the empty string for
$A$. This definition means that $\sem{\uparrow (A \lto B)}$ (or $\sem{\uparrow (B \tol A)}$) is
equivalent to the set of parse transformers:
\[ \sem{\uparrow (A \lto B)}\gamma = \sem{A \lto B}\gamma\,\varepsilon = \prod_{w'} \sem{A}\gamma\,w' \to \sem{B}\gamma\,w'\]
Next, a parse in the equalizer $\equalizer{a : A}{f}{g}$ is defined as
a parse in $\sem{A}$ that is mapped to the same parse by $f$ and
$g$. Recall that $\sem{f}\gamma$ and $\sem{g}$ are elements of
$\sem{\uparrow(A \lto B)}$, i.e., parse transformers.

The universe $L$ of linear types is interpreted as the set of all
(small) grammars.

\begin{figure}
  \begin{align*}
    \sem{c}\gamma\,w &= \{ c | w = c\}\\
    \sem{I}\gamma\,w &= \{ () \pipe w = \varepsilon \}\\
    \sem{A \otimes B}\gamma\,w &= \{ (w_1,w_2,a,b) \pipe w_1w_2 = w \wedge a \in \sem{A}\gamma\,w_1 \wedge b \in \sem{B}\gamma\,w_2 \}\\
    \sem{A \lto B}\gamma\,w &= \prod_{w'} \sem{A}\gamma\,w' \to \sem{B}\gamma\,ww'\\
    \sem{B \tol A}\gamma\,w &= \prod_{w'} \sem{A}\gamma\,w' \to \sem{B}\gamma\,w'w\\
    \sem{\bigoplus_{x:X} A}\gamma\,w &= \{ (x, a) \pipe x \in \sem{X}\gamma \wedge a \in \sem{A}(\gamma,x)w \}\\
    \sem{\bigwith_{x:X} A}\gamma\,w &= \prod_{x\in \sem{X}\gamma} \sem{A}(\gamma, x)\,w\\
    \sem{\uparrow A}\gamma &= \sem{A}\gamma\,\varepsilon\\
    \sem{\equalizer{a:A}{f}{g}}\gamma\,w &= \{ a \in \sem{A}\gamma\,w \pipe \sem{f}\gamma\,w\,a = \sem{g}\gamma\,w\,a \} \\
    \sem{L}\gamma &= \Grammar_0\\
    \sem{\SPF\,X}\gamma &= \textrm{DepPolyFunctor}(\sem{X}\gamma\times \Sigma^*,\Sigma^*)\\
    \sem{\el(F)}\gamma\,G &= \sem{F}\gamma\,G\\
    \sem{\map(F)}\gamma\,f &= \sem{F}\gamma\,f\\
    \sem{\mu A}\gamma &= \mu (\sem{A}\gamma) \tag{\Cref{thm:indexed-container}}\\
  \end{align*}
  \caption{Grammar Semantics}
\end{figure}

The most complex part of the semantics is the interpretation of
strictly positive functors and indexed inductive linear types.  We
interpret a strictly positive functor as a \emph{dependent polynomial
functor} on the category of sets, also sometimes called an
\emph{indexed container} \cite{dep-poly,indexed-container}.
\begin{definition}
  Let $I$ and $O$ be sets. A (dependent) \emph{polynomial} (of sets)
  from $I$ to $O$ consists of a set of shapes $S$, a set of positions
  $P$ and functions $I \leftarrow P \to S \to O$. The \emph{extension}
  of a polynomial is a functor $\Set/I \to \Set/O$ defined as the
  composite
  % https://q.uiver.app/#q=WzAsNCxbMCwwLCJcXFNldC9JIl0sWzMsMCwiXFxTZXQvTyJdLFsxLDAsIlxcU2V0L1AiXSxbMiwwLCJcXFNldC9TIl0sWzAsMiwiZl4qIl0sWzIsMywiXFxQaV9nIl0sWzMsMSwiXFxTaWdtYV9oIl1d
  \[\begin{tikzcd}
	    {\Set/I} & {\Set/P} & {\Set/S} & {\Set/O}
	    \arrow["{f^*}", from=1-1, to=1-2]
	    \arrow["{\Pi_g}", from=1-2, to=1-3]
	    \arrow["{\Sigma_h}", from=1-3, to=1-4]
  \end{tikzcd}\]
  Where $f^*$ is the pullback functor along $f$; $\Pi_g$ is the depedent
  product operation and $\Sigma_h$ is the dependent sum operation, which
  are, respectively, the right and left adjoint of their pullback functors
  $g^*$ and $h^*$.

  A dependent polynomial functor from $I$ to $O$ is a functor $\Set/I
  \to \Set/O$ that is nautrally isomorphic to the extension of a
  dependent polynomial from $I$ to $O$.
\end{definition}
With this interpretation, $\el(F)$ and $\map(F)$ are just interpreted
as the action of the functor on objects and morphisms,
respectively. We interpret the constructors $\mathsf{K},\Var$, etc. on
functors in the obvious way that matches the definitional behavior of
$\el$ and $\map$. The non-trivial part of the construction is
verifying that such functors are constructing corresponding dependent
polynomials. The details are tedious but straightforward extension of
prior work on dependent polynomials and indexed containers and we have
verified the construction in Agda.

The reason we use dependent polynomials functors on sets is that it is
well-known that their initial algebras exist
\cite{initial-algebras-of-dep-poly}. Additionally, initial algebras
for dependent polynomials are straightforwardly constructed in Agda as
an inductive type of ``IW'' trees, which is available as a
construction in the cubical library of Agda \cite{iw,cubical-library}.
\begin{theorem}
  Every dependent polynomial functor from $I$ to $I$ has an initial
  algebra.
\end{theorem}
Then an element $F \in \sem{X \to \SPF\,X}\gamma$ is an
$\sem{X}\gamma$-indexed family of polynomial functors from
$\sem{X}\gamma\times \Sigma^*$ to $\Sigma^*$, and taking the product
of these constructs a polynomial functor from $\sem{X}\gamma\times
\Sigma^*$ to itself. Then $\sem{\mu\,F}\gamma$ is defined to be the
initial algebra of this functor, and the initial algebra structure is
used to interpret $\roll, \fold$ and the corresponding axioms.

%% That is, strings match $A \lto B$ if when prepended with a parse of $A$ they
%% complete to parses of $B$. In this manner, the linear function types generalize
%% Brzozowksi's notion of derivative
%% \cite{brzozowskiDerivativesRegularExpressions1964}.
%% Brzozowski initially only gave an accounting of this operation for
%% generalized regular expressions, but later work from Might et al.\ demonstrates that the same
%% construction can be generalized to context free grammars
%% \cite{mightParsingDerivativesFunctional2011}. Here, via the linear function
%% types, the same notion of derivative extends to the grammars of \theoryabbv.

%% Note, to ensure that the linear function types do indeed generalize Brzozowski
%% derivatives, we must include the equalities in \cref{fig:brzozowskideriv} as axioms\max{we must? why must we}.
%% \pedro{We should further explain why these equations are required, or at the very
%% least give some intuition}

%% Of course, all the above statements for the left function type also have
%% corresponding analogues for the
%% right-handed counterpart.

%% \begin{figure}
%% \begin{align*}
%%   c\lto c &\equiv I\\
%%   c\lto d &\equiv 0\\
%%   c\lto I &\equiv 0\\
%%   c\lto 0 &\equiv 0\\
%%   c\lto (A \otimes B) & \equiv (c\lto A) \otimes B + (A \amp I) \otimes (c\lto B)\\
%%   c\lto A^* &\equiv (A \amp I)^{*} \otimes (c \lto A) \otimes A^* \\
%%   c\lto (A + B) &\equiv (c\lto A) + (c\lto B)
%% \end{align*}
%% \caption{Equality for Brzozowski Derivatives}
%% \label{fig:brzozowskideriv}
%% \end{figure}

%% \paragraph{LNL Dependent Types}
%% \steven{talk about how binary sum and binary with are definable over $\Bool$}
%% Given a non-linear type $X$, we may form both the dependent product type
%% $\LinPiTy x X A$ and the dependent pair type $\LinSigTy x X A$ as linear types
%% where $x$ is free in $A$.

%% $\sem{\LinPiTy x X A} \gamma w = \Pi(x:\sem{X}\gamma) \sem{A}(\gamma,x) w$

%% $\sem{\LinSigTy x X A} \gamma w = \Sigma(x:\sem{X}\gamma) \sem{A}(\gamma,x) w$

%% The grammar semantics of the linear product type is
%% indeed a dependent function out of the
%% semantics of $X$. Likewise, the grammar semantics of the linear dependent pair type is a dependent
%% pair in $\Set$.\pedro{The connection to grammar semantics needs to be better explained. For instance,
%% we should explain that disjunction corresponds to ``or'' of grammars}

%% Note that even though we do not take the binary additive conjunction and
%% disjunction as primitive, we may define them via these LNL dependent types. In
%% particular, via a dependence on $\Bool$.
%% %
%% \[
%%   A \amp B := \LinPiTy b \Bool {C(b)}
%% \]
%% \[
%%   A \oplus B := \LinSigTy b \Bool {C(b)},
%% \]
%% where $C(\true) = A, C(\false) = B$.

%% \paragraph{Universal Type}\max{this is absolutely not the right terminology. Universal type means multiple other completely different things}
%% \steven{Definable as nullary product}
%% The universal type $\top$ may be formed in any context.
%% %
%% \[ \sem{\top} \gamma w = \{ \ast \}\]
%% %
%% Its grammar semantics in set outputs the unit type in $\Set$ for all input strings in all contexts.

%% \paragraph{Empty Type}
%% \steven{Definable as nullary sum}
%% The empty type $0$ has no inhabitants. The elimination for the empty type
%% witnesses the principle of explosion --- i.e.\ from a term of type $0$ we may
%% introduce a term $\mathsf{absurd}_{A} : A$ for any type $A$.
%% %
%% \[ \sem {0} \gamma w = \emptyset \]
%% %
%% \paragraph{The $G$ Modality}
%% \steven{Rename this}
%% \steven{Compare to persistence modalities from separation logic}
%% The $G$ modality provides the embedding of linear types in the empty context
%% into non-linear types. The introduction and elimination forms for $G A$ obey the
%% same rules as given in \cite{bentonMixedLinearNonlinear1995} and \cite{krishnaswami_integrating_2015}.

%% The left adjoint $F$ from non-linear types back to linear types may be defined
%% as,
%% %
%% \[
%%   F X := \LinSigTy a A I
%% \]
%% %
%% \steven{Elaborate on $G$ more and rename $G A$to $\ltonl A$ in this section}

%% \pedro{Agreed :) maybe mention connections to persistent propositions from the
%% separation logic literature}

%% The semantics of $G A$ are exactly the semantics of $A$ at the empty word $\varepsilon$.
%% %
%% \[ \sem{G A} \gamma = \sem{A} \gamma \varepsilon \]
%% %
%% %
%% \paragraph{Recursive Types}
%% \steven{reflavor this section as ``indexed inductive'' instead of ``recursive'',
%% and entirely rework with the new inductive definition}
%% \steven{The non-indexed inductive are definable as being trivially indexed over unit}
%% Recursive linear types may be defined via the least-fixed point operator $\mu$.
% The grammar semantics of which are the fixed-point of sets induced by the
% grammar semantics of $A$.
% %
% \[ \sem{\mu x. A} \gamma = \mu (x:\Gr_i). \sem{A}(\gamma,x) \]
% %
% \pedro{We should justify, even if briefly, the existence of this
%   fixed-point}

% Observe that we need not take the Kleene star as a primitive
% grammar constructor, as it is definable as a fixed point.
% The Kleene star of a grammar $g$ is given as,

% \[
%   g^{*} := \mu X . I \oplus (g \otimes X)
% \]

% \begin{figure}[h!]
% \begin{mathpar}
%   \inferrule
%   {\Gamma ; \Delta \vdash p : I}
%   {\Gamma ; \Delta \vdash \mathsf{nil} : g^{*}}

%   \and

%   \inferrule
%   {\Gamma ; \Delta \vdash p : g \\ \Gamma ; \Delta' \vdash q
%   : g^{*}}
%   {\Gamma ; \Delta \vdash \mathsf{cons}(p , q) : g^{*}}

%   \\

%   \inferrule
%   {
%     \Gamma ; \Delta \vdash p : g^{*} \\
%     \Gamma ; \cdot \vdash p_{\varepsilon} : h \\
%     \Gamma ; x : g , y : h \vdash p_{\ast} : h
%   }
%   {\Gamma ; \Delta \vdash \mathsf{foldr}(p_{\varepsilon} , p_{\ast}) : g^{*}}
% \end{mathpar}
% \caption{Kleene Star Rules}
% \label{fig:star}
% \end{figure}

% Likewise, $g^{*}$ has admissible introduction and
% elimination rules, shown in \cref{fig:star}. Note that this
% definition of $g^{*}$ and these
% rules arbitrarily assigns a handedness to the Kleene star.
% We could have just as well took it to be a fixed point of
% $I \oplus (X \otimes g)$. In fact, the definitions are
% equivalent, as the existence of the $\mathsf{foldl}$ term below
% shows that $g^{*}$ is also a fixed point of
% $I \oplus (X \otimes g)$.
% This is a marked point of difference from Kleene
% algebra with recursion, where the fixed points for the left and right variants
% of Kleene star need not agree \cite{leiss}.

% \begin{equation}
%   \label{eq:foldl}
%   \inferrule
%   {
%     \Gamma ; \Delta \vdash p : g^{*} \\
%     \Gamma ; \cdot \vdash p_{\varepsilon} : h \\
%% %     \Gamma ; y : h , x : h \vdash p_{\ast} : h
%% %   }
%% %   {\Gamma ; \Delta \vdash \mathsf{foldl}(p_{\varepsilon} , p_{\ast}) : g^{*}}
%% % \end{equation}

%% % In fact, the $\mathsf{foldl}$ term is defined using
%% % $\mathsf{foldr}$ --- much in the same way one
%% % may define a left fold over lists in terms of a right fold
%% % in a functional programming language.
%% % The underlying trick is to fold over a list of linear functions
%% % instead of the original string. We curry each character $c$
%% % of the string into a function that concatenates $c$, and
%% % right fold over this list of linear functions. Because function
%% % application is left-associative, this results in a left
%% % fold over the original string.

%% % We only take fixed points of a single variable as a
%% % primitive operation in the type theory, but we may apply
%% % Beki\`c's theorem \cite{Bekić1984} to define an admissible
%% % notion of multivariate fixed point. This is particularly
%% % useful for defining grammars that encode the states of an
%% % automaton, as we will see in \cref{sec:automata}. In \cref{fig:multifix} we provide the
%% % introduction and elimination principles for such a fixed
%% % point, where $\sigma$ is the substitution that unrolls the
%% % mutually recursive definitions one level. That is,

%% % \begin{align*}
%% %   \sigma = \{ & \mu(X_{1} = A_{1} \dots, X_{n} = A_{n}).X_{1} / X_{1} , \dots, \mu(X_{1} = A_{1}, \dots, X_{n} = A_{n}).X_{n} / X_{n} \}
%% % \end{align*}

%% % \begin{figure}
%% % \begin{mathpar}
%% %   \inferrule
%% %   {\Gamma ; \Delta \vdash e : \simulsubst {A_{k}} {\sigma}}
%% %   {\Gamma ; \Delta \vdash \mathsf{cons}~e : \mu(X_{1} = A_{1}, \dots, X_{n} = A_{n}).X_{k}}

%% %   \and

%% %   \inferrule
%% %   {\Gamma ; \Delta \vdash e : \mu(X_{1} = A_{1}, \dots, X_{n} = A_{n}).X_{k} \\
%% %              \Gamma ; x_{j} : \simulsubst {A_{j}}{\sigma} \vdash e_{j} : B_{j}\quad \forall j
%% %   }
%% %   {\Gamma; \Delta \vdash \mathsf{mfold}(x_{1}.e_{1}, \dots, x_{n}.e_{n})(e) : B_{k}}
%% % \end{mathpar}
%% % \caption{Multi-fixed Points}
%% % \label{fig:multifix}
%% % \end{figure}

%% %% \section{Categorical Semantics of \theoryabbv}
%% %% \label{sec:categorify}

%% As described in previous sections, \theoryabbv has a ``standard''
%% interpretation where non-linear types denote sets, linear types denote
%% formal grammars, non-linear terms denote functions and linear terms
%% denote parse transformers.
%% %
%% Further, as is typical for type theory, every type constructor in the
%% calculus is interpreted by using a \emph{universal construction} in
%% either the category of sets or the category of grammars\footnote{the
%% only exception is that the axioms for Brzozowski derivatives we
%% considered do not follow solely from universal properties. In this
%% section we focus on models that do not necessarily satisfy these
%% axioms.}.
%% %
%% This leads to an immediate opportunity for generalization: the type
%% theory has in addition to the standard grammar-theoretic
%% interpretation, an interpretation in any categorical structure that
%% exhibits the same universal constructions.
%% %
%% On the one hand this gives us a structured way to formalize the
%% standard semantics precisely, but additionally it enables us to
%% consider \emph{non-standard} models in the next section that point to
%% further applications as well as meta-theoretic results.

%% In developing this categorical semantics, we will start with the
%% closest analogue from formal language theory: Kleene algebra.  Kleene
%% algebras are an important tool in the theory of regular languages
%% serving as a bridge between algebraic reasoning and equivalence of
%% regular expressions. More broadly, through various extensions, they
%% serve as a theoretical substrate to studying different kinds of formal
%% languages.
%% %
%% We can then see that the ``regular fragment'' of our type theory
%% (i.e., just characters, $\otimes$, $\oplus$, $0,1$ and a Kleene star)
%% has models in what we call a \emph{Kleene category} a categorification
%% of Kleene algebra from posets to categories.
%% %
%% We then further develop this into our final notion of model, which we
%% call a \emph{Chomsky category} as it can model not just regular
%% languages, but the full Chomsky hierarchy.

%% \subsection{Comparison to Kleene Algebra}
%% \subsection{Kleene Algebra and Kleene Categories}
%% A Kleene algebra is a tuple $(A, +, \cdot, (-)^*, 1, 0)$, where $A$ is
%% a set, $+$ and $\cdot$ are binary operations over $A$, $(-)^*$ is a
%% function over $A$, and $1$ and $0$ are constants. These structures
%% satisfy the axioms depicted in Figure~\ref{fig:axioms}.

%% \begin{figure}
%%   \begin{align*}
%%     x + (y + z) &= (x + y) + z & x + y &= y + x\\
%%     x + 0 &= x & x + x &= x\\
%%     x(yz) &= (xy)z & x1 &= 1x = x\\
%%     x(y + z) &= xy + xz & (x + y)z &= xz + yz\\
%%     x0 &= 0x = x & & \\
%%     1 + aa^* &\leq a^* & 1 + a^*a &\leq a^*\\
%%      b + ax \leq x &\implies a^*b \leq x &  b + xa \leq x &\implies ba^* \leq x
%%   \end{align*}
%%   \caption{Kleene algebra axioms}
%%   \label{fig:axioms}
%% \end{figure}

%% The addition operation can be used to define the partial order
%% structure $a \leq b$ if, and only if, $a + b = b$. In the theory of
%% formal languages, this order structure can be used to model language
%% containment. In this section, we categorify the concept of Kleene
%% algebra and build on top of it in order to define an abstract theory
%% of parsing. We start by defining \emph{Kleene categories}.

%% \begin{definition}
%%   A Kleene category is a distributive monoidal category $\cat{K}$
%%   such that for every objects $A$ and $B$, the endofunctors $F_{A, B}
%%   = B + A \otimes X$ and $G_{A, B} = B + X \otimes A$ have initial
%%   algebras (denoted $\mu X.\, F_{A, B}(X)$) such that $B \otimes (\mu
%%   X.\, F_{A, 1}) \cong \mu X.\, F_{A, B}(X)$ and the analogous isomorphism
%%   for $G_{A,B}$ also holds.
%% \end{definition}

%% As a sanity check, note that Kleene algebras are indeed examples of
%% Kleene categories.

%% \begin{example}
%%   Every Kleene algebra, seen a posetal category, is a Kleene category.
%%   The product $\cdot$ is a monoidal product and the addition is a
%%   least-upper bound, which corresponds to a coproduct. Lastly, the
%%   axioms of the Kleene star have a direct correspondence to the
%%   coherence conditions postulated by the initial algebras of Kleene
%%   categories.
%% \end{example}

%% This example provides a neat categorical justification to how
%% restrictive Kleene algebras are in terms of reasoning about
%% languages. By only having at most one morphism between objects, there
%% is not a lot of information they can convey. In this case, the only
%% information you get is language containment. As demonstrated by
%% $\mathbf{Gr}$, the extra degrees of freedom granted by having more
%% morphisms gives you more algebraic structure for reasoning about
%% languages.

%% For the next example, we see an unexpected connection with the theory
%% of substructural logics.

%% \begin{example}
%%   The opposite category of every Kleene category is a model of a variant of
%%   conjunctive ordered logic, where the Kleene star plays the role of the ``of
%%   course'' modality from substructural logics which allows hypotheses to
%%   be discarded or duplicated.
%% \end{example}

%% As we have seen, the proposed axioms are a direct translation of the
%% Kleene algebra axioms to a categorical setting. Its most unusual aspect is the
%% axiomatization of the Kleene star as a family of initial algebras
%% satisfying certain isomorphisms. If the Kleene category $\cat{K}$ has
%% more structure, then these isomorphisms hold ``for free''.

%% \begin{theorem}
%%   \label{th:kleeneclosed}
%%   Let $\cat{K}$ be a Kleene category such that it is also monoidal
%%   closed.  Then, the initial algebras isomorphisms hold automatically.
%% \end{theorem}
%% \begin{proof}
%%   We prove this by the unicity (up-to isomorphism) of initial
%%   algebras. Let $[hd, tl]: I + (\mu X.\, F_{A, I}(X)) \otimes A \to
%%   (\mu X.\, F_{A, I}(X))$ be the initial algebra structure of $(\mu
%%   X.\, F_{A, I}(X))$ and consider the map $[hd, tl] : B + B \otimes
%%   (\mu X.\, F_{A, I}(X)) \otimes A \to B\otimes (\mu X.\, F_{A,
%%     I}(X))$.

%%   Now, let $[f,g] : B + A \otimes Y \to Y$ be an $F_{A,B}$-algebra and
%%   we want to show that there is a unique algebra morphism $h : \mu X.\, F_{A,I} \to B \lto Y$. We can show existence and
%%   uniqueness by showing that the diagram on top commutes if, and
%%   only if, the diagram on the bottom commutes:

%% % https://q.uiver.app/#q=WzAsOCxbMCwwLCJCICsgQiBcXG90aW1lcyAoXFxtdSBYLlxcLCAxICsgWCBcXG90aW1lcyBBKSJdLFsyLDAsIkIgKyBZIFxcb3RpbWVzIEEiXSxbMiwyLCJZIl0sWzAsMiwiQiBcXG90aW1lcyAoXFxtdSBYLlxcLCAxICsgWCBcXG90aW1lcyBBKSJdLFswLDMsIjEgKyAoXFxtdSBYLlxcLCAxICsgWCBcXG90aW1lcyBBKSJdLFswLDUsIlxcbXUgWC5cXCwgMSArIFggXFxvdGltZXMgQSJdLFsyLDMsIjEgKyAoIEIgXFxsdG8gWSkgXFxvdGltZXMgQSJdLFsyLDUsIkIgXFxsdG8gWSJdLFswLDEsImlkICsgKGlkIFxcb3RpbWVzIGg7IGV2KSBcXG90aW1lcyBpZF9BIl0sWzEsMiwiW2YsZ10iXSxbMywyLCJpZCBcXG90aW1lcyBoOyBldiIsMl0sWzAsMywiW2lkIFxcb3RpbWVzIGgsIGlkIFxcb3RpbWVzIHRsXSIsMl0sWzQsNiwiaWQgKyAoaCBcXG90aW1lcyBYKVxcb3RpbWVzIGlkIl0sWzUsNywiaCIsMl0sWzYsNywiW2YnLCBnJ10iXSxbNCw1LCJbaGQsIHRsXSIsMl1d
%% \[\begin{tikzcd}
%% 	{B + B \otimes (\mu X.\, I + X \otimes A)} && {B + Y \otimes A} \\
%% 	\\
%% 	{B \otimes (\mu X.\, I + X \otimes A)} && Y \\
%% 	{I + (\mu X.\, I + X \otimes A)} && {I + ( B \lto Y) \otimes A} \\
%% 	\\
%% 	{\mu X.\, I + X \otimes A} && {B \lto Y}
%% 	\arrow["{id + (id \otimes h; ev) \otimes id_A}", from=1-1, to=1-3]
%% 	\arrow["{[f,g]}", from=1-3, to=3-3]
%% 	\arrow["{id \otimes h; ev}"', from=3-1, to=3-3]
%% 	\arrow["{[id \otimes h, id \otimes tl]}"', from=1-1, to=3-1]
%% 	\arrow["{id + (h \otimes X)\otimes id}", from=4-1, to=4-3]
%% 	\arrow["h"', from=6-1, to=6-3]
%% 	\arrow["{[f', g']}", from=4-3, to=6-3]
%% 	\arrow["{[hd, tl]}"', from=4-1, to=6-1]
%% \end{tikzcd}\]
%%   This equivalence follows by using the adjunction structure given
%%   by the monoidal closed structure of $\cat{K}$. A completely analogous
%%   argument for $G_{A,B}$ also holds. Furthermore, by generalizing the
%%   construction of \Cref{sec:formaltype}, we can also show that from
%%   the monoidal closed assumption it follows that $\mu X.\, F_{A, I}(X) \cong \mu X.\, G_{A, I}(X)$
%% \end{proof}

%% Something surprising about this lemma is that it provides an alternative
%% perspective on the observation that if a Kleene algebra has an
%% residuation operation, also called action algebra \cite{kozen1994action},
%% then the Kleene star admits a simpler axiomatization.

%% Since we want Kleene categories to generalize our notion of formal
%% grammars as presheaves $\String \to \Set$, we prove that they do
%% indeed form a Kleene category. We start by presenting a well-known
%% construction from presheaf categories.

%% \begin{definition}
%%   Let $\cat{C}$ be a locally small monoidal category and $F$, $G$ be
%%   two functors $\cat{C} \to \Set$. Their Day convolution tensor
%%   product is defined as the following coend formula:
%%   \[
%%   (F \otimes_{Day} G)(x) = \int^{(y,z) \in \cat{C}\times\cat{C}}\cat{C}(y\otimes z, x) \times F(y) \times G(z)
%%   \]
%%   Dually, its internal hom is given by the following end formula:
%%   \[
%%   (F \lto_{Day} G)(x) = \int_{y} \Set(F(y), G(x \otimes y))
%%   \]
%% \end{definition}

%% \begin{lemma}[Day \cite{day1970construction}]
%%   Under the assumptions above, the presheaf category $\Set^{\cat{C}}$ is
%%   monoidal closed.
%% \end{lemma}

%% %% \begin{theorem}
%% %%   Let $\cat{K}$ be a Kleene category and $A$ a discrete category.
%% %%   The functor category $[A, \cat{K}]$.
%% %%   (HOW GENERAL SHOULD THIS THEOREM BE? BY ASSUMING ENOUGH STRUCTURE,
%% %%   E.G. K = Set, THIS THEOREM BECOMES SIMPLE TO PROVE)
%% %% \end{theorem}
%% \begin{theorem}
%%   If $\cat{C}$ is a locally small monoidal category, then
%%   $\Set^{\cat{C}}$ is a Kleene category.
%% \end{theorem}
%% \begin{proof}

%%   By the lemma above, $\Set^{\cat{C}}$ is monoidal closed, and since it
%%   is a presheaf category, it has coproducts. Furthermore, the tensor
%%   is a left adjoint, i.e. it preserves colimits and, therefore, it is
%%   a distributive category.

%%   As for the Kleene star, since presheaf categories admit small colimits,
%%   the initial algebra of the functors $F_{A,B}$ and $G_{A,B}$ can be
%%   defined as the filtered colimit of the diagrams:

%%   From Theorem~\ref{th:kleeneclosed} it follows that these initial
%%   algebras satisfy the required isomorphisms and this concludes the
%%   proof.
%% \end{proof}

%% \begin{corollary}
%%   For every alphabet $\Sigma$, the presheaf category $\Set^{\cat{\Sigma^*}}$
%%   is a Kleene category.
%% \end{corollary}
%% \begin{proof}
%%   Note that string concatenation and the empty string make the
%%   discrete category $\Sigma^*$ a strict monoidal category.
%% \end{proof}

%% Much like in the posetal case, the abstract structure of a Kleene
%% category is expressive enough to synthetically reason about regular
%% languages. A significant difference between them is that while Kleene
%% algebras can reason about language containment, Kleene categories can
%% reason about \emph{ambiguity}, \emph{strong equivalence} of grammars.

%% \subsection{Lambek Hyperdoctrines and Chomsky Hyperdoctrines}

%% Though Kleene categories are expressive enough to reason about
%% concepts that are outside of reach of Kleene algebras, their
%% simply-typed nature makes them not so expressive from a type theoretic
%% point of view. This is limiting because type theories are successful
%% syntactic frameworks for manipulating complicated categorical
%% structures while avoiding some issues common in category theory, such
%% as coherence issues.

%% With this in mind, we want to design a categorical semantics that
%% builds on top of Kleene categories with the goal of extending them
%% with dependent types and making them capable reasoning about languages
%% and their parsers. This leads us to the abstract notion of model we
%% are interested in capturing with \theoryabbv: a \emph{Chomsky
%% category}.

%% We do this in two stages: first we define a \emph{Lambek hyperdoctrine} to be a
%% notion of model for the judgmental structure of \theoryabbv: that is we have
%% notions of linear and non-linear type, contexts, substitution and terms, but do
%% not assume any particular type constructions exist. Then we will define when a
%% Lambek hyperdoctrine is a Chomsky hyperdoctrine, meaning it can interpret all
%% the type formers of \theoryabbv and therefore arbitrary strength grammars.

%% \begin{definition}[Lambek Hyperdoctrine]
%%   A \emph{Lambek hyperdoctrine} consists of
%%   \begin{enumerate}
%%   \item A category $\mathcal C$ with a terminal object.
%%   \item A category with families structure over $\mathcal C$.
%%   \item A contravariant functor $L : \mathcal C^{o} \to \textrm{MultiCat}$ from
%%     $\mathcal C$ to the category of Multicategories.
%%   \end{enumerate}
%% \end{definition}
%% Here the objects of $\mathcal C$ model the non-linear contexts, and morphisms
%% model non-linear substitutions. The category with families structure over
%% $\mathcal C$ models the dependent non-linear types. Finally the
%% ``hyperdoctrine'' of multicategories $L$ models the linear types and terms. The
%% fact that this is functorial in $\mathcal C$ corresponds to the fact that linear
%% types are all relative to a non-linear context and that linear variables and
%% composition commute with non-linear substitution.

%% We will have three main Lambek hyperdoctrines of interest in this paper: the
%% ``standard model'' of sets and grammars, the ``syntactic model'' given by our
%% type theory itself and lastly a gluing model introduced in
%% Section~\ref{sec:canonicity} to prove the canonicity theorem.
%% \begin{example}
%%   \begin{enumerate}
%%   \item Let $\mathcal C$ be the category of sets, equipped with its usual
%%     category with families structures of families. Define $L : \mathcal C^{o}
%%     \to \textrm{MultiCat}$ to map $L(X)$ to the representable multicategory
%%     $(\Set^{\Sigma^*})^X$ of $X$-indexed families of grammars where the monoidal
%%     structure is given pointwise by the Day convolution monoidal structure.
%%   \item We can build a ``syntactic'' Lambek hyperdoctrine $\Syn(\Sigma)$ from the syntax
%%     itself: the category $\mathcal C$ is given by non-linear contexts and
%%     substitutions, the category with families by the non-linear types and terms
%%     and the hyperdoctrine of multicategories by the linear types and terms.
%%   \end{enumerate}
%% \end{example}

%% Next each type linear and non-linear type constructor corresponds to extra data
%% on a Lambek hyperdoctrine.
%% \begin{definition}
%%   \label{def:chomsky-data}
%%   \begin{enumerate}
%%   \item Dependent type structure: standard (say extensional type theory, give a reference)
%%   \item Universes
%%   \item Inductive grammars?
%%   \item Non-inductive Grammar constructors: standard monoidal category stuff
%%   \end{enumerate}
%% \end{definition}
%% \begin{definition}[Chomsky Hyperdoctrine]
%%   A Chomsky Hyperdoctrine is a Lambek hyperdoctrine equipped with all of the data in Definition~\ref{def:chomsky-data}.
%% \end{definition}
%% \pedro{We should probably define some of the words in this definition}
%% \begin{definition}
%%   A Chomsky category is a locally Cartesian category with two hierarchies of
%%   universes $\{L_i\}_{i\in \nat}$ and $\{U_i\}_{i\in \nat}$ such that
%%   every $L_i$ and $U_i$ are $U_{i+1}$-small. Furthermore, we require
%%   $U_i$ to be closed under dependent products and sum,
%%   $L_i$ to be closed under the Kleene category connectives,
%%   dependent products, left and right closed structures, with
%%   a type constructor $G : L_i \to U_i$ and a linear dependent sum
%%   going the other direction.
%% \end{definition}

%% \steven{Max suggests augmenting the definition of a Chomsky category to
%%   something like two categories $L$ and $U$, and $U$ is Cartesian closed I don't
%%   fully recall the rest.


%%   My best guess is that you take $L$ a Kleene category with a hierarchy of universes
%%   two, and you further require that $L$ is $Psh(U)$-enriched, except he
%%   suggested a further adjective on these presheaves. Perhaps representability?
%% }

%% \begin{theorem}
%%   The presheaf category $\Grammar = \Set^{\cat{\Sigma^*}}$ is a Chomsky category.
%% \end{theorem}


%% Further, the syntactic category of \theoryname is manifestly a Chomsky category.

%% \pedro{This is likely true, but if we explicitly say so, this warrants a proof. I think that
%% if we don't say anything about the syntactic category, reviewers won't mind.}

%% \steven{I agree that we don't want to say anything that opens unnecessary
%%   questions for proofs we haven't written. However, it seems hard to make the
%%   case that we have the right categorical model of the syntax if this statement
%%   isn't true. By restating our definition of Chomsky category, this should be
%%   obvious or a quick proof}

%% \pedro{I agree, then we should add the quick proof :) }

%% \subsection{Concrete Models of \theoryabbv}
%% \label{sec:othermodels}

%% \steven{Because we can define a version of semantic actions internally, we
%%   shouldn't put it as a separate model}

%% One of the powers of type theories is that it can be profitable to interpret them
%% in various models. In this section, by using our just-defined Chomsky categories,
%% we show how other useful concepts from formal language theory can also be organized
%% as models of \theoryabbv. We illustrate this point by providing two examples that
%% are closely related to the theory of formal languages: language equivalence and
%% semantic actions. Furthermore, in order to justify how $\mathbf{Gr}$ relates to
%% more traditional notions of parsing, we define a glued model that proves a
%% canonicity property of grammar terms.


%% \subsection{Language Semantics}
%% Every grammar induces a language semantics. Also languages can be taken as a
%% propositionally truncated view of the syntax. Logical equivalence should induce
%% weak equivalence, and thus even give a syntactic way to reason about language equivalence.
%% \steven{TODO language semantics}

%% % \subsection{Semantic Actions}
%% % \steven{Tentatively planning to cut this subsection for an internal representation
%% %   of semantic actions}
%% % Returning to the problem of parsing, the output of a parse usually is not the
%% % literal parse tree. Rather, the output is the result of some \emph{semantic
%% %   actions} ran on the parse tree, which usually serve to remove some syntactic
%% % details that are unnecessary for later processing.

%% % Given a grammar $G : \String \to \Set$, we define a semantic action to be a set
%% % $X$ with a function $f$ that produces a semantic element from any parse of $G$.

%% % \[
%% %   f : \PiTy w \String {G w \to X}
%% % \]

%% % Further, semantic actions can be arranged into a structured category.
%% % Define $\SemAct$ as the comma
%% % category $\Grammar / \Delta$, where $\Delta : \Set \to \Grammar$ defines a
%% % discrete presheaf. That is, for a set $X$, $\Delta (X)(w) = X$ for all
%% % $w \in \String$. As $\SemAct$ is defined as a comma category, it has a forgetful
%% % functor into $\Grammar$. That is, $\SemAct$ serves as a notion of formal
%% % grammar. Moreover, $\SemAct$ is a model of \theoryabbv.

%% % \steven{It being a notion of formal grammar is distinct from being a model. This
%% % probably warrants a proof}

%% % \steven{TODO semantic actions}

%% % \pedro{This is a very nice opportunity of showing off the supremacy of denotational
%% %   reasoning ;) We should probably prove the gluing lemma in the previous section
%% %   and apply it here and in the canonicity section. The actual proof might have to be
%% %   moved to the appendix, though}

%% \subsection{Parse Canonicity}
%% Canonicity is an important metatheoretic theorem in the type theory
%% literature.  It provides insight on the normal forms of terms and,
%% therefore, on its computational aspects. Frequently, proving
%% canonicity for boolean types, i.e. every closed term of type bool
%% reduces to either true or false, is enough to justify that the type
%% theory being studied is well-behaved. In our case, however, since we
%% want to connect \theoryabbv to parsers, we must provide a more
%% detailed account of canonicity. In particular, we give a nonstandard semantics
%% of \theoryabbv that carries a proof of canonicity along with it.

%% If $\cdot \vdash A$ is a closed linear type then there are
%% two obvious notions of what constitutes a ``parse'' of a string w
%% according to the grammar $A$:
%% \begin{enumerate}
%% \item On the one hand we have the set-theoretic semantics just
%%   defined, $\llbracket A \rrbracket \cdot w$
%% \item On the other hand, we can view the string $w = c_1c_2\cdots$ as
%%   a linear context $\lceil w \rceil = x_1:c_1,x_2:c_2,\ldots$ and
%%   define a parse to be a $\beta\eta$-equivalence class of linear terms $\cdot;
%%   \lceil w \rceil \vdash e : G$.
%% \end{enumerate}
%% It is not difficult to see that at least for the ``purely positive''
%% formulae (those featuring only the positive connectives
%% $0,+,I,\otimes,\mu, \overline\Sigma,c$) that
%% every element $t \in \llbracket A \rrbracket w$ is a kind of tree and
%% that the nodes of the tree correspond precisely to the introduction
%% forms of the type. However it is far less obvious that \emph{every}
%% linear term $\lceil w \rceil \vdash p : \phi$ is equal to some
%% sequence of introduction forms since proofs can include elimination
%% forms as well. To show that this is indeed the case we give a
%% \emph{canonicity} result for the calculus: that the parses for .

%% \begin{definition}
%%   A non-linear type $X$ is purely positive if it is built up using
%%   only finite sums, finite products and least fixed points.

%%   A linear type is purely positive if it is built up using only finite
%%   sums, tensor products, generators $c$, least fixed points and linear
%%   sigma types over purely positive non-linear types.
%% \end{definition}

%% \begin{definition}
%%   %% Let $X$ be a closed non-linear type. The closed elements $\textrm{Cl}(X)$ of $X$ are the definitional equivalence classes of terms $\cdot \vdash e : X$.

%%   Let $A$ be a closed linear type. The nerve $N(A)$ is a presheaf on
%%   strings that takes a string $w$ to the definitional equivalence
%%   classes of terms $\cdot; \lceil w\rceil \vdash e: N(A)$.
%% \end{definition}

%% \begin{theorem}[Canonicity]
%%   Let $A$ be a closed, purely positive linear type. Then there is an
%%   isomorphism between $\llbracket A\rrbracket$ and $N(A)$.
%% \end{theorem}
%% \begin{proof}
%%   We outline the proof here, more details are in the appendix. The
%%   proof proceeds first by a standard logical families construction
%%   that combines canonicity arguments for dependent type theory
%%   TODO cite coquand
%%   % \cite{coquand,etc}
%%   with logical relations constructions for linear
%%   types
%%   TODO cite hylandschalk
%%   % \cite{hylandschalk}
%%   . It is easy to see by induction that the
%%   logical family for $A$, $\hat A$ is isomorphic to $\llbracket A
%%   \rrbracket$ and the fundamental lemma proves that the projection
%%   morphism $p : \hat A \to N(A)$ has a section, the canonicalization
%%   procedure. Then we establish again by induction that
%%   canonicalization is also a retraction by observing that introduction
%%   forms are taken to constructors.
%% \end{proof}


%% \begin{enumerate}
%% \item Every term $\lceil w \rceil \vdash p : G + H$ is equal to $\sigma_1q$ or $\sigma_2 r$ (but not both)
%% \item There are no terms $\lceil w \rceil \vdash p : 0$
%% \item If there is a term $\lceil w \rceil \vdash p : c$ then $w = c$ and $p = x$.
%% \item Every term $\lceil w \rceil \vdash p : G \otimes H$ is equal to $(q,r)$ for some $q,r$
%% \item Every term $\lceil w \rceil \vdash p : \varepsilon$ is equal to $()$
%% \item Every term $\lceil w \rceil \vdash p : c$ is equal to $x:c$
%% \item Every term $\lceil w \rceil \vdash p : \mu X. G$ is equal to $\textrm{roll}(q)$ where $q : G(\mu X.G/X)$
%% \item Every term $\lceil w \rceil \vdash p : (x:A) \times G$ is equal
%%   to $(M,q)$ where $\cdot \vdash M : A$
%% \end{enumerate}

%% To prove this result we will use a logical families model. We give a
%% brief overview of this model concretely:
%% \begin{enumerate}
%% \item A context $\Gamma$ denotes a family of sets indexed by closing substitutions $\hat\Gamma : (\cdot \vdash \Gamma) \Rightarrow \Set_i$
%% \item A type $\Gamma \vdash X : U_i$ denotes a family of sets $\hat X : \Pi(\gamma:\cdot \vdash \Gamma) \hat\Gamma \Rightarrow (\cdot \vdash \simulsubst X \gamma) \Rightarrow \Set_i$
%% \item A term $\Gamma \vdash e : X$ denotes a section $\hat e : \Pi(\gamma)\Pi(\hat\gamma)\hat X \gamma \hat\gamma (\simulsubst e \gamma)$
%% \item A linear type $\Gamma \vdash A : L_i$ denotes a family of grammars $\hat A : \Pi(\gamma:\cdot\vdash\Gamma)\,\hat\Gamma \Rightarrow \Pi(w:\Sigma^*) (\cdot;\lceil w\rceil \vdash A[\gamma])\Rightarrow \Set_i$, and the denotation of a linear context $\Delta$ is similar.
%% \item A linear term $\Gamma;\Delta \vdash e : A$ denotes a function \[\hat e : \Pi(\gamma)\Pi(\hat\gamma)\Pi(w)\Pi(\delta : \lceil w \rceil \vdash \simulsubst \Delta \gamma) \hat\Delta \gamma \hat\gamma \delta \Rightarrow \hat A \gamma \hat\gamma w {(\simulsubst {\simulsubst e \gamma} \delta)}\]
%% \end{enumerate}
%% And some of the constructions:
%% \begin{enumerate}
%% \item $\widehat {(G A)} \gamma \hat\gamma e = \hat A \gamma \hat\gamma \varepsilon (G^{-1}e)$
%% \item $\widehat {(A \otimes B)} \gamma \hat\gamma w e = \Sigma(w_Aw_B = w)\Sigma(e_A)\Sigma(e_B) (e_A,e_B) = e \wedge \hat A \gamma \hat \gamma w_A e_A \times \hat B \gamma \hat \gamma w_B e_B$
%% \item $\widehat {(A \lto B)} \gamma \hat\gamma w e = \Pi(w_A)\Pi(e_A) \hat A \gamma\hat\gamma w_A e_A \Rightarrow \hat B \gamma\hat\gamma (ww_A) (\applto {e_A} e)$
%% \end{enumerate}

%% First, the category with families will be
%% the category of logical families over set contexts/types
%% $\Delta$/$A$. Then the propositional portion will be defined by
%% mapping a logical family $\hat \Gamma \to \Gamma$

%% First, let $L$ be the category of BI formulae and proofs (quotiented
%% by $\beta\eta$ equality). Define a functor $N : L \to \Set^{\Sigma^*}$ by
%% \[ N(\phi)(w) = L(w,\phi) \]

%% Then define the gluing category $\mathcal G$ as the comma category
%% $\Set^{\Sigma^*}/N$. That is, an object of this category is a pair of
%% a formula $\phi \in L$ and an object $S \in \mathcal
%% P(\Sigma^*)/N(\phi)$. We can then use the equivalence $\mathcal
%% P(\Sigma^*)/N(\phi) \cong \mathcal P(\int N(\phi))$ to get a simple
%% description of such an $S$: it is simply a family of sets indexed by
%% proofs $L(w,\phi)$:
%% \[ \prod_{w\in\Sigma^*} L(w,\phi) \to \Set \]
%% This category clearly comes with a projection functor $\pi : \mathcal
%% G \to \mathcal L$ and then our goal is to define a section by using
%% the universal property of $\mathcal L$.

%% To this end we define
%% \begin{enumerate}
%% \item $(\phi, S) \otimes (\psi, T) = (\phi \otimes \psi, S\otimes T)$ where
%%   \[ (S \otimes T)(w, p) = (w_1w_2 = w) \times (q_1,q_2 = p) \times S\,w_1\,q_1 \times T\,w_2\,q_2\]
%% \item $(\phi, S) \multimap (\psi, T) = (\phi \multimap \psi, S \multimap T)$ where
%%   \[ (S \multimap T)(w,p) = w' \to q \to S\,w'\,q \to T (ww') (p\,q) \]
%% \item $\mu X. ??$ ??
%% \end{enumerate}

%% \pedro{We should conclude this section by explaining the relevance of the canonicity
%% theorem. Could also be done before stating the theorem.}

\section{Discussion and Future Work}
\label{sec:discussion}
\paragraph{Lambek Calculus and Categorial Grammar}
In 1958 \cite{lambek1958mathematics}, Joachim Lambek introduced his syntactic
calculus as a logical system for linguistic derivations. In fact, one may view
the subtheory of our calculus generated over $\otimes, \lto, \tol$ to be a proof
relevant presentation of Lambek's original system.

\steven{Should discussion of this connection to Lambek calculus be spread
  throughout the paper?}

Lambek later explicitly connects his syntactic calculus to the structure of a
non-commutative biclosed monoidal category \cite{lambek1988categorial}. There is
a rich line of subsequent research after these revelations by Lambek, including
the study of pregroup grammars
\cite{coeckeMathematicalFoundationsCompositional2010}, the presentation of
abstract categorial grammars \cite{degrooteAbstractCategorialParsing2015}, and
even Lambek calculus with dependent types \cite{luoLambekCalculusDependent}. As
far as the authors can tell, these works seem to be primarily interested in natural
language semantics in the style of Montague grammars, rather than parsing formal
languages. That is, they seem to try to give a functorial semantic
interpretation of natural language.

There seem to be some implementations from the abstract categorial grammar
toolkit, but again they seem natural language focused and not up to the task of
verified formal language parsing.

Also mention here that \cite{luoLambekCalculusDependent} seems similar at first
glance, but isn't as similar as it might look because the judgments and
dependent types seem to
take on a different interpretation of ours. Say something here like, ``as we
understand it, it differs in x,y,z ways''

\steven{TODO Edit the above}
\steven{Even though the above sounds bad, are there Lambek/categorial things that are missed?}

%% 1. More practical: integrating it into a larger verified development
%%    - verified imperative implementation: Lambek logic ala separation
%%      logic?
%% 2. Semantic actions
%% 3. Type systems as tree grammars

\paragraph{Typed Approaches to Regular Expressions}
The usage of a simple type system to reason about regular expressions was
introduced by Frisch and Cardelli in 2004 \cite{frischCardelli}, and later expanded
by Hengelin and Nielsen in 2011 \cite{henglein_regular_2011} to handle proofs of regular expression
containment. In 1992, while investigating the addition of arbitrary recursion to Kleene
algebra \cite{leiss},
Lei{\ss} used a least fixed-point operator instead of the Kleene star. Although
he did not explicitly make use of a type system, or have access to the view of Frisch and
Cardelli, Lei{\ss} had added the full power of inductive types to a type system
of regular expressions.

Pratt's work on the residuation of action algebras in \cite{prattActionLogicPure1991} closely mirrors
our observation that Brzozowski derivatives mirror linear function types.
Moreover, he shows that his residuals form a Galois connection with sequential
composition. This is precisely a posetal --- or \emph{thin} --- version of the
adjunction between
linear function types and tensor product in \theoryabbv.

Our primary observation in this paper can be summarized as saying that all of
these above works can embed naturally into a rich, unifying theory.

\steven{I'd like to synthesize the discussion of these works with the categorial
grammar works, but I am not sure how. It seems as though there are these
parallel tracks of research that have missed each other for decades}

\paragraph{Inductive Linear Types}



% In \cite{firsovCertifiedNormalizationContextFree2015}, Firsov and Uustalu
% constructed a verified normalization procedure to turn a
% context-free grammar into an equivalent grammar in Chomsky normal form.
% Combining their work with a CYK parser, they have built a verified context-free
% grammar parser, \emph{up to weak equivalence}. To upgrade from a weak
% equivalence to a strong equivalence, they suggest updating their grammar
% formalism to treat parse trees as first-class proofs of language membership.
% That is, without explicitly stating so, they suggest that they next natural step
% is to reason about grammars in a system where parse trees are terms, as we have
% given in this paper.

% \steven{Make the above paragraph shorter. Also idk if its worth including, but I
% like that they practically ask for a type theory}

\steven{Mention Neel's typed algebraic approach paper here. Say it is similar at
  first glance, as its parsing with types, but that he actually is representing
  something quite different. Or is it so different that its not worth mentioning?}

\steven{TODO add Lambek calculus/categorial grammars here}

\paragraph{Kleene Algebra}
Since the early works in the theory of formal languages, Kleene
algebras have played an important role in its development. They
generalize the operations known from regular languages by introducing
operations generalizing language composition, language union and the
Kleene star.  More generally, they are defined as inequational theory
where the inequality is meant to capture language containment.  This
theory is extremely successful, having found applications in concurrency theory
\cite{hoare2000concurrent}, software-defined networks \cite{anderson2014netkat},
theory of programming languages \cite{angus2001kleene}, compiler optimizations
\cite{kozen2000certification} and more.

A frequently fruitful research direction is exploring varying
extensions of Kleene algebras, Kleene algebra with tests (KAT) being
one of the most notable ones \cite{kozen1997kleene}. Our approach is radically different from
most extensions, which usually aim at modifying or adding new
operations to Kleene algebras, but still keeping it as an inequational
theory. By adopting a category-theoretic treatment and allowing the
``order structure'' to encode more information than merely
inequalities, we were able to extend Kleene algebra to reason about
parsing as well.

\steven{Can we sketch here what an apt categorification of KATs might be? As a
  poor first pass: perhaps a Kleene category with a full involutive
  subcategory that corresponds to the Boolean algebra?.}
\steven{What can we say about internalizing Kleene algebra proofs or extensions?}
\steven{any KA citations needed above?}

%%    - verified imperative implementation: Lambek logic ala separation
%%      logic?
\paragraph{Relation to Separation Logic}

\theoryabbv is similar in spirit to separation logic. Semantically,
they are closely related: linear types in \theoryabbv denote families
of sets indexed by a monoid of strings, whereas separation logic
formulae typically denote families of predicates indexed by an ordered
partial commutative monoid of worlds. The monoidal structure in both
cases is are instances of the category-theoretic notion of Day
convolution monoidal structure. From a separation-logic perspective,
our notion of memory is very primitive: a memory shape is just a
string of characters and the state of the memory is never allowed to
evolve.

Syntactically, our type theory can be viewed as a kind of ``separation
type theory'' in that we care not just about provability of formulae
but the constructive content of our linear terms, as these act as
parsers. The design of \theoryabbv is based on the $\textrm{LNL}_D$
calculus which explicitly was designed to be a dependent
type-theoretic version of separation logic. Many concepts of
separation logic then have direct analogues in our system. The
characters $c$ are analogous to the ``points-to'' formulae $l \mapsto
x$ in separation logic. Our $\otimes,\lto,\tol$ are analogous to the
separating conjunction and magic-wand. The analogue of the persistence
modality $\square$ of separation logic is $\bigoplus_{\_:\uparrow A} I$,
which ``zeros out'' all but the parses from the empty string. If we
think of the portions of the string we are parsing as our notion of
resource, then these make sense as the ``resource-free'' elements of
the type $A$.

This semantic connection to separation logic also suggests an avenue
of future work: rather than working with a dependent type theory, we
could instead make a program logic based on non-commutative separation
logic for verifying imperative implementations of parsers. This could
be accomplished by modifying an existing separation logic
implementation or embedding the logic within \theoryabbv.

%% \paragraph{Linearity with Dependent Types}
%% \steven{I don't know if it worth discussing the different influences of
%%   categorical model here? We take Neel's model and add to it as necessary}

% Vakar might be worth citing to discuss different characterizations of the
% notion of model \cite{vakarSyntaxSemanticsLinear2015}

% \cite{fu2023twolevellineardependenttype} give a compiler for a two-level
% linear dependent type theory. This could serve as a template for a
% compiler implementation of \theoryname that is not just inference rules
% embedded in Agda.

\paragraph{Implementations in Parsing}
\steven{Talk about future implementation-side work}

\cite{fu2023twolevellineardependenttype} give a compiler for a two-level
linear dependent type theory. This could serve as a template for a
compiler implementation of \theoryname that is not just inference rules
embedded in Agda.

\paragraph{Type Checking and Elaboration}
Our focus in this work has been on the verification of parsers for
grammars over strings, but because \theoryabbv allows for the
definition of arbitrarily powerful grammars, the system could also be
used for more sophisticated semantic analysis such as scope checking
or type checking. Alternatively, we could more directly encode type
type systems as linear types in a modified version of \theoryabbv
whose semantics is not in families of sets indexed by \emph{strings}
but families of sets indexed by \emph{abstract syntax trees}.

%% While parsing typically refers to the generation of semantic
%% objects from string input, many tasks in programming can be
%% viewed as parsing of objects with more structure, such as
%% trees with binding structure or graphs. Fundamental to the
%% frontend of many
%% programming language implementations are type systems. In
%% particular, \emph{type checking}
%% --- analogous to language recognition --- and \emph{typed
%%   elaboration} --- analogous to parsing --- arise when
%% producing a semantic object subject to some analysis. Just
%% as our string grammars were given as functors from $\String$
%% to $\Set$, we envision adapting the same philosophy
%% to functors from a \emph{category of trees} to $\Set$ to craft a syntax
%% that natively captures typed elaboration. This suggests an
%% unusual sort of bunched type theory, where context extension
%% no longer resembles concatenation of strings but instead
%% takes on the form of tree constructors.

% type system              ~~ Formal Grammar
% typing derivation        ~~ parse tree
% algorithmic type system  ~~ LL(1), LR(1) grammar
% Uniqueness of derivations ~~ unambiguous grammar
% type elaborator           ~~ semantic actions

\subsection{Semantic Actions}

Our verification has mainly focused on the verification that a parser
outputs a correct concrete syntax tree for a grammar. However, in
practice, parsers are combined with a \emph{semantic action} that
emits an \emph{abstract} syntax tree that omits superflous syntactic
details that aren't needed in later stages of the overall program. We
can define a semantic action in \theoryabbv for a linear type $A$ with
semantic outputs in a non-linear type $X$ to be a function $\ltonl {(A
  \lto \bigoplus_{\_:X} \top)}$. That is, a semantic action is a
function that produces a semantic element of $X$ from the concrete
parses of $A$. In future work, we aim to study the question of
verifying efficient implementations of parsers with semantic actions.

\bibliographystyle{plain}
\bibliography{refs.bib}

\appendix



\section{Syntax}

\begin{figure}
  \begin{align*}
    \el(\Var\,M) B &= B\,M\\
    \el(\mathsf{K}\,A) B &= A\\
    \el(\bigoplus A) B &= \bigoplus_{y:Y}\el(A y)B\\
    \el(\bigamp A) B &= \bigamp_{y:Y}\el(A y)B\\
    \el(A \otimes A') B &= \el(A)B \otimes \el(A')B\\\\
    \map(\Var\,M)\,f &= f\,M\\
    \map(\K\,A)\,f &= \lambda a. a\\
    \map(\bigoplus A)\,f &= \lambda a. \letin{\oplusinj y {a_y}}{a}{\oplusinj y {\map(A\,y)\,f\,a_y}}\\
    \map(\bigwith\,A)\,f &= \lambda a. \withlamb{y}{\map(A\,y)\,f\,(\withprj y a)}\\
    \map(A \otimes A')\,f &= \lambda b. \letin{(a,a')}{b}{(\map(A)\,f\,a,\map(A')\,f\,a')}
  \end{align*}
  \caption{Strictly Positive Functors functorial actions}
\end{figure}

In this section we include the elided syntactic forms, as well as
definitions and basic properties of linear and non-linear
substitution.

\begin{figure}
  \begin{mathpar}
    \inferrule{~}{\ctxwff \cdot}
    \and
    \inferrule{\ctxwff \Gamma \\ \ctxwffjdg \Gamma X}{\ctxwff {\Gamma, x : X}}
    \inferrule{\Gamma \vdash X : U_i}{\ctxwffjdg \Gamma X}

    \boxed{\linctxwffjdg \Gamma A}

    \inferrule{~}{\linctxwff \Gamma \cdot}
    \and
    \inferrule{\linctxwff \Gamma \Delta \\ \linctxwffjdg \gamma A}{\linctxwff \Gamma {\Delta, a : A}}
  \end{mathpar}
  \caption{Context Rules}
\end{figure}

\max{TODO: add rules for non-linear types $\Pi,\Sigma$, equality etc}

\begin{figure}
  \begin{mathpar}
  \boxed{\Gamma \vdash M : X}

  \inferrule{~}{\Gamma, x : X, \Gamma' \vdash x : X}
  %
  \and
  %
  \inferrule{\Gamma \vdash M : Y \quad \ctxwffjdg \Gamma {X \equiv Y}}{\Gamma \vdash M : X}
  %
  \and
  %
  \inferrule{~}{\Gamma \vdash () : 1}
  %
  \and
  %
  \inferrule{\Gamma \vdash M : X \\ \Gamma \vdash N : \subst Y M x}{\Gamma \vdash (M, N) : \SigTy x X Y}
  %
  \and
%
  \inferrule{\Gamma \vdash M : \SigTy x X Y}{\Gamma \vdash \pi_1\, M : X}
  %
  \and
  %
  \inferrule{\Gamma \vdash M : \SigTy x X Y}{\Gamma \vdash \pi_2\, M : \subst Y {\pi_1\, M} x}
  \and
  \inferrule{\Gamma, x : X \vdash M : Y}{\Gamma \vdash \lamb x M : \PiTy x X Y}
  %
  \and
  %
  \inferrule{\Gamma \vdash M : \PiTy x X Y \\ \Gamma \vdash N : X}{\Gamma \vdash \app M {N} : \subst Y {N} x}
  %
  \and
  %
  \inferrule{\Gamma \vdash M \equiv N : X}{\Gamma \vdash \mathsf{refl} : M =_X N}
  \and
  \textrm{TODO: inductive types (boolean, nat) with their eliminators allowed in }
  \end{mathpar}
  \caption{Intuitionistic typing}
  \label{fig:inttyping}
\end{figure}

\begin{figure}
  \begin{mathpar}
    %
    \inferrule{\Gamma ; a : A , \Delta \vdash e : B}{\Gamma ; \Delta \vdash \lambtol a e : B\tol A}
    \and
    \inferrule{\Gamma ; \Delta \vdash e : B \tol A \\ \Gamma ; \Delta' \vdash e' : A}{\Gamma ; \Delta', \Delta \vdash \apptol e {e'} : B}
    %
    \\

  \end{mathpar}
  \caption{Linear Terms, extended}
\end{figure}

\begin{figure}
  \begin{mathpar}
    \boxed{\Gamma \vdash M \equiv N : X}

    \inferrule{\Gamma \vdash p : M =_X N}{\Gamma \vdash M \equiv N : X}
%
    \and
%
    \inferrule{~}{\Gamma \vdash \app {(\lamb x M)} {N} \equiv \subst x M {N} : X}
%
    \and
%
    \inferrule{~}{\Gamma \vdash m \equiv \lamb x {\app M x} : \PiTy x X Y}
%
    \and
%
    \inferrule{~}{\Gamma \vdash \pi_1\, (M, N) \equiv M : X}
%
    \and
%
    \inferrule{~}{\Gamma \vdash \pi_2\, (M , N) \equiv N : \subst x {M} Y}
%
    \and
%
    \inferrule{~}{\Gamma \vdash M \equiv (\pi_1\, M, \pi_2\, M) : \SigTy x X Y}
%
    \and
%
    % \inferrule{~}{}
%
    \inferrule{~}{\Gamma \vdash M \equiv N : 1}
%
    % \and
%
    % \inferrule{~}{\Gamma \vdash G\, (G^{-1} \, t) \equiv t : G A}

    \\

    \boxed{\Gamma ; \Delta \vdash e \equiv e' : A}

    % \inferrule{~}{\Gamma; \cdot \vdash G^{-1}\, (G \, t ) \equiv t: A}
%
%
    \inferrule{~}{\Gamma; \Delta \vdash \app {(\lamblto a e)} {e'} \equiv \subst e {e'} a : C}
%
    \and
%
    \inferrule{~}{\Gamma; \Delta \vdash e \equiv \lamblto a {\app e a} : A \lto B}
%
    \and
%
    \inferrule{~}{\Gamma; \Delta \vdash \app {(\lambtol a e)} {e'} \equiv \subst e {e'} a : C}
%
    \and
%
    \inferrule{~}{\Gamma; \Delta \vdash e \equiv \lambtol a {\app e a} : B \tol A}
%
    \and
%
    \inferrule{~}{\Gamma; \Delta \vdash \app {(\dlamb x e)} {M} \equiv \subst {e} M x : C}
%
    \and
%
    \inferrule{~}{\Gamma; \Delta \vdash e \equiv \dlamb x {\app e x} : \LinPiTy x X A}
%
    \and
%
%     \inferrule{~}{\Gamma; \Delta \vdash e \equiv e' : \top}
% %
%     \and
%
%     \inferrule{~}{\Gamma; \Delta \vdash e_i \equiv \pi_i (e_1, e_2) : A_i}
% %
%     \and
% %
%     \inferrule{~}{\Gamma; \Delta \vdash e \equiv (\pi_1 e, \pi_2 e) : A\& B}
%
    \and
%
    \inferrule{~}{\Gamma; \Delta \vdash \letin {()} {()} e \equiv e : C}
    %
    \and
    %
    \inferrule{~}{\Gamma; \Delta \vdash \letin {()} e {\subst {e'} {()} a} \equiv \subst {e'} e a : C}
%
    \and
%
    \inferrule{~}{\Gamma; \Delta \vdash \letin {a \otimes b} {e \otimes e'} e'' \equiv e'' \{ e/a , e'/b \} : C}
%
    \and
    %
    \inferrule{~}{\Gamma; \Delta \vdash \letin {a \otimes b} e {\subst {e'} {a \otimes b} c} \equiv \subst {e'} e c : C}
%
    \and
%
    \inferrule{~}{\Gamma;\Delta \vdash \letin {(x, a)} {(M, e)} {e'} \equiv e' \{ M/x , e/a \} : C}
    %
    \and
    %
    \inferrule{~}{\Gamma; \Delta \vdash \letin {(x, a)} e {\subst {e'} {(x, a)} y} \equiv \subst {e'} e y : C}
    \and
    %
    \inferrule
    {~}
    {\Gamma ; \Delta \vdash \equalizerpi {\equalizerin {e}} \equiv e : A}
    %
    \and
    %
    \inferrule
    {~}
    {\Gamma ; \Delta \vdash \equalizerin {\equalizerpi {e}} \equiv e : A}
\end{mathpar}
  \caption{Judgmental equality for linear terms}
  \label{fig:jdgeq}
\end{figure}
\steven{Do we need inverse for $\ltonl A$ in judgmental equality figure?}
\steven{Do we need to say anything about the equalizer proofs being unique in
  judgmental equality figure?}

\begin{definition}
  The set of (non-linear) substitutions $\gamma \in
  \textrm{Subst}(\Gamma,\Gamma')$ where $\Gamma \isCtx$ and $\Gamma'
  \isCtx$ is defined by recursion on $\Gamma$:
  \begin{align*}
    \textrm{Subst}(\Gamma,\cdot) &= \{\cdot \}\\
    \textrm{Subst}(\Gamma,\Gamma',x:A) &= \{ (\gamma,M/x) \pipe \gamma \in \textrm{Subst}(\Gamma,\Gamma') \wedge \Gamma \vdash M : A[\gamma] \}
  \end{align*}
  simultaneously with an action of substitution on types, terms,
  etc. in the standard way.

  \max{do we need identity and composition of substitutions and their functoriality?}
  %% By induction we can define an identity substitution
  %% $\textrm{Subst}(\Gamma,\Gamma)$ which maps each variable to
  %% itself. We write this substitution simply as $\Gamma$.
  %% \begin{align*}
  %%   \cdot \circ \gamma' &= \cdot\\
  %%   (\gamma,M/x) \circ \gamma' &= (\gamma\circ \gamma') , M[\gamma']/x
  %% \end{align*}

  It is straightforward, but laborious to establish that all forms in
  the type theory that are parameterized by a non-linear context
  $\Gamma$ support an admissible action of a substitution $\gamma \in
  \Subst(\Gamma',\Gamma)$:
  \begin{mathpar}
    \inferrule
    {\Gamma \vdash X \isTy}
    {\Gamma' \vdash X[\gamma] \isTy}

    \inferrule
    {\Gamma \vdash X \isSmall}
    {\Gamma' \vdash X[\gamma] \isSmall}

    \inferrule
    {\Gamma \vdash X \equiv Y}
    {\Gamma' \vdash X[\gamma] \equiv Y[\gamma]}

    \inferrule
    {\Gamma \vdash M : X}
    {\Gamma' \vdash M[\gamma] : X[\gamma]}

    \inferrule
    {\Gamma \vdash M \equiv N : X}
    {\Gamma' \vdash M[\gamma] \equiv N[\gamma] : X[\gamma]}

    \inferrule
    {\Gamma \vdash \Delta \isLinCtx}
    {\Gamma' \vdash \Delta[\gamma] \isLinCtx}

    \inferrule
    {\Gamma \vdash A \isLinTy}
    {\Gamma' \vdash A[\gamma] \isLinTy}

    \inferrule
    {\Gamma \vdash A \equiv B}
    {\Gamma' \vdash A[\gamma] \equiv B[\gamma]}

    \inferrule
    {\Gamma;\Delta \vdash e : A}
    {\Gamma'; \Delta[\gamma] \vdash e[\gamma] : A[\gamma]}j

    \inferrule
    {\Gamma;\Delta \vdash e \equiv f : A}
    {\Gamma'; \Delta[\gamma] \vdash e[\gamma] \equiv f[\gamma] : A[\gamma]}j
  \end{mathpar}

  %% Further, these all satisfy identity and composition equations M[\Gamma] = M and M[\gamma][\gamma'] = M[(\gamma \circ \gamma')]
\end{definition}

\begin{definition}
  Let $\Gamma \vdash \Delta \isLinCtx$ and $\Gamma \vdash \Delta'
  \isLinCtx$. The set of linear substitutions $\Subst(\Delta',\Delta)$
  is defined by recursion on $\Delta$:
  \begin{align*}
    \Subst(\Delta',\cdot) &= \{ \cdot \pipe \Delta' = \cdot \}\\
    \Subst(\Delta',(\Delta,a:A)) &= \{ (\delta, e/a) \pipe \delta \in \Subst(\Delta_1, \Delta) \and \Delta_2 \vdash e : A \and \Delta' = \Delta_1,\Delta_2\}
  \end{align*}

  Given substitutions $\delta_1 \in \Subst(\Delta_1', \Delta_1)$ and
  $\delta_2 \in \Subst(\Delta_2', \Delta_2)$, we can define a
  substitution $\delta_1,\delta_2 \in
  \Subst((\Delta_1',\Delta_2'),(\Delta_1,\Delta_2))$. Furthermore, for
  any substitution $\delta \in \Subst(\Delta,(\Delta_1,\Delta_2))$, we
  can deconstruct $\delta = \delta_1,\delta_2$ with $\delta_1 \in
  \Subst(\Delta_1', \Delta_1)$ and $\delta_2 \in \Subst(\Delta_2',
  \Delta_2)$.
  %% Again for any $\Gamma \vdash \Delta$, we can define an identity
  %% substitution mapping each variable to itself. Additionally we can define composition of 
\end{definition}

\begin{definition}
  Given any $\Gamma ; \Delta \vdash e : A$ and $\delta \in
  \Subst(\Delta',\Delta)$, we define the action of the substitution on
  $e$ as follows, frequently using the inversion principle to split
  the substitution into constituent components.
  \begin{align*}
    a[e/a] &= e\\
    (e_1,e_2)[\delta_1,\delta_2] &= (e_1[\delta_1], e_2[\delta_2])\\
    (\letin {(a , b)} e {e'})[\delta_1,\delta_2,\delta_3] &= \letin{(a,b)} {e[\delta_2]}{e'[\delta_1,a/a,b/b,\delta_2]}\\
    ()[\cdot] &= ()\\
    \letin{()} e {e'}[\delta_1,\delta_2,\delta_3] &= \letin{()} {e[\delta_2]}{e'[\delta_1,a/a,b/b,\delta_2]}\\
    (\lamblto {a} e)[\delta] &= \lamblto a {e[\delta,a/a]}\\
    (\applto {e'} {e})[\delta_1,\delta_2] &= \applto {e'[\delta_1]} {e[\delta_2]}\\
    (\lamblto {a} e)[\delta] &= \lamblto a {e[\delta,a/a]}\\
    (\applto {e'} {e})[\delta_1,\delta_2] &= \applto {e'[\delta_1]} {e[\delta_2]}\\
    (\lambtol {a} e)[\delta] &= \lamblto a {e[a/a,\delta]}\\
    (\apptol {e'} {e})[\delta_1,\delta_2] &= \apptol {e'[\delta_1]} {e[\delta_2]}\\
    (\dlamb x e)[\delta] &= \dlamb x {e[\delta]}\\
    (e\,.\pi\,M)[\delta] &= (e[\delta]\,.\pi\,M)\\
    (\sigma\,M\,e)[\delta] &= \sigma\,M\,e[\delta]\\
    (\letin{\sigma\,x\,a}{e}{e'})[\delta_1,\delta_2,\delta_3] &= \letin{\sigma\,x\,a} {e[\delta_2]}{e'[\delta_1,\delta_3]}\\
    (\equalizerin{e})[\delta] &= \equalizerin{e[\delta]}\\
    (\equalizerpi{e})[\delta] &= \equalizerpi{e[\delta]}
  \end{align*}
  By induction on linear term and equality judgments, we establish the
  following admissible rules for $\delta \in \Subst(\Delta',\Delta)$:
  \begin{mathpar}
    \inferrule
    {\Gamma; \Delta \vdash e : A}
    {\Gamma; \Delta' \vdash e[\delta] : A}

    \inferrule
    {\Gamma; \Delta \vdash e \equiv f : A}
    {\Gamma; \Delta' \vdash e[\delta] \equiv f[\delta'] : A}
  \end{mathpar}

  \max{identity and composition?}
\end{definition}

\section{Denotational Semantics}

\begin{definition}
  We define the unit grammar
  \[ I\,w = \{ ()\pipe w = \varepsilon \}\]
  and given grammars $A$ and $B$ we define their tensor product
  \[ (A \otimes B)\,w = \{ (w_1,w_2,a,b) \pipe w_1w_2 = w \wedge a \in A\,w_1 \wedge b \in B\,w_2 \} \]
  This extends to a monoidal structure on the category of grammars. Note that we defined in the semantics of linear types $\sem{I}\gamma = I$ and $\sem{A \otimes B}\gamma = \sem{A}\gamma \otimes \sem{B} \gamma$
\end{definition}

\begin{definition}[Denotation of Linear Contexts]
  The semantics of linear contexts $\Gamma \vdash \Delta \isLinCtx$ is
  defined as follows:
  \max{make sure this aligns with our interpretation of $\lto$/$\tol$}
  \begin{align*}
    \sem{\cdot}\,\gamma &= I \\
    \sem{\Delta,x:A}\,\gamma &= \sem{\Delta}\,\gamma \otimes \sem{A}\gamma \\
  \end{align*}
\end{definition}

We define denotations of linear terms in
\Cref{fig:linear-term-semantics}. Note that the denotations interpret
typing derivations, not raw terms, as the data of how contexts are
split is needed in order to construct the correct associator
functions.
\max{todo: describe associators, define denotations, prove axioms}
\begin{figure}
  \begin{mathpar}
  \inferrule
  {\Delta_1 \vdash e_1 : A_1 \and \Delta_2 \vdash e_2 : A_2}
  {\sem{(e_1,e_2)}\gamma\,w\,\delta
    = \letin{(w_1,w_2,\delta_1,\delta_2)} {m(\Delta_1,\Delta_2)(\delta)} {(w_1,w_2,\sem{e_1}\gamma\,w_1\,\delta_1, \sem{e_2}\gamma\,w_2\,\delta_2)}
  }

  \inferrule
  {\Delta_2 \vdash e : A \otimes B \and \Delta_1,a:A,b:B,\Delta_3 \vdash e' : C}
  {\sem{\letin{(a,b)}{e}{e'}}\gamma\,w\,\delta =
    \letin{(w_1,w_2w_3,\delta_1,(w_2,w_3,\delta_2,\delta_3))}{m_{\Delta_1,\Delta_2,\Delta_3}(\delta)}
    \letin{(w_{2,1},w_{2,2},a,b)}{\sem{e}\gamma\,w_2\,\delta_2}
    {\sem{e'}\gamma\,w\,(w_1,w_{2,1}w_{2,2}w_3, \delta_1, (w_{2,1},w_{2,2}w_3, a, (w_{2,2},w_3, b, \delta_3)))}
    }
\end{mathpar}
  \caption{Denotations of linear terms}
\end{figure}

\begin{definition}
  For any $\Gamma \vdash \Delta_1,\Delta_2 \isLinCtx$ and $\gamma \in
  \sem{\Gamma}$ there is an isomorphism $m_{\Delta_1,\Delta_2} :
  \sem{\Delta_1, \Delta_2}\gamma \cong \sem{\Delta_1}\gamma \otimes
  \sem{\Delta_2}\gamma$.

  This can be extended to a sequence of contexts of any length.
\end{definition}

\begin{lemma}[Equations for $\otimes$]
  
\end{lemma}

\begin{lemma}[Equations for $I$]
  
\end{lemma}

\begin{lemma}[Equations for $\lto$]
  
\end{lemma}

\begin{lemma}[Equations for $\tol$]
  
\end{lemma}


\begin{align*}
  \sem{(e_1,e_2)}\gamma\,w\,\delta &=
  \letin{(w_1,w_2,\delta_1,\delta_2)} {m(\Delta_1,\Delta_2)(\delta)} {(w_1,w_2,\sem{e_1}\gamma\,w_1\,\delta_1, \sem{e_2}\gamma\,w_2\,\delta_2)}\\
\end{align*}



\end{document}
