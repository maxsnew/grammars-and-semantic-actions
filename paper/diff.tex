% -*- fill-column: 80; -*-
%DIF LATEXDIFF DIFFERENCE FILE
%DIF DEL submission.tex   Tue Mar 25 11:26:41 2025
%DIF ADD paper.tex        Tue Mar 25 21:46:00 2025
%DIF 2c2-36
%DIF < \documentclass[review,anonymous,screen,acmsmall,nonacm]{acmart}
%DIF -------
%% For double-blind review submission, w/o CCS and ACM Reference (max submission space) %DIF > 
\documentclass[acmsmall,review,anonymous]{acmart}\settopmatter{printfolios=true,printccs=false,printacmref=false} %DIF > 
%% For final camera-ready submission, w/ required CCS and ACM Reference %DIF > 
%\documentclass[acmsmall]{acmart}\settopmatter{} %DIF > 
 %DIF > 
 %DIF > 
%% Journal information %DIF > 
%% Supplied to authors by publisher for camera-ready submission; %DIF > 
%% use defaults for review submission. %DIF > 
\acmJournal{PACMPL} %DIF > 
\acmVolume{1} %DIF > 
\acmNumber{CONF} % CONF = POPL or ICFP or OOPSLA %DIF > 
\acmArticle{1} %DIF > 
\acmYear{2018} %DIF > 
\acmMonth{1} %DIF > 
\acmDOI{} % \acmDOI{10.1145/nnnnnnn.nnnnnnn} %DIF > 
\startPage{1} %DIF > 
 %DIF > 
%% Copyright information %DIF > 
%% Supplied to authors (based on authors' rights management selection; %DIF > 
%% see authors.acm.org) by publisher for camera-ready submission; %DIF > 
%% use 'none' for review submission. %DIF > 
\setcopyright{none} %DIF > 
%\setcopyright{acmcopyright} %DIF > 
%\setcopyright{acmlicensed} %DIF > 
%\setcopyright{rightsretained} %DIF > 
%\copyrightyear{2018}           %% If different from \acmYear %DIF > 
 %DIF > 
%% Bibliography style %DIF > 
\bibliographystyle{ACM-Reference-Format} %DIF > 
%% Citation style %DIF > 
%% Note: author/year citations are required for papers published as an %DIF > 
%% issue of PACMPL. %DIF > 
\citestyle{acmauthoryear}   %% For author/year citations %DIF > 
%% \citestyle{acmnumeric} %DIF > 
%DIF -------
\usepackage{mathpartir}
\usepackage{tikz-cd}
\usepackage{lipsum}
\usepackage{enumitem}
\usepackage{wrapfig}
\usepackage{fancyvrb}
\usepackage{xspace}
%DIF 10a44-46
\usepackage{thmtools} % needed for sane cleveref naming of theorems %DIF > 
\usepackage{hyperref} %DIF > 
\usepackage[capitalise]{cleveref} %DIF > 
%DIF -------

\usepackage[LGR,T1]{fontenc}
\DeclareMathAlphabet{\mathgtt}{LGR}{cmtt}{m}{n}

%DIF 14-15d51
%DIF <  %% \steven{TODO make the Agda logo clickable} %
%DIF < \newcommand{\Agda}{\agdalogo}
%DIF -------
\renewcommand{\Gamma}{\mathgtt{G}}
\renewcommand{\Delta}{\mathgtt{D}}
\renewcommand{\Sigma}{\mathgtt{S}}
\renewcommand{\mu}{\mathgtt{m}}

\renewcommand{\familydefault}{\ttdefault}
\usepackage{mathastext}
\renewcommand{\familydefault}{\rmdefault}

\definecolor{codegreen}{rgb}{0,0.6,0}
\definecolor{codegray}{rgb}{0.5,0.5,0.5}
\definecolor{codepurple}{rgb}{0.58,0,0.82}
\definecolor{backcolour}{rgb}{0.95,0.95,0.92}

\usepackage{listings}
\lstdefinelanguage{Agda}{
keywords={data, where, module, import, open, public,
record, field, let, in, if, then, else, case, of, with,
do, postulate, primitive, mutual, abstract, private,
forall, exists, cong, set, prop, sort, Level, Data, Type,
Renamer},
morekeywords=[2]{Set, Type, Prop, moduleFuncs, instance, Named},
sensitive=true,
comment=[l]{--},
morecomment=[s]{\{-}{-\}},
morestring=[b]",
mathescape=true,
escapeinside={(*@}{@*)}
}
\lstset{
language=Agda,
%DIF 47c82
%DIF < basicstyle=\ttfamily\small,
%DIF -------
basicstyle=\ttfamily\footnotesize, %DIF > 
%DIF -------
keywordstyle=\color{blue},
keywordstyle=[2]\color{teal},
identifierstyle=\color{black},
commentstyle=\color{gray}\textit,
stringstyle=\color{orange},
numbers=none,
numberstyle=\tiny\color{gray},
stepnumber=1,
numbersep=5pt,
showstringspaces=false,
tabsize=4,
captionpos=b,
breaklines=true,
breakatwhitespace=false,
rulecolor=\color{black},
texcl=true,
backgroundcolor=\color{backcolour},
frame=single
}

%DIF 68-69d103
%DIF < \usepackage{hyperref}
%DIF < \usepackage[capitalise]{cleveref}
%DIF -------
\usepackage{stmaryrd}
\usepackage{scalerel}
\usepackage{tikz}
\usetikzlibrary{automata, positioning, arrows, fit}

\usepackage{pdfpages}

\newcommand{\Subst}{\textrm{Subst}}
%DIF 78c111
%DIF < \newcommand{\SPF}{\mathsf{SPF}}
%DIF -------
\newcommand{\SPF}{SPF} %DIF > 
%DIF -------
\newcommand{\Var}{\mathsf{Var}}
\newcommand{\K}{\mathsf{K}}
\newcommand{\map}{\mathsf{map}}
\newcommand{\roll}{\mathsf{roll}}
\newcommand{\fold}{\mathsf{fold}}
\newcommand{\inl}{\mathsf{inl}}
\newcommand{\inr}{\mathsf{inr}}
%DIF 86c119
%DIF < \newcommand{\sem}[1]{\llbracket{#1}\rrbracket}
%DIF -------
\newcommand{\sem}[1]{\left\llbracket{#1}\right\rrbracket} %DIF > 
%DIF -------
\newcommand{\semg}[1]{\sem{#1}\gamma}
\newcommand{\cat}[1]{\mathbf{#1}}
\newcommand{\lto}{\multimap}
\newcommand{\tol}{\mathrel{\rotatebox[origin=c]{180}{$\lto$}}}
%DIF 91a124
 %DIF > 
%DIF -------
\newcommand{\String}{\textbf{String}}
\newcommand{\Char}{\textbf{Char}}
\newcommand{\stringg}{\texttt{String}}
\newcommand{\charg}{\mathtt{Char}}
%DIF 95a129-132
\newcommand{\StringSem}{\mathbf{String}} %DIF > 
\newcommand{\CharGram}{\texttt{Char}} %DIF > 
\newcommand{\StringGram}{\texttt{String}} %DIF > 
\newcommand{\stringNL}{String'} %DIF > 
%DIF -------
\newcommand{\Set}{\mathbf{Set}}
%DIF 96a134
\newcommand{\hSet}{\mathbf{hSet}} %DIF > 
%DIF -------
\newcommand{\Syn}{\mathbf{Synx}}
\newcommand{\SemAct}{\mathbf{SemAct}}
\newcommand{\Gr}{\mathbf{Gr}}
\newcommand{\Grammar}{\mathbf{Gr}}
\newcommand{\semcat}{\mathbf{C}}
\newcommand{\Type}{\mathbf{Type}}
\newcommand{\Prop}{\mathbf{Prop}}
\newcommand{\Bool}{\mathtt{Bool}}
\newcommand{\true}{\mathtt{true}}
\newcommand{\false}{\mathtt{false}}
\newcommand{\nat}{\mathbb{N}}
\newcommand{\theoryname}{Dependent Lambek Calculus\xspace}
%DIF 108c147
%DIF < \newcommand{\theoryabbv}{$\textrm{Lambek}^D$~}
%DIF -------
\newcommand{\theoryabbv}{$\textrm{Lambek}^D$\xspace} %DIF > 
%DIF -------
\newcommand{\lnld}{$\textrm{LNL}_D$}

\newcommand{\isTy}{\textrm{ type}}
\newcommand{\isCtx}{\textrm{ ctx}}
\newcommand{\isSmall}{\textrm{ small}}
\newcommand{\isLinTy}{\textrm{ lin. type}}
%DIF 115a154
\newcommand{\isSmallLin}{\textrm{ small lin.}} %DIF > 
%DIF -------
\newcommand{\isLinCtx}{\textrm{ lin. ctx.}}

\newcommand{\quoteTy}[1]{\lceil{#1}\rceil}
\newcommand{\unquoteTy}[1]{\lfloor{#1}\rfloor}

\newcommand{\gluedNL}{{\mathcal G}_S}
\newcommand{\gluedNLUniv}{{\mathcal G}_{S,i}}
\newcommand{\gluedL}{{\mathcal G}_L}

\newcommand{\amp}{\mathrel{\&}}
\newcommand{\pair}{\amp}
\DeclareMathOperator*{\bigamp}{\scalerel*{\&}{\bigoplus}}
\DeclareMathOperator*{\bigwith}{\scalerel*{\&}{\bigoplus}}


\newcommand{\bang}{~\textbf{!}~}
% Tentatively using uparrow for linear to non-linear to be in line with
% Pfennings adjoint functional programming
\newcommand{\ltonl}[1]{~\uparrow #1}
\newcommand{\nil}{\texttt{nil}}
\newcommand{\ident}{\texttt{id}}
\newcommand{\cons}{\texttt{cons}}
\newcommand{\epscons}{\varepsilon\texttt{cons}}
\newcommand{\data}{\mathsf{data}}
\newcommand{\where}{\mathsf{where}}
\newcommand{\Trace}{\texttt{Trace}}
\newcommand{\literal}[1]{\texttt{\textquotesingle#1\textquotesingle}}
\newcommand{\stringquote}[1]{\texttt{\textquotedbl#1\textquotedbl}}
\newcommand{\internalize}[1]{\lceil#1\rceil}
%DIF 144a184
\newcommand{\listTy}[1]{List\left( #1 \right)} %DIF > 
%DIF -------

\newcommand{\oplusinj}[2]{\sigma\,#1\,#2}
\newcommand{\withprj}[2]{\pi\,#1\,#2}

\newcommand{\simulsubst}[2]{#1\{#2\}}
\newcommand{\subst}[3]{\simulsubst {#1} {#2/#3}}
\newcommand{\el}{\mathsf{el}}
\newcommand{\letin}[3]{\mathsf{let}\, #1 = #2 \, \mathsf{in}\, #3}
\newcommand{\lamb}[2]{\lambda #1.\, #2}
\newcommand{\lamblto}[2]{\lambda^{{\lto}} #1.\, #2}
\newcommand{\lambtol}[2]{\lambda^{{\tol}} #1.\, #2}
\newcommand{\dlamb}[2]{{\lambda}^{{\&}} #1.\, #2}
\newcommand{\withlamb}[2]{{\lambda}^{{\&}} #1.\, #2}
\newcommand{\app}[2]{#1 \, #2}
\newcommand{\applto}[2]{#1 \, #2}
\newcommand{\apptol}[2]{#1 \mathop{{}^{\tol}} #2}
%DIF 160-162c201-205
%DIF < \newcommand{\PiTy}[3]{\textstyle\prod (#1 : #2). #3}
%DIF < \newcommand{\SigTy}[3]{\textstyle\sum (#1 : #2). #3}
%DIF < \newcommand{\LinPiTy}[3]{\textstyle\bigamp (#1 : #2). #3}
%DIF -------
\newcommand{\PiTy}[3]{\textstyle\prod_{#1 : #2} #3} %DIF > 
\newcommand{\SigTy}[3]{\textstyle\sum_{#1 : #2} #3} %DIF > 
\newcommand{\PiTyLimit}[3]{\textstyle\prod\limits_{#1 : #2} #3} %DIF > 
\newcommand{\SigTyLimit}[3]{\textstyle\sum\limits_{#1 : #2} #3} %DIF > 
\newcommand{\LinPiTy}[3]{\textstyle\bigamp_{#1 : #2} #3} %DIF > 
%DIF -------
\newcommand{\LinPiTyLimit}[3]{\bigwith\limits_{#1 : #2} #3}
%DIF 164c207
%DIF < \newcommand{\LinSigTy}[3]{\textstyle\bigoplus (#1 : #2). #3}
%DIF -------
\newcommand{\LinSigTy}[3]{\textstyle\bigoplus_{#1 : #2} #3} %DIF > 
%DIF -------
\newcommand{\LinSigTyLimit}[3]{\bigoplus\limits_{#1 : #2} #3}
%% \newcommand{\DepWith}[2]{{\textstyle\bigamp}\limits_{#1}{#2}
%% \newcommand{\DepPlus}[2]{{\textstyle\bigoplus}\limits_{#1}{#2}
\newcommand{\GrTy}{\mathsf{Gr}}

\newcommand{\equalizer}[3]{\{#1\,|\,\applto {#2}{#1} = \applto{#3}{#1} \}}
\newcommand{\equalizerin}[1]{\langle #1 \rangle}
\newcommand{\equalizerpi}[1]{#1.\pi}

\newcommand{\ctxwff}[1]{#1 \isCtx}
\newcommand{\ctxwffjdg}[2]{#1 \vdash #2 \isTy}
\newcommand{\linctxwff}[2]{#1 \vdash #2 \isLinCtx}
\newcommand{\linctxwffjdg}[2]{#1 \vdash #2 \isLinTy}
%DIF 178a221-224
\newcommand{\nonlinterm}[3]{#1 \vdash #2 : #3} %DIF > 
\newcommand{\linterm}[4]{#1 ; #2 \vdash #3 : #4} %DIF > 
\newcommand{\nonlineq}[4]{#1 \vdash #2 \equiv #3 : #4} %DIF > 
\newcommand{\lineq}[5]{#1 ; #2 \vdash #3 \equiv #4 : #5} %DIF > 
%DIF -------

%DIF 179a226-232
\newcommand{\LLL}{\textrm{LL}} %DIF > 
\newcommand{\LRR}{\textrm{LR}} %DIF > 
\newcommand{\LALRR}{\textrm{LALR}} %DIF > 
\newcommand{\LL}[1]{\LLL(#1)} %DIF > 
\newcommand{\LR}[1]{\LRR(#1)} %DIF > 
\newcommand{\LALR}[1]{\LALRR(#1)} %DIF > 
 %DIF > 
%DIF -------
\newsavebox{\logoagdabox}
\sbox{\logoagdabox}{%
  %
  \raisebox{-2pt}{\includegraphics[height=1em]{logo-agda.pdf}}%
}
\newcommand{\agdalogo}{%
  \usebox{\logoagdabox}}%

%DIF 187a241-243
\newcommand{\fillerlink}{http://localhost:1313} %DIF > 
\newcommand{\Agda}[1]{\href{#1}{\agdalogo}} %DIF > 
 %DIF > 
%DIF -------
\newif\ifdraft
\draftfalse
\newcommand{\steven}[1]{\ifdraft{\color{orange}[{\bf Steven says}: #1]}\fi}
\renewcommand{\max}[1]{\ifdraft{\color{blue}[{\bf Max says}: #1]}\fi}
\newcommand{\pedro}[1]{\ifdraft{\color{red}[{\bf Pedro says}: #1]}\fi}
\newcommand{\nathan}[1]{\ifdraft{\color{green}[{\bf Nathan says}: #1]}\fi}
\newcommand{\pipe}{\,|\,}
%DIF 194a251-288
 %DIF > 
\newcommand{\states}{\mathtt{states}} %DIF > 
\newcommand{\labelt}{label} %DIF > 
\newcommand{\transitions}{\texttt{transitions}} %DIF > 
\newcommand{\epstransitions}{\epsilon\texttt{transitions}} %DIF > 
\newcommand{\isAcc}{\mathtt{isAcc}} %DIF > 
\newcommand{\init}{\mathtt{init}} %DIF > 
\newcommand{\src}{\mathtt{src}} %DIF > 
\newcommand{\dst}{\mathtt{dst}} %DIF > 
\newcommand{\pparse}{\mathtt{parse}} %DIF > 
\newcommand{\print}{\mathtt{print}} %DIF > 
\newcommand{\epssrc}{\epsilon\mathtt{src}} %DIF > 
\newcommand{\epsdst}{\epsilon\mathtt{dst}} %DIF > 
\newcommand{\balanced}{\mathtt{balanced}} %DIF > 
\newcommand{\Dyck}{\mathtt{Dyck}} %DIF > 
\newcommand{\N}{\texttt{N}} %DIF > 
 %DIF > 
% These declarations are to get cleveref to play nice with %DIF > 
% different theorem-like environemnts. Otherwise \cref{lem1.1} %DIF > 
% will display 'Theorem 1.1' %DIF > 
 %DIF > 
\declaretheoremstyle[headfont=\normalfont\itshape]{defstyle} %DIF > 
\declaretheoremstyle[headfont=\normalfont\scshape]{thmstyle} %DIF > 
 %DIF > 
\declaretheorem[name=Definition, style=defstyle, numberwithin=section]{definition} %DIF > 
\declaretheorem[style=thmstyle, name=Theorem, numberlike=definition]{theorem} %DIF > 
\declaretheorem[style=thmstyle, name=Axiom, numberlike=definition]{axiom} %DIF > 
\declaretheorem[style=thmstyle, name=Corollary, numberlike=definition]{corollary} %DIF > 
\declaretheorem[style=thmstyle, name=Construction, numberlike=definition]{construction} %DIF > 
 %DIF > 
\crefname{definition}{Definition}{Definitions} %DIF > 
\crefname{theorem}{Theorem}{Theorems} %DIF > 
\crefname{axiom}{Axiom}{Axioms} %DIF > 
\crefname{corollary}{Corollary}{Corollarys} %DIF > 
\crefname{construction}{Construction}{Constructions} %DIF > 
 %DIF > 
% \newtheorem{axiom}{Axiom}[section] %DIF > 
% \newtheorem{construction}{Construction} %DIF > 
%DIF -------

\newif\ifappendix
\appendixtrue
%DIF PREAMBLE EXTENSION ADDED BY LATEXDIFF
%DIF UNDERLINE PREAMBLE %DIF PREAMBLE
\RequirePackage[normalem]{ulem} %DIF PREAMBLE
\RequirePackage{color}\definecolor{RED}{rgb}{1,0,0}\definecolor{BLUE}{rgb}{0,0,1} %DIF PREAMBLE
\providecommand{\DIFaddtex}[1]{{\protect\color{blue}\uwave{#1}}} %DIF PREAMBLE
\providecommand{\DIFdeltex}[1]{{\protect\color{red}\sout{#1}}} %DIF PREAMBLE
%DIF SAFE PREAMBLE %DIF PREAMBLE
\providecommand{\DIFaddbegin}{} %DIF PREAMBLE
\providecommand{\DIFaddend}{} %DIF PREAMBLE
\providecommand{\DIFdelbegin}{} %DIF PREAMBLE
\providecommand{\DIFdelend}{} %DIF PREAMBLE
\providecommand{\DIFmodbegin}{} %DIF PREAMBLE
\providecommand{\DIFmodend}{} %DIF PREAMBLE
%DIF FLOATSAFE PREAMBLE %DIF PREAMBLE
\providecommand{\DIFaddFL}[1]{\DIFadd{#1}} %DIF PREAMBLE
\providecommand{\DIFdelFL}[1]{\DIFdel{#1}} %DIF PREAMBLE
\providecommand{\DIFaddbeginFL}{} %DIF PREAMBLE
\providecommand{\DIFaddendFL}{} %DIF PREAMBLE
\providecommand{\DIFdelbeginFL}{} %DIF PREAMBLE
\providecommand{\DIFdelendFL}{} %DIF PREAMBLE
%DIF HYPERREF PREAMBLE %DIF PREAMBLE
\providecommand{\DIFadd}[1]{\texorpdfstring{\DIFaddtex{#1}}{#1}} %DIF PREAMBLE
\providecommand{\DIFdel}[1]{\texorpdfstring{\DIFdeltex{#1}}{}} %DIF PREAMBLE
\newcommand{\DIFscaledelfig}{0.5}
%DIF HIGHLIGHTGRAPHICS PREAMBLE %DIF PREAMBLE
\RequirePackage{settobox} %DIF PREAMBLE
\RequirePackage{letltxmacro} %DIF PREAMBLE
\newsavebox{\DIFdelgraphicsbox} %DIF PREAMBLE
\newlength{\DIFdelgraphicswidth} %DIF PREAMBLE
\newlength{\DIFdelgraphicsheight} %DIF PREAMBLE
% store original definition of \includegraphics %DIF PREAMBLE
\LetLtxMacro{\DIFOincludegraphics}{\includegraphics} %DIF PREAMBLE
\newcommand{\DIFaddincludegraphics}[2][]{{\color{blue}\fbox{\DIFOincludegraphics[#1]{#2}}}} %DIF PREAMBLE
\newcommand{\DIFdelincludegraphics}[2][]{% %DIF PREAMBLE
\sbox{\DIFdelgraphicsbox}{\DIFOincludegraphics[#1]{#2}}% %DIF PREAMBLE
\settoboxwidth{\DIFdelgraphicswidth}{\DIFdelgraphicsbox} %DIF PREAMBLE
\settoboxtotalheight{\DIFdelgraphicsheight}{\DIFdelgraphicsbox} %DIF PREAMBLE
\scalebox{\DIFscaledelfig}{% %DIF PREAMBLE
\parbox[b]{\DIFdelgraphicswidth}{\usebox{\DIFdelgraphicsbox}\\[-\baselineskip] \rule{\DIFdelgraphicswidth}{0em}}\llap{\resizebox{\DIFdelgraphicswidth}{\DIFdelgraphicsheight}{% %DIF PREAMBLE
\setlength{\unitlength}{\DIFdelgraphicswidth}% %DIF PREAMBLE
\begin{picture}(1,1)% %DIF PREAMBLE
\thicklines\linethickness{2pt} %DIF PREAMBLE
{\color[rgb]{1,0,0}\put(0,0){\framebox(1,1){}}}% %DIF PREAMBLE
{\color[rgb]{1,0,0}\put(0,0){\line( 1,1){1}}}% %DIF PREAMBLE
{\color[rgb]{1,0,0}\put(0,1){\line(1,-1){1}}}% %DIF PREAMBLE
\end{picture}% %DIF PREAMBLE
}\hspace*{3pt}}} %DIF PREAMBLE
} %DIF PREAMBLE
\LetLtxMacro{\DIFOaddbegin}{\DIFaddbegin} %DIF PREAMBLE
\LetLtxMacro{\DIFOaddend}{\DIFaddend} %DIF PREAMBLE
\LetLtxMacro{\DIFOdelbegin}{\DIFdelbegin} %DIF PREAMBLE
\LetLtxMacro{\DIFOdelend}{\DIFdelend} %DIF PREAMBLE
\DeclareRobustCommand{\DIFaddbegin}{\DIFOaddbegin \let\includegraphics\DIFaddincludegraphics} %DIF PREAMBLE
\DeclareRobustCommand{\DIFaddend}{\DIFOaddend \let\includegraphics\DIFOincludegraphics} %DIF PREAMBLE
\DeclareRobustCommand{\DIFdelbegin}{\DIFOdelbegin \let\includegraphics\DIFdelincludegraphics} %DIF PREAMBLE
\DeclareRobustCommand{\DIFdelend}{\DIFOaddend \let\includegraphics\DIFOincludegraphics} %DIF PREAMBLE
\LetLtxMacro{\DIFOaddbeginFL}{\DIFaddbeginFL} %DIF PREAMBLE
\LetLtxMacro{\DIFOaddendFL}{\DIFaddendFL} %DIF PREAMBLE
\LetLtxMacro{\DIFOdelbeginFL}{\DIFdelbeginFL} %DIF PREAMBLE
\LetLtxMacro{\DIFOdelendFL}{\DIFdelendFL} %DIF PREAMBLE
\DeclareRobustCommand{\DIFaddbeginFL}{\DIFOaddbeginFL \let\includegraphics\DIFaddincludegraphics} %DIF PREAMBLE
\DeclareRobustCommand{\DIFaddendFL}{\DIFOaddendFL \let\includegraphics\DIFOincludegraphics} %DIF PREAMBLE
\DeclareRobustCommand{\DIFdelbeginFL}{\DIFOdelbeginFL \let\includegraphics\DIFdelincludegraphics} %DIF PREAMBLE
\DeclareRobustCommand{\DIFdelendFL}{\DIFOaddendFL \let\includegraphics\DIFOincludegraphics} %DIF PREAMBLE
%DIF COLORLISTINGS PREAMBLE %DIF PREAMBLE
\RequirePackage{listings} %DIF PREAMBLE
\RequirePackage{color} %DIF PREAMBLE
\lstdefinelanguage{DIFcode}{ %DIF PREAMBLE
%DIF DIFCODE_UNDERLINE %DIF PREAMBLE
  moredelim=[il][\color{red}\sout]{\%DIF\ <\ }, %DIF PREAMBLE
  moredelim=[il][\color{blue}\uwave]{\%DIF\ >\ } %DIF PREAMBLE
} %DIF PREAMBLE
\lstdefinestyle{DIFverbatimstyle}{ %DIF PREAMBLE
	language=DIFcode, %DIF PREAMBLE
	basicstyle=\ttfamily, %DIF PREAMBLE
	columns=fullflexible, %DIF PREAMBLE
	keepspaces=true %DIF PREAMBLE
} %DIF PREAMBLE
\lstnewenvironment{DIFverbatim}{\lstset{style=DIFverbatimstyle}}{} %DIF PREAMBLE
\lstnewenvironment{DIFverbatim*}{\lstset{style=DIFverbatimstyle,showspaces=true}}{} %DIF PREAMBLE
\lstset{extendedchars=\true,inputencoding=utf8}

%DIF END PREAMBLE EXTENSION ADDED BY LATEXDIFF

\begin{document}

%\pagestyle{plain}

\pagebreak

\title{Intrinsic Verification of Parsers and Formal Grammar Theory in Dependent Lambek Calculus}

\author{Steven Schaefer}
\affiliation{\department{Electrical Engineering and Computer Science}
  \institution{University of Michigan}
  \country{USA}
}
\email{stschaef@umich.edu}

\author{Nathan Varner}
\affiliation{\department{Electrical Engineering and Computer Science}
  \institution{University of Michigan}
  \country{USA}
}
\email{nmvarner@umich.edu}

\author{Pedro H. Azevedo de Amorim}
\affiliation{
  \department{Department of Computer Science}
  \institution{University of Oxford}
  \country{UK}
}
\email{pedro.azevedo.de.amorim@cs.ox.ac.uk}

\author{Max S. New}
\affiliation{
  \department{Electrical Engineering and Computer Science}
  \institution{University of Michigan}
  \country{USA}
}
\email{maxsnew@umich.edu}

\makeatletter
\let\@authorsaddresses\@empty
\makeatother

\begin{abstract}
  We present \theoryname~(\theoryabbv), a domain-specific dependent
  type theory for verified parsing and formal grammar theory. In
  \theoryabbv, linear types are used as a syntax for formal grammars,
  and parsers can be written as linear terms. The linear typing
  restriction provides a form of intrinsic verification that a parser
  yields only valid parse trees for the input string. We demonstrate
  the expressivity of this system by showing that the combination of
  inductive linear types and dependency on non-linear data can be used
  to encode commonly used grammar formalisms such as regular and
  context-free grammars as well as traces of various types of
  automata. Using these encodings, we define parsers for regular
  expressions using deterministic automata, as well as
  examples of verified parsers of context-free grammars.

  We present a denotational semantics of our type theory that
  interprets the types as a mathematical notion of formal
  grammars. Based on this denotational semantics, we have made a
  prototype implementation of \theoryabbv using a shallow embedding in
  the Agda proof assistant. All of our examples parsers have been
  implemented in this prototype implementation.
\end{abstract}

\maketitle

\section{Introduction}
\label{sec:intro}
Parsing structured data from untrusted input is a ubiquitous task in
computing. Any formally verified software system that interacts with
the outside world must contain some parsing component. For example, in
an extensive experiment finding bugs in C compilers
\cite{yangFindingUnderstandingBugs}, an early version of the formally
verified CompCert C compiler only contained bugs in the then
unverified parsing component \cite{leroy_formal_2009}. Bugs in parsers
undermine the overall correctness theorem for a verified system: an
incorrectly parsed C program will be compiled correctly but this is
not very useful if it did not correctly correspond to the actual
source program. Eventually, a correct parser was implemented using an
automaton that is formally verified to implement an \DIFdelbegin \DIFdel{LR }\DIFdelend \DIFaddbegin \DIFadd{$\LR{1}$ }\DIFaddend grammar
\cite{jourdanValidatingLRParsers2012}.

It is entirely understandable from an engineering perspective
\emph{why} verified parsing was not part of the initial releases of
CompCert: parsing algorithms and formal grammars are a complex area,
featuring a variety of domain-specific formalisms such as context-free
grammars and various automata. These formalisms have little relation
to the main components of a verified compiler. For this reason, it is
advantageous for verified parsers to be implemented using a reusable
verified library, just as parser generators and regular expression
matchers have done for many decades in unverified software.

Prior approaches to verified parsing focus on verification of a
particular grammar formalism such as non-left-recursive grammars or
\DIFdelbegin \DIFdel{LL(1) }\DIFdelend \DIFaddbegin \DIFadd{$\LL{1}$ }\DIFaddend grammars
\cite{lasserCoStarVerifiedALL2021,EdelmannZippy2020,danielssonTotalParserCombinators2010}.
Each new grammar formalism is extended with its own independent
verified implementation.

In this work, we present the design of \theoryname (\theoryabbv), a
domain-specific language for formal verification of parsers. A key
property is that \theoryabbv is an \emph{extensible} framework for
verification of parsers in that it supports the definition of grammar
formalisms of unrestricted complexity. That is, \theoryabbv is not a
system for verifying \emph{one} type of grammar formalism, but instead
is a domain-specific language in which many grammar formalisms and
their verified parsers can be implemented. For example, \theoryabbv is
not a verified parser generator that compiles regular expressions to
deterministic finite automata, but is instead \DIFdelbegin \DIFdel{is }\DIFdelend a domain specific
language \emph{for writing} such a verified parser generator.

The design of \theoryabbv
is an extension of Joachim Lambek's \emph{syntactic calculus}
\cite{lambek58}. Lambek calculus is a grammar formalism equivalent
in expressive power to context-free grammars that in modern
terminology would be considered a kind of \emph{non-commutative linear
logic} --- a version of linear logic where the tensor product is not
commutative, reflecting the obvious property that the relative
ordering of characters is significant in parsing problems. We extend
non-commutative linear logic with two key components that increase its
power to support arbitrarily powerful grammar formalisms: inductive
linear\DIFaddbegin \footnote{\DIFadd{Throughout, we shall use ``linear types'' to refer to the
non-commutative linear types.}} \DIFaddend types, as well as dependency of linear types on non-linear
data. The resulting system has two kinds of types: non-linear types
which model sets and linear types which model formal
grammars. Crucially, the non-linear types and linear types are allowed
to be dependent on non-linear types, but not on linear types. This
combination has been used previously in the ``linear-non-linear
dependent'' type theory with \emph{commutative} linear logic to model
imperative programming \cite{krishnaswami_integrating_2015}.

The substructural nature of \theoryabbv is well-aligned with the
requirements intrinsic to parsing and to the theory of formal
languages, where strings constitute a very clear notion of resource
that cannot be duplicated, reordered, or dropped. Moreover, the constructive
aspect of \theoryabbv ensures that verification of parsers written in the
calculus are \emph{correct-by-construction}. The type system is powerful enough
that derivations of a term type checking carry intrinsic proofs of correctness.
Parsers written in \theoryabbv take on a linear functional style, which makes
them familiar to write and amenable to compositional verification techniques.

To show the feasibility of our design, we have implemented \theoryabbv
as a shallowly embedded domain-specific language in the Cubical Agda
proof assistant \cite{VezzosiMortbergAbel2019}. We have implemented many example
grammars and parsers in our system including regular expressions,
non-deterministic and deterministic automata, as well as some example
context-free grammars and parsers based on \DIFdelbegin \DIFdel{LL(1) and LR(1)
}\DIFdelend \DIFaddbegin \DIFadd{$\LL{1}$ and $\LR{1}$
}\DIFaddend automata. Throughout this paper, \DIFdelbegin \DIFdel{we will use }\DIFdelend \agdalogo~ \DIFdelbegin \DIFdel{to }\DIFdelend \DIFaddbegin \DIFadd{will }\DIFaddend mark
results that are mechanized in our Agda development \DIFaddbegin \DIFadd{and provide a link to their
implementation}\DIFaddend .

Our Agda prototype is based on a \emph{denotational semantics} of
\theoryabbv. The core idea of the denotational semantics \DIFdelbegin \DIFdel{is
\mbox{%DIFAUXCMD
\cite{elliottSymbolicAutomaticDifferentiation2021}}\hskip0pt%DIFAUXCMD
, Elliottdescribes
}\DIFdelend \DIFaddbegin \DIFadd{stems from an observation of Elliott:
%DIF >  The core idea of the denotational semantics is
%DIF >  \cite{elliottSymbolicAutomaticDifferentiation2021}, Elliott describes
}\DIFaddend a formal grammar \DIFdelbegin \DIFdel{as }\DIFdelend \DIFaddbegin \DIFadd{is a }\DIFaddend type-level \DIFdelbegin \DIFdel{predicates }\DIFdelend \DIFaddbegin \DIFadd{predicate }\DIFaddend on strings that \DIFdelbegin \DIFdel{prove
language membership }\DIFdelend \DIFaddbegin \DIFadd{proves
language membership \mbox{%DIFAUXCMD
\cite{elliottSymbolicAutomaticDifferentiation2021}}\hskip0pt%DIFAUXCMD
}\DIFaddend . That is, a \emph{formal grammar} $A$ is a
function \DIFdelbegin \DIFdel{$\String \to \Set$ }\DIFdelend \DIFaddbegin \DIFadd{$\StringSem \to \Set$ }\DIFaddend such that for a string $w$, $A~w$ is the
set of ``proofs'' showing that $w$ belongs to the language recognized
by $A$. We show that all linear types in \theoryabbv can be so
interpreted as an abstract formal grammar in this sense, and that
linear terms are a kind of \emph{parse transformer}, a function that
takes a parse tree from one grammar to a parse tree in a different
grammar but over the same underlying string.

%% Formal verification encompasses a broad suite of techniques one may use to
%% mathematically guarantee the correctness of software systems. We can only
%% reap the benefits of
%% any proof effort if its underlying assumptions hold. For instance, verifying the
%% functional correctness of an algorithm is no use if it is run on buggy hardware.
%% Likewise, the promised guarantees of a program may be jeopardized if it is
%% parsed incorrectly.

Our contributions are then:% \max{This contribution section needs to be punchier}
%
\begin{itemize}
  \item The design of \theoryname (\theoryabbv): A dependent
    linear-non-linear type theory for building verified parsers, which
    extends prior work on dependent linear-non-linear type theory to
    support inductive linear types\DIFaddbegin \DIFadd{.
  }\DIFaddend \item \DIFdelbegin \DIFdel{Demonstration }\DIFdelend \DIFaddbegin \DIFadd{A demonstration }\DIFaddend of how to encode many common grammar formalisms
    (regular expressions, (non-)deterministic automata, context-free
    grammars) and parser formalisms within our type theory.
  \item A prototype implementation of \theoryabbv in Agda with all
    examples mechanized.
  \item A denotational semantics for \theoryabbv that shows that the
    parsers are in fact verified to be correct\DIFdelbegin \DIFdel{and }\DIFdelend \DIFaddbegin \DIFadd{, and the }\DIFaddend soundness of the
    equational theory.
\end{itemize}

This paper begins in \cref{sec:type-theory-examples} by studying small
example programs from \theoryabbv to build intuition.  From there, in
\cref{sec:tt} we provide the syntax, typing\DIFaddbegin \DIFadd{, }\DIFaddend and equational theory of
\theoryname. In \cref{sec:applications} we demonstrate the
applicability of \theoryabbv for relating familiar grammar and
automata formalisms as well as building concrete parsers.  Then in
\cref{sec:semantics-and-metatheory}, we give a denotational semantics
that makes precise the connection between \theoryabbv syntax and
formal grammars. Finally in \Cref{sec:discussion} we discuss related
and future work.

\section{\theoryname by Example}
\label{sec:type-theory-examples}
To gain intuition for working in \theoryabbv, we begin with some
illustrative examples drawn from the theory of formal languages. Each
of our examples will be defined for strings over the three character
alphabet $\Sigma = \{ \texttt{a} , \texttt{b}, \texttt{c} \}$.

\newcommand{\A}{\texttt{A}}
\newcommand{\B}{\texttt{B}}
\newcommand{\I}{\texttt{I}}
\newcommand{\f}{\texttt{f}}
\newcommand{\g}{\texttt{g}}
\renewcommand{\L}{\texttt{L}}
\renewcommand{\a}{\texttt{a}}
\renewcommand{\b}{\texttt{b}}
\renewcommand{\c}{\texttt{c}}
\newcommand{\w}{\texttt{w}}

\paragraph{Finite Grammars}
First consider finite grammars --- those built from base types via disjunctions and
concatenations. The base types comprise characters drawn from the alphabet, the
empty string, and the empty grammar.
For each character $a$ in the alphabet we have a type $\literal a$ which
has a single parse tree for the string
\stringquote{a} and no parse trees at any other strings. The grammar $\I$ has a single
parse tree for the empty string $\epsilon = \stringquote{}$ and no parses for any other strings.
The final base type, the empty grammar $0$, has no parses for any string. We
use type-theoretic syntax to represent disjunction $\oplus$ and concatenation
$\otimes$ of
grammars. Over an input string $\w$, a parse of the disjunction $\A \oplus \B$ is either
a parse of $\A$ over the string $\w$ or a
parse of $\B$ over the string $\w$. Similarly, \DIFdelbegin \DIFdel{$\w$ matches }\DIFdelend \DIFaddbegin \DIFadd{a parse of }\DIFaddend $\A \otimes \B$ \DIFdelbegin \DIFdel{if
}\DIFdelend \DIFaddbegin \DIFadd{for
}\DIFaddend $\w$ \DIFdelbegin \DIFdel{can be split }\DIFdelend \DIFaddbegin \DIFadd{is a splitting of $w$ }\DIFaddend into two strings $\w_{\A}$ and $\w_{\B}$ \DIFdelbegin \DIFdel{that match }\DIFdelend \DIFaddbegin \DIFadd{with
parses for }\DIFaddend $\A$
and $\B$, respectively.

\DIFdelbegin \DIFdel{For a type $\A$,
a parse tree of a string $\w$ is represented as a
term of type $\A$ in the context $\internalize \w$, }\DIFdelend \DIFaddbegin \DIFadd{A grammar $A$ derives the word $w$ if there exists a derivation
$\internalize{w} \vdash a : A$,
}\DIFaddend where $\internalize \w$ is a context with one variable for each character of
$\w$. \DIFaddbegin \DIFadd{The term $a : A$ represents a }\emph{\DIFadd{parse tree}} \DIFadd{of $w$ for the grammar $A$. }\DIFaddend For
example, to define a parse tree for \stringquote{ab}, we use
the context $\internalize{\stringquote{ab}} = a : \literal a , b :
\literal b$. In \Cref{fig:fingram}, we give a lambda term and its
typing derivation to define a parse for a finite grammar.

\begin{figure}
\DIFmodbegin
\begin{lstlisting}[alsolanguage=DIFcode]
f : (*@$\uparrow$@*)('a' (*@$\otimes$@*) 'b' (*@$\lto$@*) ('a' (*@$\otimes$@*) 'b') (*@$\oplus$@*) 'c')
%DIF < f (a , b) = inl (a (*@$\otimes$@*) b)
%DIF > f (a , b) = inl (a , b)
\end{lstlisting}
\DIFmodend\begin{mathpar}
  \DIFdelbeginFL %DIFDELCMD < \inferrule
%DIFDELCMD <   {
%DIFDELCMD <     \inferrule
%DIFDELCMD <     {
%DIFDELCMD <       \inferrule
%DIFDELCMD <       {~}
%DIFDELCMD <       {a : \literal a \vdash a : \literal a}
%DIFDELCMD <       \\
%DIFDELCMD <       \inferrule
%DIFDELCMD <       {~}
%DIFDELCMD <       {b : \literal b \vdash b : \literal b}
%DIFDELCMD <     }
%DIFDELCMD <     {a : \literal a , b : \literal b \vdash a \otimes b : \literal a \otimes
%DIFDELCMD <       \literal b}
%DIFDELCMD <   }
%DIFDELCMD <   {a : \literal a , b : \literal b \vdash \texttt{f} := \inl(a \otimes b) :
%DIFDELCMD <     (\literal a \otimes \literal b) \oplus \literal c}
%DIFDELCMD < %%%
\DIFdelendFL \DIFaddbeginFL \footnotesize
  \inferrule
  {
    \inferrule
    {
      \inferrule
      {~}
      {a : \literal a \vdash a : \literal a}
      \\
      \inferrule
      {~}
      {b : \literal b \vdash b : \literal b}
    }
    {a : \literal a , b : \literal b \vdash (a , b) : \literal a \otimes
      \literal b}
  }
  {a : \literal a , b : \literal b \vdash \texttt{f} := \inl(a , b) :
    (\literal a \otimes \literal b) \oplus \literal c}
\DIFaddendFL \end{mathpar}
\caption{\DIFdelbeginFL \DIFdelFL{\stringquote{ac} matches
  }\DIFdelendFL \DIFaddbeginFL \DIFaddFL{(}\Agda{fillerlink}\DIFaddFL{) \stringquote{ab} is parsed by }\DIFaddendFL $(\literal a \oplus \literal b) \otimes \literal c$
}
\label{fig:fingram}
\end{figure}
For this interpretation of parse trees as terms to make sense, our
calculus cannot allow for \emph{any} of the usual structural rules of
type theory: weakening, contraction and exchange. Weakening allows
for variables to go unused, while contraction allows for the same
variable to be used twice, but in a parse tree, every character must
be accounted for exactly once. That is, we want to prevent the following
erroneous derivations,
\DIFdelbegin \begin{displaymath}%DIFAUXCMD
%DIFDELCMD < \[
%DIFDELCMD <   %%%
\DIFdel{a : \literal a , b : \literal b }%DIFDELCMD < \not %%%
\DIFdel{\vdash a : \literal a \qquad a : \literal a , b : \literal b }%DIFDELCMD < \not %%%
\DIFdel{\vdash (a, a) : \literal a \otimes \literal a
}
\end{displaymath}%DIFAUXCMD
%DIFDELCMD < \]
%DIFDELCMD < %%%
\DIFdelend \DIFaddbegin \DIFadd{\(
{a : \literal a , b : \literal b \not \vdash a : \literal a}\) and
\({a : \literal a , b : \literal b \not \vdash (a, a) : \literal a \otimes \literal a}
\).
}\DIFaddend 

%DIF < 
Finally, the ordering of characters in a string cannot be ignored while
parsing, so we omit the exchange rule because it would allow
for variables in the context to be reordered\DIFdelbegin \DIFdel{,
}\begin{displaymath}%DIFAUXCMD
%DIFDELCMD < \[
%DIFDELCMD <   %%%
\DIFdel{a : \literal a ,
b : \literal b }%DIFDELCMD < \not %%%
\DIFdel{\vdash (b , a) : \literal b \otimes \literal a
}
\end{displaymath}%DIFAUXCMD
%DIFDELCMD < \]
%DIFDELCMD < %%%
\DIFdelend \DIFaddbegin \DIFadd{. Likewise, we prevent the following derivation,
\(
  {a : \literal a , b : \literal b \not \vdash (b , a) : \literal b \otimes \literal a}
\).
}\DIFaddend % otherwise we would have that $\inl(x\otimes y)$ is a valid
% term in context $y:b,x:a$, but there should be no parses of $ba$ in
% this grammar.

\paragraph{Regular Expressions}
Regular expressions can be encoded as types generated by base types,
$\oplus$, and $\otimes$, and the Kleene star $(\cdot)^{\ast}$.  For a
grammar $\A$, we define the Kleene star $\A^{*}$ as a particular
\emph{inductive linear type} of linear lists, as shown in
\cref{fig:kleenestarinductive}. Here $\A^{*} : \L$ means we are defining
a \emph{linear} type. $\A^{*}$ has two constructors: $\nil$, which
builds a parse of type $\A^{*}$ from nothing; and $\cons$, which
linearly consumes a parse of $\A$ and a parse of $\A^{*}$ and builds a
parse of $\A^{*}$. This linear consumption is defined by the linear
function type $\lto$. The linear function type $\A \lto \B$ defines functions that
take in parses of $\A$ as input, \emph{consume} the input, and return a parse of
$\B$ as output. The arrow, $\uparrow$, wrapping these
constructors means that the constructors \DIFdelbegin \DIFdel{by themselves }\DIFdelend are not consumed
upon usage, and so are \emph{non-linear} values themselves. That is, the
names $\nil$ and $\cons$ are function symbols that may be reused as many times
as we wish.

\begin{figure}
\begin{lstlisting}
data A(*@$^*$@*) : L where
  nil : (*@$\uparrow$@*)(A(*@$^*$@*))
  cons : (*@$\uparrow$@*)(A (*@$\lto$@*) A(*@$^*$@*) (*@$\lto$@*) A(*@$^*$@*))
\end{lstlisting}
%DIF <  \begin{align*}
%DIF <  \data &~A^{*} : L~\where\\
%DIF <        & \nil : \ltonl {A^{*}} \\
%DIF <        & \cons : \ltonl {(A \lto A^{*} \lto A^{*})}
%DIF <  \end{align*}
\caption{Kleene Star as an inductive type}
\label{fig:kleenestarinductive}
\end{figure}

Through repeated application of the Kleene star constructors,
\cref{fig:kleenestarderivation} gives a derivation that shows
\stringquote{ab} \DIFdelbegin \DIFdel{matches }\DIFdelend \DIFaddbegin \DIFadd{is parsed by }\DIFaddend the regular expression $({\literal a}^{*}
\otimes \literal b) \oplus \literal c$. The leaves of the proof tree
that mention the arrow $\uparrow$ describe a cast from a non-linear
type to a linear type.  For instance, the premise of the leaf
involving $\nil$ views $\nil : \ltonl {({\literal a}^{*})}$ as the
name of a constructor, and a constructor should be nonlinearly valued
because we may call it several times (or not at all). However, the
conclusion of this leaf views $\nil : {\literal a}^{*}$ as a linear
value, which in our syntax is an implicit coercion from a nonlinear
value to a linear value. After we call the constructor it ``returns''
a value that may only be used a single time.

\begin{figure}
\begin{mathpar}
\footnotesize
\DIFdelbeginFL %DIFDELCMD < \inferrule
%DIFDELCMD < {
%DIFDELCMD <   \inferrule
%DIFDELCMD <   {
%DIFDELCMD <     \inferrule
%DIFDELCMD <     {
%DIFDELCMD <       \inferrule
%DIFDELCMD <       {
%DIFDELCMD <         \inferrule
%DIFDELCMD <         {
%DIFDELCMD <           \inferrule
%DIFDELCMD <           {~}
%DIFDELCMD <           {\cdot \vdash \cons : \ltonl {({\literal a} \lto {\literal a}^* \lto {\literal a}^*)}}
%DIFDELCMD <         }
%DIFDELCMD <         {\cdot \vdash \cons : {\literal a} \lto {\literal a}^* \lto {\literal a}^*}
%DIFDELCMD <         \\
%DIFDELCMD <         \inferrule
%DIFDELCMD <         {~}
%DIFDELCMD <         {a : {\literal a} \vdash a : {\literal a}}
%DIFDELCMD <       }
%DIFDELCMD <       {a : a \vdash \cons~a : {\literal a}^* \lto {\literal a}^*}
%DIFDELCMD <       \\
%DIFDELCMD <       \inferrule
%DIFDELCMD <       {
%DIFDELCMD <         \inferrule
%DIFDELCMD <         {~}
%DIFDELCMD <         {\cdot \vdash \nil : \ltonl {({\literal a}^*)}}
%DIFDELCMD <       }
%DIFDELCMD <       {\cdot \vdash \nil : {\literal a}^*}
%DIFDELCMD <     }
%DIFDELCMD <     {a : {\literal a} \vdash \cons~a~\nil : {\literal a}^{*}}
%DIFDELCMD <     \\
%DIFDELCMD <     \inferrule
%DIFDELCMD <     {~}
%DIFDELCMD <     {b : {\literal b} \vdash b : {\literal b}}
%DIFDELCMD <   }
%DIFDELCMD <   {a : {\literal a} , b : {\literal b} \vdash (\cons~a~\nil) \otimes b : {\literal a}^{*} \otimes {\literal b}}
%DIFDELCMD < }
%DIFDELCMD < {a : \literal a , b : \literal b \vdash \texttt{g} := \inl ((\cons~a~\nil)
%DIFDELCMD <   \otimes b) : ({\literal a}^{*} \otimes \literal b) \oplus \literal c}
%DIFDELCMD < %%%
\DIFdelendFL \DIFaddbeginFL \inferrule
{
  \inferrule
  {
    \inferrule
    {
      \inferrule
      {
        \inferrule
        {
          \inferrule
          {~}
          {\cdot \vdash \cons : \ltonl {({\literal a} \lto {\literal a}^* \lto {\literal a}^*)}}
        }
        {\cdot \vdash \cons : {\literal a} \lto {\literal a}^* \lto {\literal a}^*}
        \\
        \inferrule
        {~}
        {a : {\literal a} \vdash a : {\literal a}}
      }
      {a : a \vdash \cons~a : {\literal a}^* \lto {\literal a}^*}
      \\
      \inferrule
      {
        \inferrule
        {~}
        {\cdot \vdash \nil : \ltonl {({\literal a}^*)}}
      }
      {\cdot \vdash \nil : {\literal a}^*}
    }
    {a : {\literal a} \vdash \cons~a~\nil : {\literal a}^{*}}
    \\
    \inferrule
    {~}
    {b : {\literal b} \vdash b : {\literal b}}
  }
  {a : {\literal a} , b : {\literal b} \vdash (\cons~a~\nil , b) : {\literal a}^{*} \otimes {\literal b}}
}
{a : \literal a , b : \literal b \vdash \texttt{g} := \inl (\cons~a~\nil , b) : ({\literal a}^{*} \otimes \literal b) \oplus \literal c}
\DIFaddendFL \end{mathpar}
\DIFmodbegin
\begin{lstlisting}[alsolanguage=DIFcode]
g : (*@$\uparrow$@*)(('a' (*@$\otimes$@*) 'b') (*@$\lto$@*) ('a'(*@$^*$@*) (*@$\otimes$@*) 'b') (*@$\oplus$@*) 'c')
%DIF < g (a , b) = inl (cons a' nil (*@$\otimes$@*) b')
%DIF > g (a , b) = inl (cons a nil , b)
\end{lstlisting}
\DIFmodend
\caption{\DIFaddbeginFL \DIFaddFL{(}\Agda{\fillerlink}\DIFaddFL{) }\DIFaddendFL \stringquote{ab} \DIFdelbeginFL \DIFdelFL{matches
  }\DIFdelendFL \DIFaddbeginFL \DIFaddFL{is parsed by
  }\DIFaddendFL $({\literal a}^{*} \otimes \literal b) \oplus \literal c$}
\label{fig:kleenestarderivation}
\end{figure}

We may also have derivations where the term in context is not simply a
string of literals. In \cref{fig:kleeneabstractproof} we show that every parse
of the grammar $(A \otimes A)^{*}$ induces a parse of $A^{*}$ for an
arbitrary grammar $A$. The context $(A \otimes A)^{*}$ does not correspond directly to a string, so it is
not quite appropriate
to think of a linear term here as a parse \textit{tree}.
The
context $a : (A \otimes A)^{*}$ does not contain concrete data to be parsed; rather, there may be many choices of string underlying the parse tree captured
by the variable $a$. Thus, the term $\texttt{h}$ from
\cref{fig:kleeneabstractproof} is not a parse of a string, and
it is more
appropriate to think of it as a parse \textit{transformer} --- a function from
parses of $(\A \otimes \A)^{*}$ to \DIFdelbegin \DIFdel{parse }\DIFdelend \DIFaddbegin \DIFadd{parses }\DIFaddend of $\A^{*}$.

We define \DIFdelbegin \DIFdel{$\texttt{h}$ }\DIFdelend \DIFaddbegin \DIFadd{$h$ }\DIFaddend by recursion on terms of type $(\A \otimes \A)^{*}$.
This recursion is expressed in the derivation tree by invoking the
elimination principle for Kleene star, written
as $\fold$. The parse transformer \DIFdelbegin \DIFdel{$\texttt{h}$ }\DIFdelend \DIFaddbegin \DIFadd{$h$ }\DIFaddend is more compactly presented in the pseudocode
of \cref{fig:kleeneabstractproof} by pattern matching on the input and making an
explicit recursive call in the body of its definition.

\begin{figure}
\DIFaddbeginFL \footnotesize
\DIFaddendFL \begin{mathpar}
  \DIFdelbeginFL %DIFDELCMD < \inferrule
%DIFDELCMD <   {
%DIFDELCMD <     \inferrule
%DIFDELCMD <     {~}
%DIFDELCMD <     {\cdot \vdash \nil : \A^*}
%DIFDELCMD <     \\
%DIFDELCMD <     \inferrule
%DIFDELCMD <     {
%DIFDELCMD <       \inferrule
%DIFDELCMD <       {
%DIFDELCMD <         \inferrule
%DIFDELCMD <         {~}
%DIFDELCMD <         {a_1 : \A, a_2 : \A , a_{*} : \A^* \vdash \cons (a_1 , \cons(a_2 , a_{*})) : \A^*}
%DIFDELCMD <       }
%DIFDELCMD <       {a' : \A \otimes \A , a_{*} : \A^* \vdash g := \letin {a_1 \otimes a_2} {a'} {\cons (a_1 , \cons(a_2 , a_{*})) : \A^*}}
%DIFDELCMD <     }
%DIFDELCMD <     {\cdot \vdash f := \lamblto a {\lamblto {a_{*}} {\app {\app g a} {a_{*}} }} : (\A \otimes \A) \lto \A^* \lto \A^*}
%DIFDELCMD <   }
%DIFDELCMD <   {a : (\A \otimes \A)^* \vdash \texttt{h} := \fold(\nil , f)(a) : \A^* }
%DIFDELCMD < %%%
\DIFdelendFL \DIFaddbeginFL \inferrule
  {
    \inferrule
    {~}
    {\cdot \vdash \nil : \A^*}
    \\
    \inferrule
    {
      \inferrule
      {
        \inferrule
        {~}
        {a_1 : \A, a_2 : \A , as : \A^* \vdash \cons~a_1 (\cons~a_2~as) : \A^*}
      }
      {aa : \A \otimes \A , as : \A^* \vdash \letin {(a_1 , a_2)}
        {aa} {\cons~a_1 (\cons~a_2~as)} : \A^*}
    }
    {\cdot \vdash f := \lamblto {aa} {\lamblto {as} {\letin {(a_1 , a_2)}
        {aa} {\cons~a_1 (\cons~a_2~as)}}} : (\A \otimes \A) \lto \A^* \lto \A^*}
  }
  {aas : (\A \otimes \A)^* \vdash \texttt{h} := \fold(\nil , f)(aas) : \A^* }
\DIFaddendFL \end{mathpar}
\DIFmodbegin
\begin{lstlisting}[alsolanguage=DIFcode]
h : (*@$\uparrow$@*)((A (*@$\otimes$@*) A)(*@$^*$@*) (*@$\lto$@*) A(*@$^*$@*))
h nil = nil
%DIF < h (cons (a1 (*@$\otimes$@*) a2) as) = cons a1 (cons a2 (h as))
%DIF > h (cons (a1 , a2) as) = cons a1 (cons a2 (h as))
\end{lstlisting}
\DIFmodend
\caption{\DIFaddbeginFL \DIFaddFL{(}\Agda{\fillerlink}\DIFaddFL{) }\DIFaddendFL A parse transformer for abstract grammars}
\label{fig:kleeneabstractproof}
\end{figure}

\paragraph{Non-deterministic Finite Automata}
\newcommand{\s}{\texttt{s}}
\newcommand{\0}{\texttt{0}}
\newcommand{\1}{\texttt{1}}
\newcommand{\2}{\texttt{2}}
Regular expressions are a compact formalism for defining a formal grammar,
but an expression such as
$({\literal a}^{*} \otimes \literal b) \oplus \literal c$ does not give a very operational perspective of how to parse
it. For this reason, most parsers are based on compiling a grammar to a corresponding notion of automaton, which is readily implemented. To implement
these algorithms in \theoryabbv, we need a way to represent automata as types in the same way we can represent regular expressions.

Finite automata are precisely the class of machines that recognize regular
expressions. \cref{fig:exampleNFA} shows a non-deterministic
finite automaton (NFA) for the regular expression
$({\literal a}^{*} \otimes \literal b) \oplus \literal c$, along with a type $\Trace$, an \emph{indexed} inductive linear type of traces through this automaton. Defining an indexed inductive type can be thought
of as defining a family of mutually recursive inductive types, one for each element of the indexing type. Here $\Trace$ 
uses an index $\s : \texttt{Fin 3}$ which picks out which
state in the automaton a trace begins at --- where $\texttt{Fin 3}$ is the
finite type containing inhabitants $\{\0 , \1 , \2\}$. We can think of this as defining three mutually recursive inductive types $\Trace~\0$,
$\Trace~\1$, and $\Trace~\2$.
%
There are three kinds of constructors for $\Trace$: (1) those that
terminate traces, (2) those that correspond to transitions labeled by
a character, and (3) those that correspond to transitions labeled by
the empty string $\epsilon$. The constructor $\texttt{stop}$
terminates a trace in the accepting state $\2$. The constructors
$\texttt{1to1}$, $\texttt{1to2}$, $\texttt{0to2}$ each define a
labeled transition through the NFA, and each of these consumes a parse
of the label's character and a trace beginning at the destination of a
transition to produce a trace beginning at the source of a
transition. The constructor $\texttt{0to1}$ behaves similarly, except
its transition is labeled with the empty string $\epsilon$. Therefore,
$\texttt{0to1}$ takes in a trace beginning at state $\1$ and returns a
trace beginning at state $\0$ corresponding to the same underlying
string.
%
Lastly, we give a lambda term that constructs an accepting trace
starting at the initial state for the string \stringquote{ab}.  Later
in \Cref{sec:applications}, we will show that we can actually
construct mutually inverse functions between the regular expression
$({\literal a}^{*} \otimes \literal b) \oplus \literal c$ and its
corresponding NFA traces ($\Trace~\0$) demonstrating that the regular
expression and the automaton capture the same language. Further, since
the functions are mutually inverse, this shows they are \emph{strongly
equivalent} as grammars.

\begin{figure}
  \DIFdelbeginFL %DIFDELCMD < \begin{tikzpicture}[node distance = 25mm ]
%DIFDELCMD <     \node[state, initial] (0) {$\0$};
%DIFDELCMD <     \node[state, below left of=0] (1) {$\1$};
%DIFDELCMD <     \node[state, right of=1, accepting] (2) {$\2$};
%DIFDELCMD < 

%DIFDELCMD <     \path[->] (0) edge[above] node{$\epsilon$} (1)
%DIFDELCMD <               (0) edge[right] node{$\stringquote{c}$} (2)
%DIFDELCMD <               (1) edge[loop left] node{$\stringquote{a}$} (1)
%DIFDELCMD <               (1) edge[below] node{$\stringquote{b}$} (2);
%DIFDELCMD <   \end{tikzpicture}
%DIFDELCMD <   %%%
\DIFdelendFL \DIFaddbeginFL \begin{minipage}[t]{.6\textwidth}
    \vspace{0pt}
  \DIFaddendFL \begin{lstlisting}
data Trace : (s : Fin 3) (*@$\to$@*) L where
  stop : (*@$\uparrow$@*)(Trace 2)
  1to1 : (*@$\uparrow$@*)('a' (*@$\lto$@*) Trace 1 (*@$\lto$@*) Trace 1)
  1to2 : (*@$\uparrow$@*)('b' (*@$\lto$@*) Trace 2 (*@$\lto$@*) Trace 1)
  0to2 : (*@$\uparrow$@*)('c' (*@$\lto$@*) Trace 2 (*@$\lto$@*) Trace 0)
  0to1 : (*@$\uparrow$@*)(Trace 1 (*@$\lto$@*) Trace 0)

k : (*@$\uparrow$@*)(('a' (*@$\otimes$@*) 'b') (*@$\lto$@*)  Trace 0)
k (a , b) = 0to1 (1to1 a (1to2 b stop))
\end{lstlisting}
  \DIFaddbeginFL \end{minipage}%DIF > 
  \begin{minipage}[t]{.4\textwidth}
  \vspace{8pt}
  \DIFaddFL{\hspace{10pt}
  }\begin{tikzpicture}[node distance = 25mm ]
    \node[state, initial] (0) {$\0$};
    \node[state, below left of=0] (1) {$\1$};
    \node[state, right of=1, accepting] (2) {$\2$};

    \path[->] (0) edge[above] node{$\epsilon$} (1)
              (0) edge[right] node{$\stringquote{c}$} (2)
              (1) edge[loop left] node{$\stringquote{a}$} (1)
              (1) edge[below] node{$\stringquote{b}$} (2);
  \end{tikzpicture}
  \end{minipage}%DIF > 
  \DIFaddendFL \caption{\DIFaddbeginFL \DIFaddFL{(}\Agda{\fillerlink}\DIFaddFL{) }\DIFaddendFL NFA for $(\a^{*} \otimes \b) \oplus \c$ and its corresponding type}
  \label{fig:exampleNFA}
\end{figure}

%% 3. Third example: NFA traces to show the indexed inductive types

\section{Syntax and Typing for \theoryname}
\label{sec:tt}

%% OUTLINE
%%
%% 1. Overview of syntax: what are the judgments
%% 2. Cover the linear types except for inductives
%% 3. Cover non-linear types, mainly focus on the universes
%% 4. Cover indexed inductive linear types
%% 5. Cover the "axioms" we add

The design of \theoryabbv is based on the dependent
linear-non-linear calculus (\lnld) and Lambek calculus, also known as
non-commutative linear logic
\DIFdelbegin \DIFdel{.
}\DIFdelend \cite{krishnaswami_integrating_2015,lambek58}. As in \lnld,
\theoryabbv includes both non-linear dependent types, as well as
linear types, which are allowed to depend on the non-linear types, but
not on other linear types.
%
The main point of departure from \lnld's design is that, as in Lambek calculus \cite{lambek58}, the linear
typing is \emph{non-commutative} --- i.e., that exchange is not an
admissible structural rule. Furthermore, we add a general-purpose
indexed inductive linear type connective, as well as an
\emph{equalizer} type, which we will show allows us to perform
inductive proofs of equalities between linear terms.
%
Finally, while \lnld was enhanced with special connectives
inspired by separation logic to model imperative programming, we
instead add base types and axioms to the system specifically to model
formal grammars and parsing.

The formation rules for the judgments of \theoryabbv are shown in
\DIFdelbegin \DIFdel{\mbox{%DIFAUXCMD
\Cref{fig:formation}}\hskip0pt%DIFAUXCMD
}\DIFdelend \DIFaddbegin \DIFadd{\mbox{%DIFAUXCMD
\Cref{fig:contexts,fig:non-linear-types,fig:linear-formation}}\hskip0pt%DIFAUXCMD
}\DIFaddend . $\Gamma$ stands for non-linear contexts; $X,Y,Z$
stand for non-linear types; $M,N$ stand for non-linear terms, these
act as in an ordinary dependent type theory; $\Delta$ stands for
linear contexts; $A,B,C$ for linear types; and, $e,f,g$ for linear
terms. These contexts, types and terms are allowed to depend on an
ambient non-linear context $\Gamma$, but note that linear types $A$
cannot depend on any \emph{linear} variables in $\Delta$. We include
definitional equality judgments for both kinds of type and term
judgments as well. Additionally, we have a judgment \DIFdelbegin \DIFdel{$\Gamma\vdash X
\isSmall$ }\DIFdelend \DIFaddbegin \DIFadd{$\Gamma \vdash X
\isSmall$ }\DIFaddend which is used in the definition of universe types.

\begin{figure}
  \begin{mathpar}
    \DIFdelbeginFL %DIFDELCMD < \inferrule{~}{\ctxwff \Gamma}
%DIFDELCMD < %%%
\DIFdelendFL \DIFaddbeginFL \footnotesize
    \boxed{\ctxwff \Gamma}
\DIFaddendFL 

    \DIFdelbeginFL %DIFDELCMD < \inferrule{\ctxwff \Gamma}{\ctxwffjdg \Gamma X}
%DIFDELCMD < %%%
\DIFdelendFL \DIFaddbeginFL \inferrule{~}{\ctxwff \cdot}
\DIFaddendFL 

    \DIFdelbeginFL %DIFDELCMD < \inferrule{\ctxwffjdg \Gamma X}{\Gamma \vdash X \isSmall}
%DIFDELCMD < %%%
\DIFdelendFL \DIFaddbeginFL \inferrule{\ctxwff \Gamma \and \ctxwffjdg \Gamma X}{\ctxwff {\Gamma , x : X}}
\DIFaddendFL 

    \DIFdelbeginFL %DIFDELCMD < \inferrule{\ctxwffjdg \Gamma X \and \ctxwffjdg \Gamma Y}{\ctxwffjdg \Gamma {X \equiv Y}}
%DIFDELCMD < %%%
\DIFdelendFL \DIFaddbeginFL \boxed{\linctxwff \Gamma \Delta}
\DIFaddendFL 

    \DIFdelbeginFL %DIFDELCMD < \inferrule{\ctxwffjdg \Gamma X}{\Gamma \vdash M : X}
%DIFDELCMD < %%%
\DIFdelendFL \DIFaddbeginFL \inferrule{~}{\linctxwff \Gamma \cdot}
\DIFaddendFL 

    \DIFdelbeginFL %DIFDELCMD < \inferrule{\Gamma \vdash M : X \and \Gamma \vdash N : X}{\Gamma \vdash M \equiv N : X}
%DIFDELCMD < %%%
\DIFdelendFL \DIFaddbeginFL \inferrule{\linctxwff \Gamma \Delta \and \linctxwffjdg \Gamma A}{\linctxwff
      {\Gamma} {\Delta , a : A}}
  \end{mathpar}
  \caption{\DIFaddFL{Context well-formedness rules}}
  \label{fig:contexts}
\end{figure}
\DIFaddend 

\DIFdelbegin %DIFDELCMD < \inferrule{\ctxwff \Gamma}{\linctxwff \Gamma \Delta}
%DIFDELCMD < %%%
\DIFdelend \DIFaddbegin \subsection{\DIFadd{Non-linear Typing}}
\DIFaddend 

\DIFdelbegin %DIFDELCMD < \inferrule{\ctxwff \Gamma}{\Gamma \vdash A \isLinTy}
%DIFDELCMD < %%%
\DIFdelend \DIFaddbegin \begin{figure}
  \begin{mathpar}
  \footnotesize
    \boxed{\inferrule{\ctxwff\Gamma}{\ctxwffjdg \Gamma X}}
\DIFaddendFL 

    \DIFdelbeginFL %DIFDELCMD < \inferrule{\Gamma \vdash A \isLinTy \and \Gamma \vdash B \isLinTy}{\Gamma \vdash A \equiv B}
%DIFDELCMD < %%%
\DIFdelendFL \DIFaddbeginFL \inferrule{~}{\ctxwffjdg \Gamma {U}}
\DIFaddendFL 

    \DIFdelbeginFL %DIFDELCMD < \inferrule{\Gamma \vdash \Delta \isLinCtx \and \Gamma \vdash A \isLinTy}{\Gamma;\Delta\vdash e : A}
%DIFDELCMD < %%%
\DIFdelendFL \DIFaddbeginFL \inferrule{~}{\ctxwffjdg \Gamma {L}}
\DIFaddendFL 

    \DIFdelbeginFL %DIFDELCMD < \inferrule{\Gamma; \Delta \vdash e : A \and \Gamma;\Delta \vdash f : A}{\Gamma;\Delta\vdash e \equiv f : A}
%DIFDELCMD <   %%%
\DIFdelendFL \DIFaddbeginFL \inferrule{~}{\ctxwffjdg \Gamma 1}

    \inferrule{~}{\ctxwffjdg \Gamma \bot}

    \inferrule{~}{\ctxwffjdg \Gamma {Bool}}

    \inferrule{~}{\ctxwffjdg \Gamma {Nat}}

    \inferrule{\ctxwffjdg \Gamma X \and \ctxwffjdg {\Gamma , x : X} {Y}}{\ctxwffjdg \Gamma {\PiTyLimit {x}{X}{Y}}}

    \inferrule{\ctxwffjdg \Gamma X \and \ctxwffjdg {\Gamma , x : X} {Y}}{\ctxwffjdg \Gamma {\SigTyLimit {x}{X}{Y}}}

    \\

    \inferrule{\nonlinterm \Gamma M U}{\ctxwffjdg \Gamma {\unquoteTy M}}

    \inferrule{\linctxwffjdg \Gamma A}{\ctxwffjdg \Gamma {\ltonl A}}

    \inferrule{\ctxwffjdg \Gamma X \and \nonlinterm \Gamma M X \and \nonlinterm \Gamma N X}{\ctxwffjdg \Gamma {M =_{X} N}}

    \\

    \boxed{\inferrule{\ctxwffjdg\Gamma X \and \ctxwffjdg\Gamma Y}{\ctxwffjdg \Gamma {X \equiv Y}}}

    \inferrule{\nonlinterm \Gamma {X \equiv Y} {U}}{\ctxwffjdg \Gamma {X \equiv Y}}

    \inferrule{\linctxwffjdg \Gamma X}{\linctxwffjdg \Gamma {\unquoteTy
        {\quoteTy X} \equiv X}}

    \boxed{\inferrule{\ctxwffjdg \Gamma X}{\Gamma \vdash X \isSmall}}

    \\
    \boxed{\inferrule{\ctxwffjdg \Gamma X}{\nonlinterm \Gamma M X}}

    \inferrule{\Gamma \vdash  X \isSmall}{\nonlinterm \Gamma {\quoteTy X} U}

    \inferrule{\Gamma \vdash A \isSmallLin}{\nonlinterm \Gamma {\quoteTy A} L}

    \inferrule{\linterm \Gamma \cdot e A}{\nonlinterm \Gamma e \ltonl{A}}

    \inferrule{\nonlinterm \Gamma {M \equiv N} {X}}
      {\nonlinterm \Gamma {refl} {M =_{X} N}}

    \\
    \boxed{\inferrule{\nonlinterm\Gamma M X \and \nonlinterm \Gamma N X}{\nonlinterm \Gamma {M \equiv N} X}}

    \inferrule{\nonlinterm \Gamma P {M =_{X} N}}{\nonlinterm\Gamma{M \equiv N} X}
  \DIFaddendFL \end{mathpar}
  \caption{\DIFaddbeginFL \DIFaddFL{Non-linear }\DIFaddendFL Formation \DIFdelbeginFL \DIFdelFL{rules}\DIFdelendFL \DIFaddbeginFL \DIFaddFL{and Typing Rules (selection)}\DIFaddendFL }
  \DIFdelbeginFL %DIFDELCMD < \label{fig:formation}
%DIFDELCMD < %%%
\DIFdelendFL \DIFaddbeginFL \label{fig:non-linear-types}
\DIFaddendFL \end{figure}

\DIFdelbegin \subsection{\DIFdel{Non-linear Typing}}
%DIFAUXCMD
\addtocounter{subsection}{-1}%DIFAUXCMD
\DIFdel{The }\DIFdelend \DIFaddbegin \DIFadd{We present a selection of the }\DIFaddend non-linear \DIFdelbegin \DIFdel{types of }%DIFDELCMD < \theoryabbv %%%
\DIFdel{include the standard dependent
types for $\Pi,\Sigma$, extensional equality, natural numbers,
booleans, unit and empty types, and we assume function
extensionality\mbox{%DIFAUXCMD
\cite{Hofmann_1997}}\hskip0pt%DIFAUXCMD
. We present the
other non-linear }\DIFdelend type constructions in
\Cref{fig:non-linear-types} \DIFaddbegin \DIFadd{and provide the rest in
\mbox{%DIFAUXCMD
\cref{fig:full-non-linear-types,fig:jdg-eq-nonlinear} }\hskip0pt%DIFAUXCMD
of the appendix}\DIFaddend .  First,
we include universe types $U$ of small non-linear types and $L$ of linear
types. These are defined as
universes ``ala Coquand'' in that we define \DIFdelbegin \DIFdel{a judgment saying when
a
}\DIFdelend \DIFaddbegin \DIFadd{judgments saying when
}\DIFaddend non-linear \DIFdelbegin \DIFdel{type is small and define the universe }\DIFdelend \DIFaddbegin \DIFadd{and linear types are }\emph{\DIFadd{small}} \DIFadd{and define the universes }\DIFaddend to internalize
precisely this judgment \cite{coquandPresheafModel,lmcs:7713}. \DIFaddbegin \DIFadd{The definition of smallness is simply that all types are small as long as their sub-formulae are, with the exception of the two universe types themselves. }\DIFaddend These universe
types are needed so that we can define \DIFaddbegin \DIFadd{non-linear and linear }\DIFaddend types by recursion on natural
numbers.
Next, we include \DIFdelbegin \DIFdel{a }\DIFdelend \DIFaddbegin \DIFadd{standard $\Sigma,\Pi$, empty, unit, Boolean, and natural number
types. More complex inductive types --- such as sum types, list types, and
$Fin~n$ --- can be defined in terms of these primitives, and we give their
encoding in the appendix.
}

\DIFadd{We use an }\emph{\DIFadd{extensional}} \DIFadd{equality type $M =_{X} N$ with introduction form
$refl$, but no elimination form. Instead we have the equality reflection rule
which allows us to conclude a definitional equality $M =_{X} N$ from an
arbitrary typal equality proof $P$.  The usage of an extensional equality type
matches our implementation, which interprets both judgmental and typal equality
as Cubical Agda's }\texttt{\DIFadd{Path}} \DIFadd{type, and so naturally supports the equality
reflection rule. Extensional equality makes type checking of our syntax as such
undecidable \mbox{%DIFAUXCMD
\citep{Hofmann_1997} }\hskip0pt%DIFAUXCMD
because the conversion rule may require an arbitrarily complex
equality proof with no explicit proof term, but in our Agda implementation we
must provide all of these equalities manually, and so the extensionality does
not raise any issues for our implementation. This makes
}\theoryabbv \DIFadd{into an extensional theory, supporting function extensionality and
the uniqueness of identity proofs. The development could be ported to an intensional type theory in the future, possibly requiring the use of setoids to handle function extensionality.
}

\DIFadd{Lastly, we include a }\DIFaddend non-linear type $\ltonl A$ where $A$ is a linear type. The
intuition for this type is that its elements are the linear terms that are
``resource free'': its introduction rule says we can construct an $\ltonl A$
when we have a linear term of type $A$ with no free linear
variables. \DIFaddbegin \DIFadd{Semantically, this is the type of parses of the empty string. }\DIFaddend This
type is used extensively in our examples, playing a similar role to the $!$
modality of ordinary linear logic or the persistence modality $\square$ of
separation logic \cite{girard_linear_1987,jung_higher-order_2016}.

%DIF < % \begin{figure}
%DIF < %   \[
%DIF < %   \begin{array}{rrcl}
%DIF < %     \textrm{non-linear contexts} & \Gamma & ::= & \cdot \pipe \Gamma,x:X \\
%DIF < %     \textrm{non-linear types} & X , Y , Z & ::= & \mathsf{Nat} \pipe
%DIF < %                                                   \mathsf{Bool} \pipe
%DIF < %                                                   \mathsf{Empty} \pipe
%DIF < %                                                   \mathsf{Unit} \pipe
%DIF < %                                                   \mathsf{SPF}\,X\pipe U \pipe L
%DIF < %                                                   \pipe \ltonl A \pipe \sum\limits_{x:X} Y
%DIF < %                                                   \pipe \prod\limits_{x:X} Y\pipe M =_A
%DIF < %                                                   N
%DIF < %   \end{array}
%DIF < %   \]
%DIF < %   \caption{Non-linear Types}
%DIF < % \end{figure}
\DIFaddbegin \subsection{\DIFadd{Linear Typing}}

\DIFaddend \begin{figure}
  \DIFdelbeginFL %DIFDELCMD < \begin{footnotesize}
%DIFDELCMD <     %%%
\DIFdelendFL \begin{mathpar}
    \DIFdelbeginFL %DIFDELCMD < \inferrule{}{\Gamma \vdash U \isTy}\and
%DIFDELCMD <     \inferrule
%DIFDELCMD <     {\Gamma \vdash M : U}
%DIFDELCMD <     {\Gamma \vdash \unquoteTy M \isTy}\and
%DIFDELCMD <     \inferrule
%DIFDELCMD <     {\Gamma \vdash X \isTy}
%DIFDELCMD <     {\Gamma \vdash \quoteTy X : U}\and
%DIFDELCMD <     \unquoteTy{\quoteTy{X}} %%%
\DIFdelFL{\equiv X }%DIFDELCMD < \and \inferrule{\Gamma \vdash M : U}{\Gamma\vdash \quoteTy{\unquoteTy{M}}\equiv M : U}
%DIFDELCMD < %%%
\DIFdelendFL \DIFaddbeginFL \footnotesize
    \boxed{\inferrule{\ctxwff\Gamma}{\linctxwffjdg \Gamma A}}
\DIFaddendFL 

    \DIFdelbeginFL %DIFDELCMD < \inferrule{}{\Gamma \vdash L \isTy}\and
%DIFDELCMD <     \inferrule
%DIFDELCMD <     {\Gamma \vdash M : L}
%DIFDELCMD <     {\Gamma \vdash \unquoteTy M \isLinTy}\and
%DIFDELCMD <     \inferrule
%DIFDELCMD <     {\Gamma \vdash A \isLinTy}
%DIFDELCMD <     {\Gamma \vdash \quoteTy A : L}\and
%DIFDELCMD <     \unquoteTy{\quoteTy{A}} %%%
\DIFdelFL{\equiv A }%DIFDELCMD < \and \inferrule{\Gamma \vdash M : U}{\Gamma\vdash \quoteTy{\unquoteTy{M}}\equiv M : U}
%DIFDELCMD < %%%
\DIFdelendFL \DIFaddbeginFL \inferrule{~}{\linctxwffjdg \Gamma I}
\DIFaddendFL 

    \DIFdelbeginFL %DIFDELCMD < \inferrule
%DIFDELCMD <     {\Gamma \vdash A \isLinTy}
%DIFDELCMD <     {\Gamma \vdash \ltonl { A } \isTy}
%DIFDELCMD <     \and
%DIFDELCMD <     \inferrule{\Gamma ; \cdot \vdash e : A}{\Gamma \vdash e : \ltonl A}\and
%DIFDELCMD <     \inferrule{\Gamma \vdash M : \ltonl A}{\Gamma ; \cdot \vdash M : A} \and
%DIFDELCMD <   %%%
\DIFdelendFL \DIFaddbeginFL \inferrule{c \in \Sigma}{\linctxwffjdg \Gamma {\literal{c}}}

    \inferrule{\linctxwffjdg \Gamma A \and \linctxwffjdg \Gamma B}{\linctxwffjdg
      \Gamma {A \otimes B}}

    \inferrule{\linctxwffjdg \Gamma A \and \linctxwffjdg \Gamma B}{\linctxwffjdg
      \Gamma {A \lto B}}

    \inferrule{\linctxwffjdg \Gamma A \and \linctxwffjdg \Gamma B}{\linctxwffjdg
      \Gamma {A \tol B}}

    \inferrule{\linctxwffjdg {\Gamma, x : X} A}{\linctxwffjdg
      \Gamma {\LinSigTyLimit{x}{X}{A}}}

    \inferrule{\linctxwffjdg {\Gamma, x : X} A}{\linctxwffjdg
      \Gamma {\LinPiTyLimit{x}{X}{A}}}

    \inferrule{\nonlinterm \Gamma f {\ltonl{(A \lto B)}} \and
      \nonlinterm \Gamma g {\ltonl{(A \lto B)}}}{\linctxwffjdg \Gamma {\equalizer {a}{f}{g}}}

    \inferrule{\nonlinterm \Gamma M L}{\linctxwffjdg \Gamma {\unquoteTy M}}

    \boxed{\inferrule{\linctxwffjdg \Gamma A}{\Gamma \vdash A \isSmallLin}}

    \\
    \boxed{\inferrule{\linctxwffjdg \Gamma A \and \linctxwffjdg \Gamma B}{\linctxwffjdg \Gamma {A \equiv B}}}

    \inferrule{\nonlinterm \Gamma {A \equiv B} L}{\linctxwffjdg
      \Gamma {A \equiv B}}

    \inferrule{\linctxwffjdg \Gamma A}{\linctxwffjdg \Gamma {\unquoteTy
        {\quoteTy A} \equiv A}}
  \DIFaddendFL \end{mathpar}
  \DIFdelbeginFL %DIFDELCMD < \end{footnotesize}
%DIFDELCMD <   %%%
\DIFdelendFL \caption{\DIFdelbeginFL \DIFdelFL{Non-linear types (selection)}\DIFdelendFL \DIFaddbeginFL \DIFaddFL{Linear Type Formers, Type Equivalence}\DIFaddendFL }
  \DIFdelbeginFL %DIFDELCMD < \label{fig:non-linear-types}
%DIFDELCMD < %%%
\DIFdelendFL \DIFaddbeginFL \label{fig:linear-formation}
\DIFaddendFL \end{figure}

\DIFdelbegin \subsection{\DIFdel{Linear Typing}}
%DIFAUXCMD
\addtocounter{subsection}{-1}%DIFAUXCMD
%DIFDELCMD < 

%DIFDELCMD < %%%
\DIFdel{We give an overview of the linear types and terms in
\mbox{%DIFAUXCMD
\Cref{fig:linear-types-and-terms}}\hskip0pt%DIFAUXCMD
}\DIFdelend \DIFaddbegin \DIFadd{We give the rules for linear type
formation in \mbox{%DIFAUXCMD
\cref{fig:linear-formation} }\hskip0pt%DIFAUXCMD
and the definition of linear terms in
\mbox{%DIFAUXCMD
\cref{fig:linear-terms}}\hskip0pt%DIFAUXCMD
}\DIFaddend . The equational theory for these
types is straightforward $\beta\eta$ equivalence and included in the
appendix. First, the linear variable rule says that a linear variable
can be used if it is the \emph{only} variable in the context.

\DIFdelbegin \DIFdel{First}\DIFdelend \DIFaddbegin \DIFadd{Next}\DIFaddend , we cover the ``multiplicative'' connectives of non-commutative
linear logic. The linear unit ($\I$) and tensor product ($\otimes$)
are standard for a non-commutative linear logic: when we construct a
linear unit we cannot use any variables and when we construct a tensor
product, the two sides must use disjoint variables, and the variables
the left side of the product uses must be to the left in the context
of the variables used by the right side of the tensor product. The
elimination rules for unit and tensor are given by pattern
matching. The pattern matching rules split the linear context into
three pieces $\Delta_1,\Delta_2,\Delta_3$: the middle $\Delta_2$ is
used by the scrutinee of the pattern match, and in the continuation
this context is replaced by the variables brought into scope by the
pattern match. This ensures that pattern matches maintain the proper
ordering of resource usage.

Because we are non-commutative, there are two function types: $A \lto
B$ and $B \tol A$, which have similar $\lambda$ introduction forms and
application elimination forms. The difference between these is that
the introduction rule for $A \lto B$ adds a variable to the right side
of the context, whereas the introduction rule \DIFdelbegin \DIFdel{(elided) }\DIFdelend for $B \tol A$ adds a
variable to the left side of the context. In our experience, because
by convention parsing algorithms parse from left-to-right, we rarely
need to use the $B \tol A$ connective. As we have already seen, the
$\lto$ connective is frequently used in conjunction with the
$\uparrow$ connective so that we can abstract non-linearly over linear
functions.

Next, we cover the ``additive'' connectives. First, we use the
non-linear types to define \emph{indexed} versions of the additive
disjunction $\oplus$ and additive conjunction $\&$ of linear logic,
which can be thought of as linear versions of the $\Sigma$ and $\Pi$
connectives of ordinary dependent type theory, respectively. The
indexed $\&$ is defined by a $\lambda$ that brings a \emph{non-linear}
variable into scope and eliminated using projection where the index
specified is given by a non-linear term. The rules for indexed
$\oplus$ are analogous to a ``\emph{weak}'' $\Sigma$ type: it has an
injection introduction rule $\sigma$, but its elimination rule is
given by \emph{pattern matching} rather than first and second
projections. We can define the more typical nullary and binary
versions of these connectives by using indexing over the empty and
boolean type respectively. We will freely use $0$ to refer to this
empty disjunction and $\top$ to refer to the empty conjunction, and
use infix $\oplus/\&$ for binary disjunction/conjunction.

Lastly, we include a type $\equalizer {a}{f}{g}$ that we call the
\emph{equalizer} of linear functions $f$ and $g$. We think of this
type as the ``subtype'' of elements of $A$ that satisfy the equation
$f\,a\equiv g\,a$. Note that it is important here that $f, g$
themselves are non-linearly used functions, as linear values cannot be
used in a type.  Equalizer types are not needed for non-linear types
since they can be constructed using the equality type as $\sum_{x:X}
f\,x=_Y g\,x$, but this construction can't be used for linear types
because it uses a \emph{dependent} version of the equality type, which
we cannot define as a linear type. While the equalizer type is not
used directly in defining any of our parsers or formal grammars, it is
used for several proofs, allowing for inductive arguments about our
indexed inductive types.

In addition to these type-theoretic principles, we need two additional
axioms that do not generally hold in systems based on linear
logic. First, we need that additive conjunction \emph{distributes}
over additive disjunction --- e.g., in the finitary case that $0 \& A
\cong 0$ and $(A + B) \& C \cong (A \& C) + (B \& C)$. In its most general form,
\DIFdelbegin \DIFdel{the axiom says that the definable function
${\bigoplus\limits_{f : \prod_{x:X}
    Y(x)}\bigamp\limits_{x:X}A\,x\,(f\,x) \lto
  \bigamp\limits_{x:X}\bigoplus\limits_{y:Y(x)}A\,x\,y}$ }\DIFdelend \DIFaddbegin 

\begin{axiom}[Distributivity]
\label{ax:dist}
\DIFadd{For any $A : (x : X) \to Y(x) \to L$, the definable distribuitivity function
%DIF > %%%%%%%%% An attempt to inline
%DIF >  \(
%DIF >   {\LinSigTy{f}{\left(\PiTy{x}{X}{Y(x)}\right)}{\LinPiTy{x}{X}{A\,x\,(f\,x)}} \lto
%DIF >  \LinPiTy{x}{X}{\LinSigTy{y}{Y(x)}{A\,x\,y}} \right)}
%DIF >  \)
\(
 \lamblto{e}{\letin{\sigma\,f\,e'}{e}{\withlamb{x}{e'\,.\pi\,x}}} : \LinSigTyLimit{f}{\PiTy{x}{X}{Y(x)}}{\LinPiTyLimit{x}{X}{A\,x\,(f\,x)}} \lto
\LinPiTyLimit{x}{X}{\LinSigTyLimit{y}{Y(x)}{A\,x\,y}}
\)
}\DIFaddend has an inverse.
\DIFaddbegin \end{axiom}

\DIFadd{The following corollary is a well known consequence of distributivity \mbox{%DIFAUXCMD
\cite{Cockett_1993}}\hskip0pt%DIFAUXCMD
, which we use in \mbox{%DIFAUXCMD
\cref{lem:unambig-to-disjoint} }\hskip0pt%DIFAUXCMD
to prove that unambiguous binary sums have unambiguous summands.
}\begin{corollary}
  \label{cor:binary-mono}
  \DIFadd{Distributivity implies that the constructors $\inl : A \to A \oplus B$,
  $\inr : B \to A \oplus B$ of a binary sum are injective --- i.e. if
  $\inl\,a \equiv \inl\,a'$, then $a \equiv a'$.
}\end{corollary}

\DIFadd{Our primary use of distributivity is to define an equivalence that
expresses a linear type $A$ as a sum over which character it starts with, if any,
\(
A \cong \left(A \& I \right) \oplus \LinSigTy{c}{\Sigma_0}{\left( A \& \left( \literal{c} \otimes \top \right) \right)}
\). We use this equivalence when building a parser for the traces of the lookahead
automaton in \mbox{%DIFAUXCMD
\cref{fig:binop-inductive}}\hskip0pt%DIFAUXCMD
.
}

\DIFaddend Second, we need that the different constructors of
$\bigoplus$ are disjoint. \DIFdelbegin \DIFdel{We add this by adding the axiom that for }\DIFdelend \DIFaddbegin \DIFadd{That is, we want to enforce that $\sigma~x~a$ and
$\sigma~x'~a'$ of type $\LinSigTy{x}{X}{A\,x}$ are not equal whenever $x \neq x'$.
However, because these are linear terms we cannot state their disequality
directly. Instead, we encode the disequality via a function out of an equalizer,
}

\begin{axiom}[$\sigma$-Disjointness]
\label{ax:disjointness}
\DIFadd{For }\DIFaddend any $A : X \to L$ and $x \neq x' : X$ there is a function\DIFdelbegin \DIFdel{$\uparrow(\equalizer{b}{\sigma\,x \circ \pi_1}{\sigma\,x'\circ \pi_2}
\lto 0)$ }\DIFdelend \DIFaddbegin \DIFadd{,
}\[
  \DIFadd{\uparrow(\equalizer{b}{\left( \sigma\,x \circ \pi_1 \right)}{\left(  \sigma\,x'\circ \pi_2 \right)}
  }\lto \DIFadd{0)
}\]
  \DIFaddend where $b: A(x) \& A(x')$\DIFdelbegin \DIFdel{, i.e., }\DIFdelend \DIFaddbegin \DIFadd{. That is, }\DIFaddend the grammar of pairs of an
$a:A(x)$ and an $a': A(x')$ such that $\sigma\,x\,a = \sigma\,x'\,a'$
is empty.
\DIFaddbegin \end{axiom}
\DIFaddend 

\DIFdelbegin %DIFDELCMD < \pedro{imo, since we are providing an Agda implementation, we can
%DIFDELCMD <   be looser with the explanation of the meaning of the disjointness axioms.
%DIFDELCMD <   As it stands, it's a bit hard do follow along. Is there a simpler example
%DIFDELCMD <   we could point to?}
%DIFDELCMD < %%%
\DIFdelend \DIFaddbegin \DIFadd{We use $\sigma$-disjointness in \mbox{%DIFAUXCMD
\cref{lem:unambig-to-disjoint} }\hskip0pt%DIFAUXCMD
to prove that
unambiguous sums have disjoint summands.
}\DIFaddend 

%DIF >  \pedro{imo, since we are providing an Agda implementation, we can
%DIF >    be looser with the explanation of the meaning of the disjointness axioms.
%DIF >    As it stands, it's a bit hard do follow along. Is there a simpler example
%DIF >    we could point to?}
\DIFaddbegin 

\DIFaddend \begin{figure}
  \DIFdelbeginFL %DIFDELCMD < \begin{footnotesize}
%DIFDELCMD <   %%%
\DIFdelendFL \DIFaddbeginFL \footnotesize
  \DIFaddendFL \begin{mathpar}
    \DIFdelbeginFL %DIFDELCMD < \inferrule{}{\Gamma \vdash \I \isLinTy}\and
%DIFDELCMD <     \inferrule{\Gamma \vdash A \isLinTy \and \Gamma \vdash B \isLinTy}{\Gamma \vdash A \otimes B \isLinTy}\and
%DIFDELCMD <     \inferrule{\Gamma \vdash A \isLinTy \and \Gamma \vdash B \isLinTy}{\Gamma \vdash A \lto B \isLinTy}\and
%DIFDELCMD <     \inferrule{\Gamma \vdash A \isLinTy \and \Gamma \vdash B \isLinTy}{\Gamma \vdash A \tol B \isLinTy}\and
%DIFDELCMD <     \inferrule{\Gamma,x:X \vdash A \isLinTy}{\Gamma \vdash \bigoplus\limits_{x:X} A \isLinTy}\and
%DIFDELCMD <     \inferrule{\Gamma,x:X \vdash A \isLinTy}{\Gamma \vdash \bigamp\limits_{x:X} A \isLinTy}\and
%DIFDELCMD <     \inferrule
%DIFDELCMD <     {\Gamma \vdash f : \ltonl {(A \lto B)} \and \Gamma \vdash g : \ltonl { (A \lto B) }}
%DIFDELCMD <     {\Gamma \vdash \equalizer {a}{f}{g} \isLinTy}
%DIFDELCMD <   \end{mathpar}
%DIFDELCMD < \begin{mathpar}
%DIFDELCMD <     %%%
\DIFdelendFL \DIFaddbeginFL \boxed{
      \inferrule
      {\linctxwff \Gamma \Delta \and \linctxwffjdg \Gamma \A}
      {\linterm \Gamma \Delta {e} {A}}}

    \DIFaddendFL \inferrule{~}{\Gamma ; a : A \vdash a : A}
    \and
    \DIFaddbeginFL \inferrule{\nonlinterm \Gamma {M} {\ltonl A}}{\linterm {\Gamma} {\cdot} {M} {A}}
    \and
    \DIFaddendFL \inferrule{\Gamma ; \Delta \vdash e : B \\ \linctxwffjdg \Gamma {A \equiv B}}{\Gamma ; \Delta \vdash e : A}
    %
    \\
    %
    \inferrule{~}{\Gamma ; \cdot \vdash () : I}
    \and
    \inferrule{\Gamma ; \Delta_2 \vdash e : I \\ \Gamma ; \Delta_1,\Delta_3 \vdash e' : C}{\Gamma ; \Delta_1,\Delta_2,\Delta_3 \vdash \letin {()} e {e'} : C}
    %
    \\
    %
    \inferrule{\Gamma ; \Delta \vdash e : A \\ \Gamma ; \Delta' \vdash e' : B}{\Gamma ; \Delta, \Delta' \vdash (e , e') : A \otimes B}
    %
    \and
    %
    \inferrule{\Gamma ; \Delta_2 \vdash e : A \otimes B \\ \Gamma ; \Delta_1, a : A, b : B, \Delta_2 \vdash e' : C}{\Gamma ;  \Delta_1, \Delta_2, \Delta_3 \vdash \letin {(a , b)} e {e'} : C}
    \\
    %
    \inferrule{\Gamma ; \Delta , a : A \vdash e : B}{\Gamma ; \Delta \vdash \lamblto a e : A\lto B}
    \and
    \DIFdelbeginFL %DIFDELCMD < \inferrule{\Gamma ; \Delta' \vdash e' : A \\ \Gamma ; \Delta \vdash e : A \lto B}{\Gamma ; \Delta, \Delta' \vdash \applto {e'} {e} : B}
%DIFDELCMD <     %%%
\DIFdelendFL \DIFaddbeginFL \inferrule{\Gamma ; \Delta \vdash e : A \lto B \\ \Gamma ; \Delta' \vdash e' : A} {\Gamma ; \Delta, \Delta' \vdash \applto {e} {e'} : B}
    \DIFaddendFL \\
    %
    \DIFaddbeginFL \inferrule{\Gamma ; a : A , \Delta \vdash e : B}{\Gamma ; \Delta \vdash \lambtol a e : B\tol A}
    \and
    \inferrule{\Gamma ; \Delta \vdash e : A \\ \Gamma ; \Delta' \vdash e'
      : B \tol A}{\Gamma ; \Delta, \Delta' \vdash \apptol {e'} {e} : B}
    \\
    %DIF > 
    \DIFaddendFL \inferrule{\Gamma, x : X ; \Delta  \vdash e : A}
              {\Gamma ; \Delta \vdash \dlamb x e : \LinPiTy x X A}
    \and
    \inferrule{\Gamma ; \Delta \vdash e : \LinPiTy x X A \\ \Gamma \vdash M : X}{\Gamma ; \Delta \vdash e\,.\pi\,M : \subst A {M} x}
    %
    \\
    %
    \inferrule{\Gamma \vdash M : X \quad \Gamma ; \Delta \vdash e : \subst A M x}{\Gamma ; \Delta \vdash \sigma\,M\,e : \bigoplus\limits_{x:X} A}
    %
    \and
    %
    \inferrule{\Gamma ; \Delta_2 \vdash e : \bigoplus\limits_{x:X} A \quad \Gamma, x : X ; \Delta_1, a : A, \Delta_3 \vdash e' : C}{\Gamma; \Delta_1, \Delta_2, \Delta_3 \vdash \letin {\sigma\,x\,a} e {e'}: C}
    %
    \DIFdelbeginFL %DIFDELCMD < \and
%DIFDELCMD <     %%%
\DIFdelendFL \DIFaddbeginFL \\
    \DIFaddendFL %
    \inferrule
    { \Gamma ; \Delta \vdash e : A \\
      \Gamma ; \Delta \vdash \applto {f}{e} \equiv \applto {g}{e}}
    {\Gamma ; \Delta \vdash \equalizerin{e} : \equalizer {a}{f}{g}}
    %
    \and
    %
    \inferrule
    {\Gamma ; \Delta \vdash e : \equalizer{a}{f}{g}}
    {\Gamma ; \Delta \vdash \equalizerpi {e} : A}
  \end{mathpar}
  \DIFdelbeginFL %DIFDELCMD < \end{footnotesize}
%DIFDELCMD <   %%%
\DIFdelendFL \caption{Linear \DIFdelbeginFL \DIFdelFL{types and }\DIFdelendFL terms\DIFdelbeginFL \DIFdelFL{(selection)}\DIFdelendFL }
  \DIFdelbeginFL %DIFDELCMD < \label{fig:linear-types-and-terms}
%DIFDELCMD < %%%
\DIFdelendFL \DIFaddbeginFL \label{fig:linear-terms}
\DIFaddendFL \end{figure}

\subsection{Indexed Inductive Linear Types}

Next, we introduce the most complex and important linear type
constructors of our development, \emph{indexed inductive linear
types}. We encode these by adding a mechanism for constructing initial
algebras of strictly positive functorial type expressions, following
prior work on inductive types
\cite{nakov_quantitative_2022,altenkirch_indexed_2015}. The syntax is
given in \Cref{fig:iilt}. First, we add a non-linear type $\SPF\,X$ of
\emph{strictly positive functorial} linear type expressions indexed by
a non-linear type $X$. We think of the elements of this type as
syntactic descriptions of linear types that are parameterized by
$X$-many variables standing for linear types that are only used in
strictly positive positions. Accordingly, the $\SPF\,X$ type supports
an operation $\el$ that interprets it as such a type constructor, as
well as an operator $\map$ that defines a functorial action on parse
transformers. The $\SPF\,X$ type supports constructors for a reference
$\Var~x$ to one of the linear type variables, a constant expression
that doesn't mention any type variables $K$, as well as tensor
products and additive conjunction and disjunction of type expressions.
We additionally add equations in the appendix that say that the
$\el$/$\map$ operations correspond to these descriptions of the
constructors.

\begin{figure}
  \begin{footnotesize}
    \begin{mathpar}
      \inferrule{\Gamma \vdash X\isTy}{\Gamma \vdash \SPF\,X \isTy}\and
      \inferrule{\Gamma \vdash X\isSmall}{\Gamma \vdash \SPF\,X \isSmall}\and
    \el : \prod_{X:U}\SPF\,X \to (X \to L) \to L\and
    \map : \prod_{X:U}\prod_{F : \SPF\,X}\prod_{A,B:X\to L}{\left(\prod_{x:X}\uparrow(\unquoteTy{A\,x}\lto \unquoteTy{B\,x})\right)} \to {\uparrow(\unquoteTy{\el(F)(A)} \lto \unquoteTy{\el(F)(B)})}\and
    \mathsf{Var} : \prod_{X:U} X \to \SPF\,X\and
    \mathsf{K} : \prod_{X:U} L \to \SPF\,X\and
    \mathsf{\bigoplus} : \prod_{X:U}\prod_{Y:U}(Y \to \SPF\,X) \to \SPF\,X\and
    \mathsf{\bigamp} : \prod_{X:U}\prod_{Y:U}(Y \to \SPF\,X) \to \SPF\,X\and
    \mathsf{\otimes} : \prod_{X:U}\SPF\,X \to \SPF\,X \to \SPF\,X\and
    \roll : \prod_{X:U}\prod_{F:X \to \SPF\,X}\prod_{x:X}\ltonl{(\el(F\,x)(\mu\,F))}\and
    \fold : \prod_{X:U}\prod_{F:X\to\SPF\,X}\prod_{A:X \to L}
    \left(\prod_{x:X}\ltonl{(\unquoteTy{\el(F\,x)(\unquoteTy{A})} \lto \unquoteTy{A\,x})}\right)
    \to \prod_{x:X}\ltonl{(\mu F\,x \lto A\,x)}\and

    \DIFdelbeginFL %DIFDELCMD < \inferrule*[right=Ind$\beta$]
%DIFDELCMD <     {\Gamma \vdash f : \prod_{x:X}\ltonl{(\el(F\,x)(A) \lto A\,x)} \and \Gamma; \Delta \vdash e : \el(F\,x)(\mu\,F)}
%DIFDELCMD <     {\Gamma;\Delta \vdash \fold\,F\,f\,x\,(\roll e) \equiv f\,x\,(\map(F\,x)\,(\fold\,F\,f)) : A\,x}
%DIFDELCMD < %%%
\DIFdelendFL \DIFaddbeginFL \inferrule*[right=Ind$\beta$]
    {\Gamma \vdash f : \prod_{x:X}\ltonl{(\el(F\,x)(A) \lto A\,x)} \and \Gamma; \Delta \vdash e : \el(F\,x)(\mu\,F)}
    {\Gamma;\Delta \vdash \fold\,F\,f\,x\,(\roll\,e) \equiv f\,x\,(\map(F\,x)\,(\fold\,F\,f)) : A\,x}
\DIFaddendFL 

    \DIFdelbeginFL %DIFDELCMD < \inferrule*[right=Ind$\eta$]
%DIFDELCMD <     {\Gamma \vdash f : \prod_{x:X}\ltonl{(\el(F\,x)(A) \lto A\,x)}
%DIFDELCMD <      \and \Gamma \vdash e : \prod_{x:X}\ltonl{(\mu F\,x \lto A\,x)}\\\\
%DIFDELCMD <       \Gamma,x:X;a:\el(F\,x)(\mu F) \vdash e\,x\,(\roll a) \equiv f\,x\,(\map(F\,x)\,e) : A\,x }
%DIFDELCMD <     {\fold\,F\,f\equiv e' : \prod_{x:X}\ltonl{(\mu F\,x \lto A\,x)}}
%DIFDELCMD <   %%%
\DIFdelendFL \DIFaddbeginFL \inferrule*[right=Ind$\eta$]
    {\Gamma \vdash f : \prod_{x:X}\ltonl{(\el(F\,x)(A) \lto A\,x)}
     \and \Gamma \vdash e : \prod_{x:X}\ltonl{(\mu F\,x \lto A\,x)}\\\\
      \Gamma,x:X;a:\el(F\,x)(\mu F) \vdash e\,x\,(\roll\,a) \equiv f\,x\,(\map(F\,x)\,e) : A\,x }
    {\fold\,F\,f\equiv e' : \prod_{x:X}\ltonl{(\mu F\,x \lto A\,x)}}
  \DIFaddendFL \end{mathpar}
  \end{footnotesize}
  %% \begin{footnotesize}
  %%   \begin{align*}
  %%   \mathsf{Var} &: \prod_{X:U} X \to \SPF\,X\\
  %%   \mathsf{K} &: \prod_{X:U} L \to \SPF\,X\\
  %%   \mathsf{\bigoplus} &: \prod_{X:U}\prod_{Y:U}(Y \to \SPF\,X) \to \SPF\,X\\
  %%   \mathsf{\bigamp} &: \prod_{X:U}\prod_{Y:U}(Y \to \SPF\,X) \to \SPF\,X\\
  %%   \mathsf{\otimes} &: \prod_{X:U}\SPF\,X \to \SPF\,X \to \SPF\,X\\
  %%   \el &: \prod_{X:U}\SPF\,X \to (X \to A) \to L\\
  %%   \map &: \prod_{X:U}\prod_{F : \SPF\,X}\prod_{A,B:X\to L}{\left(\prod_{x:X}\uparrow(\unquoteTy{A\,x}\lto \unquoteTy{B\,x})\right)} \to {\uparrow(\unquoteTy{\el(F)(A)} \lto \unquoteTy{\el(F)(B)})}\\\\
  %%   \roll &: \prod_{X:U}\prod_{F:X \to \SPF\,X}\prod_{x:X}\ltonl{(\el(F\,x)(\mu\,F))}\\
  %%   \fold &: \prod_{X:U}\prod_{F:X\to\SPF\,X}\prod_{A:X \to L}
  %%   \left(\prod_{x:X}\ltonl{(\el(F\,x)(A) \lto A\,x)}\right)
  %%   \to \prod_{x:X}\ltonl{(\mu F\,x \lto A\,x)}
  %% \end{align*}
  %% \end{footnotesize}
  \caption{Strictly positive functors and indexed inductive linear types}
  \label{fig:iilt}
\end{figure}

Next, given a family of $X$-many strictly positive linear type
expressions $F : X \to \SPF\,X$, we define a family $\mu F : X \to L$
of $X$-many mutually recursive inductive types. The introduction rule
for this is $\roll$, which constructs an element of $\mu F\,x$ from
the one-level of the $x$th type expression. The elimination principle
is defined by a mutual $\fold$ operation: given a family of output
types $A$ indexed by $X$, we can define a family of functions from
$\mu F\,x \multimap A\,x$ if you specify how to interpret all of the
constructors as operations on $A$ values. We add $\beta\eta$ equations
that specify that this makes the family $\mu F$ into an \emph{initial
algebra} for the functor $\el(F)$. That is the $\beta$ rule says that
a $\mathsf{fold}$ applied to a $\mathsf{roll}$ is equivalent to
$\mathsf{map}$ping the $\mathsf{fold}$ over all the sub-expressions,
which means that $\mathsf{fold}$ interprets all of the constructors
homomorphically using the provided interpretation $\mathsf{f}$. Then
the $\eta$ rule says that $\mathsf{fold}$ is the \emph{unique} such
homomorphism, i.e. anything that satisfies the recurrence equation of
the $\mathsf{fold}$ is equal to it.

This definition as an initial algebra is well-understood semantically
but the $\eta$ principle in particular is somewhat cumbersome to use
directly in proofs. In dependent type theory, we would have a
dependent \emph{elimination} principle, which can be used to
implement functions by recursion as well as proofs by
induction. Unfortunately, since linear types do not support dependency on
linear types, we cannot directly adapt this approach. However, if we
are trying to prove that two morphisms out of a mutually recursive
type are equal, we can use the \emph{equalizer} type to prove their
equality by induction. That is, if our goal is to prove two functions
$f,g : \uparrow(\mu F\,x \lto A\,x)$ equal, it suffices to
implement a function $\mathsf{ind} : \uparrow(\mu F\,x \lto \equalizer
{a} f g)$ such that $\textrm{ind}(a) \equiv a$. Then an
inductive-style proof can be implemented by constructing
$\mathsf{ind}$ using a $\fold$. This can all be justified using only
the $\beta\eta$ principles for equalizers and inductive types, and
this is how our most complex inductive proofs are implemented in the Agda
formalization.
\pedro{I don't like the inductive proof explanation above, but I
  don't know how I would phrase it}

\subsection{Grammar-specific Additions}

So far, our calculus is a somewhat generic combination of dependent
types with non-commutative linear types. In order to carry out formal
grammar theory and define parsers, we need only add a few
grammar-specific constructions. First for each character in our
alphabet, we add a corresponding linear type $c$. We can then define a
non-linear type \DIFdelbegin \DIFdel{$\Char$ }\DIFdelend \DIFaddbegin \DIFadd{$\CharGram$ }\DIFaddend as the disjunction of all of these characters,
and define a type \DIFdelbegin \DIFdel{$\String$ }\DIFdelend \DIFaddbegin \DIFadd{$\StringGram$ }\DIFaddend as the Kleene star of \DIFdelbegin \DIFdel{$\Char$}\DIFdelend \DIFaddbegin \DIFadd{$\CharGram$}\DIFaddend , i.e. as an
inductive linear type.
Then we add a function \DIFdelbegin \DIFdel{$\mathsf{read} :
\ltonl{(\top\lto \String)}$ }\DIFdelend \DIFaddbegin \DIFadd{$\mathsf{read} :
\ltonl{(\top\lto \StringGram)}$ }\DIFaddend that intuitively ``reads'' the input
string from the input and makes it available. It is important that the
input type of $\mathsf{read}$ is $\top$, which can control any amount
of resources, and not $\I$ which controls no resources.
\DIFdelbegin \DIFdel{Further, we
add an axiom that }\DIFdelend \DIFaddbegin 


\begin{axiom}
  \label{ax:string-top}
   \DIFaddend $\lambda s. \mathsf{read}(!(s)) \equiv \lambda
s. s$ where $!$ is the unique function \DIFdelbegin \DIFdel{$\ltonl(\String\lto \top)$, i.e., that if }\DIFdelend \DIFaddbegin \DIFadd{$\ltonl(\StringGram\lto \top)$. That is, $\StringGram$ is strongly equivalent to $\top$.
}\end{axiom}

\DIFadd{If }\DIFaddend we have a string, but then throw it away and read
it from the input, then \DIFdelbegin \DIFdel{in fact that is equivalent to the string we
were given originally}\DIFdelend \DIFaddbegin \DIFadd{we, in fact, recover the original string}\DIFaddend .
This ensures that the elements of the \DIFdelbegin \DIFdel{$\String$
}\DIFdelend \DIFaddbegin \DIFadd{$\StringGram$
}\DIFaddend type always stand for the actual input string in our reasoning. In the
next section, we will show how these basic principles are enough to
provide a basis for verified parsing and formal grammar theory.

% \steven{TODO the intro to this section is to be condensed based on what is
%   included in the examples section as motivation and intuition building}

% Omission of the structural rules of a deductive system, such as in
% linear logic \cite{GIRARD19871}, offers precise control over how a value is used
% in a derivation. Linear logic omits the weakening and contraction rules
% to ensure that every value in context is used exactly once. This control enables
% \emph{resource-sensitive} reasoning, where we may treat a resource as
% \emph{consumed} after usage. This viewpoint is amenable to parsing applications,
% as we may treat characters of a string as finite resources that are consumed at
% parse-time. That is, when reading a string, the occurrence of any character is read once. Freely duplicating or dropping characters from a string changes the meaning of that string. \max{what about a backtracking parser?}. One may then envision a linear type system where the types comprise
% formal grammars generated over some alphabet $\Sigma$, and the type constructors
% correspond precisely to inductive constructions on grammars --- such as
% conjunction, disjunction, concatenation, etc.

% Programming in a purely linear term language is limiting due to the variable
% usage restrictions. Code in such a language can
% become unnecessarily verbose when ensuring the linearity invariant.
% To alleviate this
% concern, in 1995 Benton et al.\ proposed an alternative categorical
% semantics of linear logic based on adjoint interactions between linear
% and non-linear logics \cite{bentonMixedLinearNonlinear1995} ---
% appropriately referred to as a \emph{linear-non-linear} (LNL)
% system. This work is simply typed, so the boundary between linear and
% non-linear subtheories is entirely characterized via a monoidal
% adjunction between linear terms and non-linear terms.

% \steven{Describe this monoidal adjunction with $\ltonl A$ and the derivable
%   version of $F$}

% Inside of an LNL system, linearity may be thought of as an option that users can
% choose to deliberately invoke at deliberate points in their developments in an
% otherwise intuitionistic system. However, if we are wishing to treat parsers as
% linear terms over input strings, the non-linear fragment of an LNL theory does
% not really assist in the development of parsers. It is instead the case that
% parsers may benefit from a \emph{dependence} on non-linear terms.
% Through the approach described by Krishnaswami et al.\ in
% \cite{krishnaswami_integrating_2015},
% we may define a restricted form of dependent types.

% In particular, dependence
% on linear terms
% is disallowed; however, through dependence of a linear term on a non-linear
% index, we can recover the definition of Aho's indexed grammars \cite{AhoIndexed}
% internal to \theoryabbv. Although, we can define strictly more things, as Aho's
% indexed grammars provide restricted access to the index whereas we can do
% whatever with it.

% \steven{elaborate on indexed grammars in a more articulate way in the last sentence}
% \steven{Put the aho stuff in the subtheory section}
%

% \paragraph{Regular Expressions}
% We may realize regular expression as a subtheory
% of our language by restricting the type constructors to the
% linear unit, characters, $\otimes$, $\oplus$, and Kleene star. Classically, the
% languages recognized by these are exactly the regular languages --- the lowest
% on the Chomsky hierarchy.

% \paragraph{$\mu$-Regular Expressions} Similarly, we may restrict the connectives
% to be linear unit, characters, $\otimes$, $\oplus$, and arbitrary recursion via
% left-fixed point rather than only Kleene star.
% Instead of regular expressions these correspond to Lei{\ss}'s $\mu$-regular
% expressions, which are known to be equivalent to context-free grammars
% \cite{leiss,krishnaswami_typed_2019}.

% \paragraph{Beyond Context-Free Grammars} The previous two subtheories induce as semantics
% regular grammars and context-free grammars, respectively; however, by including
% the LNL dependent types we may actually express the entirety of the Chomsky
% hierarchy. Through use of the LNL dependent types, we may encode indexed
% grammars, which are known to be properly between context-free and
% context-sensitive grammars within the Chomsky hierarchy \cite{AhoIndexed}. We
% will further see in \cref{subsubsec:tm} how to use dependence to encode Turing machines internal to our
% calculus, which of course induces a semantics of unrestricted grammars.

% \paragraph{Indexed Grammars}
% The LNL dependent types can be used to encode Aho's indexed grammars
% \cite{AhoIndexed} internal to \theoryabbv. Although, we can define strictly more
% things with this non-linear index, as we have unrestricted access to the index
% whereas Aho's grammars can only manipulate it in a particular way.

% \steven{Elaborate on these, and note that they are bw context free and context
%   sensitive on the Chomsky hierarchy}

\section{Formal Grammar Theory in \theoryname}
\label{sec:applications}
This section explores the applications of \theoryabbv to conducting formal
grammar theory. We demonstrate that several classical notions and constructions
integral to the theory of formal languages are faithfully represented
in \theoryname. By encoding well-established formal grammar concepts, we ensure
that our framework remains grounded in the
foundational principles of formal language theory while opening the door to
compositional formal verification of parsers.

%% \paragraph{Weak and Strong Equivalence}
In the theory of formal grammars, there are two different notions of
equivalence: up to weak generative capacity, meaning just which strings are
accepted by the grammar; and up to \emph{strong} generative capacity, when
the parse trees of the two grammars are isomorphic
\cite{chom1963}. Using linear types as grammars, we can define both of
these notions of equivalence in \theoryabbv.
\DIFaddbegin 

\DIFaddend \begin{definition}
  \label{def:weakequiv}
  Grammars $\A$ and $\B$ are \emph{weakly equivalent} if there \DIFdelbegin \DIFdel{exists
  }\DIFdelend \DIFaddbegin \DIFadd{exist
  }\DIFaddend parse transformers $\f : \ltonl{(\A \lto \B)}$ and $\g : \ltonl {(\B
    \lto \A)}$. $\A$ is a \emph{retract} of $\B$ if \DIFdelbegin \DIFdel{$\A$ and $\B$ }\DIFdelend \DIFaddbegin \DIFadd{they }\DIFaddend are
  weakly equivalent and $\lambda a. g(f(a)) \equiv \lambda a.a$. They
  are \emph{strongly equivalent} if further the other composition is
  the identity, i.e., $\lambda b. f(g(b)) \equiv \lambda b.b$.
\end{definition}

%% That is, grammars are weakly equivalent if we can construct a parse
%% tree of $B$ from a parse tree of $A$ and vice-versa, whereas they are
%% strongly equivalent if these processes are inverse to each other. The
%% definition of retract is a useful intermediate notion. In retrospect,
%DIF < % our axiom about $\String$ says that $\String$ is a retract of $\top$.
%DIF > % our axiom about $\StringGram$ says that $\StringGram$ is a retract of $\top$.

%% \newcommand{\lang}{\texttt{Lang}}
%% A parse transformer $\f : \ltonl{(\A \lto \B)}$ takes as input parses of $\A$ for string
%% $\w$ and returns a parse of $\B$ for $\w$. Therefore, $\f$ demonstrates an
%% inclusion of languages $\lang(\A) \subset \lang(\B)$. Similarly, the existence of
%% $\g$ shows that $\lang(\B) \subset \lang(\A)$, and thus $\lang(\A) = \lang(\B)$.

%% \steven{Need better notation for language that is cohesive throughout the paper}

%% \begin{definition}
%%   \label{def:strongequiv}
%%   Grammars $\A$ and $\B$ are \emph{strongly equivalent} if there exists parse
%%   transformers $\f : \ltonl{(\A \lto \B)}$ and $\g : \ltonl {(\B \lto \A)}$ such
%%   that $\f \circ \g \equiv \ident$ and $\g \circ \f \equiv \ident$.
%% \end{definition}

%% For a string $\w$, \cref{def:strongequiv} induces a bijection between the
%% $\A$-parses of $\w$ and the $\B$-parses of $\w$, thus demonstrating that $\A$
%% and $\B$ agree in their strong generative capacity.

%% \paragraph{Ambiguity}
A formal grammar $\A$ is ambiguous if there are multiple parse trees
for the \emph{same} string $\w$.  For example, $\a \oplus \a$ is
ambiguous because there are two parses of $\stringquote{a}$,
constructed using $\inl$ and $\inr$. On the other hand, a formal
grammar is unambiguous when there is at most one parse tree for any
input string. We can capture this notion as a type in \theoryabbv in a
clever way:
\begin{definition}
  \label{def:unambig}
  A grammar $\A$ is \emph{unambiguous} if for every linear type $\B$,
  $\f : \ltonl{(\B \lto \A)}$, and $\g : \ltonl{(\B \lto \A)}$
  then $\f \equiv \g$.
\end{definition}
\cref{def:unambig} can be read more intuitively as stating that $\A$
is unambiguous if there is at most one way to transform parses of any
other grammar $\B$ into \DIFdelbegin \DIFdel{parse }\DIFdelend \DIFaddbegin \DIFadd{parses }\DIFaddend of $\A$. This notion of an unambiguous
type is the analog for linear types of the definition of a (homotopy)
\emph{proposition} in the terminology of homotopy type
theory\cite{hottbook}. The most basic unambiguous types are $\top$ and
$0$, and in a system of classical logic all unambiguous types would
have to be equivalent to one of these, but with our axioms we can show
also that $\I$ and literals $\literal c$ are unambiguous. To see this, first, we
establish two useful properties of unambiguity.
\DIFdelbegin %DIFDELCMD < \begin{lemma}[\agdalogo]
%DIFDELCMD <   %%%
\DIFdelend \DIFaddbegin \begin{lemma}[\Agda{\fillerlink}]
  \label{lem:retract}
  \DIFaddend If $\B$ is unambiguous and $\A$ is a retract of $\B$ then $\A$ is unambiguous.
\DIFdelbegin \DIFdel{If a disjunction $\bigoplus\limits_{x:X} A(x)$ }\DIFdelend \DIFaddbegin \end{lemma}
\begin{lemma}[\Agda{\fillerlink}]
  \label{lem:unambig-sum}
  \DIFadd{As a consequence of \mbox{%DIFAUXCMD
\cref{cor:binary-mono}}\hskip0pt%DIFAUXCMD
, if a binary disjunction $A \oplus B$ }\DIFaddend is unambiguous then
  \DIFdelbegin \DIFdel{each $A(x)$ is }\DIFdelend \DIFaddbegin \DIFadd{$A$ and $B$ are each }\DIFaddend unambiguous.
\end{lemma}
From \DIFdelbegin \DIFdel{the first principle}\DIFdelend \DIFaddbegin \DIFadd{\mbox{%DIFAUXCMD
\cref{lem:retract}}\hskip0pt%DIFAUXCMD
}\DIFaddend , we can prove that \DIFdelbegin \DIFdel{$\String$ }\DIFdelend \DIFaddbegin \DIFadd{$\StringGram$ }\DIFaddend is unambiguous,
since it is a retract of $\top$. In fact, observe that if $\A$ is a
retract of $\B$ and $\B$ is unambiguous, then in fact $\A$ and $\B$
are strongly equivalent, as the equation $\lambda b. f(g(b)) \equiv
\lambda b. b$ follows because $\B$ is unambiguous. Therefore
\DIFdelbegin \DIFdel{$\String$ }\DIFdelend \DIFaddbegin \DIFadd{$\StringGram$ }\DIFaddend is also strongly equivalent to $\top$. Next, since \DIFdelbegin \DIFdel{$\String$ }\DIFdelend \DIFaddbegin \DIFadd{$\StringGram$ }\DIFaddend is
defined as a Kleene star, we can easily show that \DIFdelbegin \DIFdel{$\String \cong \I
\oplus \Char \oplus (\Char \otimes \Char \otimes \String)$}\DIFdelend \DIFaddbegin \DIFadd{$\StringGram \cong \I
\oplus \CharGram \oplus (\CharGram \otimes \CharGram \otimes \StringGram)$}\DIFaddend . Then by
\DIFdelbegin \DIFdel{the second principle }\DIFdelend \DIFaddbegin \DIFadd{\mbox{%DIFAUXCMD
\cref{lem:unambig-sum}}\hskip0pt%DIFAUXCMD
, }\DIFaddend we have that $\I$ and
\DIFdelbegin \DIFdel{$\Char$ and }\DIFdelend \DIFaddbegin \DIFadd{$\CharGram$ --- and thus }\DIFaddend each literal
$\literal c$ \DIFaddbegin \DIFadd{--- }\DIFaddend are unambiguous as well.

% \paragraph{A Grammar of Strings}
%DIF <  Define the character grammar $\charg$ to be the disjunction over characters in the alphabet $\Sigma$,
%DIF <  $\charg = \LinSigTy {c} {\Sigma} c$. Then define a linear type of strings as a
%DIF >  Define the character grammar $\CharGram$ to be the disjunction over characters in the alphabet $\Sigma$,
%DIF >  $\CharGram = \LinSigTy {c} {\Sigma} c$. Then define a linear type of strings as a
% linear list of characters,
%DIF <  $\stringg = \charg^{*}$.
%DIF >  $\StringGram = \CharGram^{*}$.

% In the grammars model, we can build a
%DIF <  strong equivalence between $\top$ and $\stringg$. hen building parsers, we may wish to enforce this
%DIF <  syntactically by including $\top \cong \stringg$ as an axiom.
%DIF >  strong equivalence between $\top$ and $\StringGram hen building parsers, we may wish to enforce this
%DIF >  syntactically by including $\top \cong \StringGram$ as an axiom.

%DIF <  Through inclusion of this axiom, $! : \stringg \to \top$ is an isomorphism, and
%DIF <  thus also a monomorphism. That is, $\stringg$ is unambiguous.
%DIF >  Through inclusion of this axiom, $! : \StringGram \to \top$ is an isomorphism, and
%DIF >  thus also a monomorphism. That is, $\StringGram$ is unambiguous.
% \max{we should have a section of the previous section which gives the axioms, this one and the distributivity axiom}

% \paragraph{Parseability}
% We say $A$ is \emph{parseable} if there exists a grammar $B$ and a map
%DIF <  $\top \to A \oplus B$. Taking $\top \cong \stringg$, a grammar $A$ is parseable
%DIF >  $\top \to A \oplus B$. Taking $\top \cong \StringGram$, a grammar $A$ is parseable
% exactly when we may read an arbitrary string into a parse of $A$ or a $B$-valued
% refutation.

% \paragraph{Decidability}
% In the case that $A$ is parseable with respect to $\neg A$ --- i.e.\
% there's a map $\top \to A \oplus \neg A$ --- then call $A$ \emph{decidable}.

%% \paragraph{Partial and Total Parsers}

We now turn to our main task, which is using our linear type system to
implement verified parsers. Given a grammar defined as a linear type
$A$, a first attempt at defining a parser would be to implement a
function \DIFdelbegin \DIFdel{$\uparrow{(\String \lto A)}$}\DIFdelend \DIFaddbegin \DIFadd{$\uparrow{(\StringGram \lto A)}$}\DIFaddend . But since our linear functions
must be total, this means that we can construct an $A$ parse for
\emph{every} input string, which is impossible for most grammars of
interest. Instead we might try to write a partial function as a
\DIFdelbegin \DIFdel{$\ltonl{(\String \lto (A \oplus \top))}$ }\DIFdelend \DIFaddbegin \DIFadd{$\ltonl{(\StringGram \lto (A \oplus \top))}$ }\DIFaddend using the ``option''
monad. This allows for the possibility that the input string doesn't
parse, but is far too weak as a specification: we can trivially
implement a parser for any type by always returning $\inr$. The
correct notion of a parser should be one that allows for failure, but
only in the case that a parse cannot be constructed.
\DIFaddbegin 

\DIFaddend \begin{definition}
  \DIFaddbegin \label{def:disjoint}
  \DIFadd{Linear types $A$ and $B$ are }\emph{\DIFadd{disjoint}} \DIFadd{if there is a function
  $\ltonl {\left( A \& B \lto 0 \right)}$.
}\end{definition}

\begin{definition}
  \label{def:parser}
  \DIFaddend A parser for a linear type $A$ is \DIFdelbegin \DIFdel{a function $\uparrow{(\String \lto
    A \oplus A_{\neg})}$, where }\DIFdelend \DIFaddbegin \DIFadd{the choice a type }\DIFaddend $A_{\neg}$ \DIFdelbegin \DIFdel{is a linear type that is
  }\emph{\DIFdel{disjoint}} %DIFAUXCMD
\DIFdelend \DIFaddbegin \DIFadd{disjoint }\DIFaddend from
  $A$ \DIFdelbegin \DIFdel{in that we can implement a function $\uparrow{(A \& A_{\neg} \lto 0)}$}\DIFdelend \DIFaddbegin \DIFadd{and function $\uparrow{(\StringGram \lto
    A \oplus A_{\neg})}$}\DIFaddend .
\end{definition}
Here we replace $\top$ in our partial parser type with a type
$A_{\neg}$ that we can think of as a negation of $A$. The function
$\uparrow{(A \& A_{\neg} \lto 0)}$ ensures that it is impossible for
$A$ and $A_{\neg}$ to \DIFdelbegin \DIFdel{match }\DIFdelend \DIFaddbegin \DIFadd{parse }\DIFaddend the same input string. This means that in
defining a parser, we will need to define a kind of negative grammar
for strings that do not parse.
\DIFaddbegin 

\DIFaddend Fortunately, we will see that
deterministic automata naturally support such a notion with no
additional effort: the negative grammar is simply the grammar for
traces that end in a rejecting state. This follows from the following
principle, a consequence of \DIFdelbegin \DIFdel{our axiom that $\oplus$ constructors }\DIFdelend \DIFaddbegin \DIFadd{\mbox{%DIFAUXCMD
\cref{ax:disjointness}}\hskip0pt%DIFAUXCMD
.
}

\begin{lemma}[\Agda{\fillerlink}]
  \label{lem:unambig-to-disjoint}
  \DIFadd{If $\LinSigTy{x}{X}{A\,x}$ is unambiguous, then for $x \neq x'$, $A\,x$ and $A\,x'$ }\DIFaddend are
  disjoint. \DIFdelbegin %DIFDELCMD < \begin{lemma}[\agdalogo]
%DIFDELCMD <   %%%
\DIFdel{If }\DIFdelend \DIFaddbegin \DIFadd{In particular, if the binary product }\DIFaddend $A \oplus A_{\neg}$ is unambiguous, then
  $A$ and $A_{\neg}$ are disjoint.
\end{lemma}

\DIFaddbegin \begin{proof}
\DIFadd{If $\LinSigTy{x}{X}{A\,x}$ is unambiguous, then all functions into it from
$A\,x \& A\,x'$ are equal. In particular,
$\sigma\,x\,\circ\,\pi_{1} \equiv \sigma\,x'\,\circ\,\pi_{2}$ so there is a
function \(
\ltonl{\left(A\,x \& A\,x' \lto
  \equalizer{b}{\left( \sigma\,x \circ \pi_1 \right)}{\left(  \sigma\,x'\circ \pi_2 \right)}
\right)}.
\)
We then compose with the function in \mbox{%DIFAUXCMD
\cref{ax:disjointness} }\hskip0pt%DIFAUXCMD
to prove that $A\,x$ and $A\,x'$ are disjoint.
}\end{proof}

\DIFaddend Writing a parser as a linear term \DIFdelbegin \DIFdel{in this way is an intrinsic
verification of }\DIFdelend \DIFaddbegin \DIFadd{intrinsically verifies }\DIFaddend the \emph{soundness} of
the parser \DIFdelbegin \DIFdel{completely }\DIFdelend for free from the typing: any $\inl$ parse that we return \emph{must}
correspond to a parse tree of the input string. Further\DIFaddbegin \DIFadd{, }\DIFaddend if we verify the
disjointness property \DIFaddbegin \DIFadd{of \mbox{%DIFAUXCMD
\cref{def:parser} }\hskip0pt%DIFAUXCMD
}\DIFaddend we then also get the
\emph{completeness} of the parser as well, that \DIFdelbegin \DIFdel{it never fails to generate an $A$ parse when it is possible}\DIFdelend \DIFaddbegin \DIFadd{when the parser rejects the
input that there are no valid parses}\DIFaddend .

Our main method for constructing verified parsers is to show that a
grammar $A$ is weakly equivalent to a grammar for a deterministic
automaton. Parsers for deterministic automata are simple to implement
by stepping through the states of the automaton, with the rejecting
traces serving as the negative grammar. This is sufficient due to the
following:
\DIFdelbegin %DIFDELCMD < \begin{lemma}[\agdalogo]
%DIFDELCMD <   %%%
\DIFdelend \DIFaddbegin \begin{lemma}[\Agda{\fillerlink}]
  \DIFaddend \label{lem:wk-eqv-parse}
  If $\A$ is weakly equivalent to $\B$ then any parser for $\A$ \DIFdelbegin \DIFdel{can be
  extended }\DIFdelend \DIFaddbegin \DIFadd{extends }\DIFaddend to a
  parser for $\B$.
\end{lemma}
Here we need both directions of the weak equivalence. We need $A \lto
B$ to extend the parser from \DIFdelbegin \DIFdel{$\String \lto A \oplus A_{\neg}$ to
$\String \lto B \oplus A_{\neg}$ }\DIFdelend \DIFaddbegin \DIFadd{$\StringGram \lto A \oplus A_{\neg}$ to
$\StringGram \lto B \oplus A_{\neg}$ }\DIFaddend but then we also need $B \lto A$ to
establish that $A_{\neg}$ is disjoint from $B$.

\subsection{Regular Expressions and Finite Automata}
%% 2. Regex, NFA, DFA

In this section, we describe how to construct an intrinsically
verified parser for regular expressions by compiling it to an NFA and
then a DFA. That is, for each regular expression $A$, we construct an
NFA $N(A)$ and a corresponding DFA $D(A)$ such that $A$ is strongly
equivalent to the traces of $N(A)$ and weakly equivalent to the
accepting traces of $D(A)$. Then we can easily construct a parser for
traces of $D(A)$ and combine this using \Cref{lem:wk-eqv-parse} to get
a verified \DIFdelbegin \DIFdel{regex }\DIFdelend \DIFaddbegin \DIFadd{regular expression }\DIFaddend parser.

\DIFdelbegin %DIFDELCMD < \newcommand{\states}{\mathtt{states}}
%DIFDELCMD < \newcommand{\labelt}{label}
%DIFDELCMD < \newcommand{\transitions}{\texttt{transitions}}
%DIFDELCMD < \newcommand{\epstransitions}{\epsilon\texttt{transitions}}
%DIFDELCMD < \newcommand{\isAcc}{\mathtt{isAcc}}
%DIFDELCMD < \newcommand{\init}{\mathtt{init}}
%DIFDELCMD < \newcommand{\src}{\mathtt{src}}
%DIFDELCMD < \newcommand{\dst}{\mathtt{dst}}
%DIFDELCMD < \newcommand{\pparse}{\mathtt{parse}}
%DIFDELCMD < \newcommand{\print}{\mathtt{print}}
%DIFDELCMD < \newcommand{\epssrc}{\epsilon\mathtt{src}}
%DIFDELCMD < \newcommand{\epsdst}{\epsilon\mathtt{dst}}
%DIFDELCMD < 

%DIFDELCMD < \newcommand{\N}{\texttt{N}}
%DIFDELCMD < 

%DIFDELCMD < %%%
\DIFdel{A regular expressions }\DIFdelend \DIFaddbegin \DIFadd{A regular expression }\DIFaddend in \theoryabbv is a linear type constructed
using only the connectives $\literal c$, $0$, $\oplus$, $I$,
$\otimes$, and Kleene star.  In \cref{sec:type-theory-examples}, we
saw one particular NFA and its corresponding type of traces. More
generally we define the linear type of traces as in
\Cref{fig:nfatrace}

%% The definition of the linear type of traces
%% through this
%% NFA, as
%% presented in \cref{fig:nfainductive}, inspects the concrete structure of the
%% NFA.\ When generalizing from this example to an arbitrary NFA, we do not have the
%% luxury of inspecting a concrete definition. Therefore, we abstract over the
%% specification of an arbitrary NFA and define traces with respect to that
%% specification.

%% Let the data of an NFA $\N$ be given by the following nonlinear data,
%% \begin{itemize}
%% \item $\N.\states$ --- a finite set of states
%% \item $\N.\init : \N.\states$ --- the initial state
%% \item $\N.\isAcc : \N.\states \to \Bool$ --- a mapping marking which states are
%%         accepting
%% \item $\N.\transitions$ --- a finite set of labeled transitions
%% \item $\N.\labelt : \N.\transitions \to \Sigma$ --- a mapping from $N.\transitions$ to the character labeling the transition
%% \item $\N.\src , \N.\dst : \N.\transitions \to \N.\states$ --- mappings from
%%         $\N.\transitions$ to the source and target states, respectively, of a labeled transition
%% \item $\N.\epstransitions$ --- a finite set of $\varepsilon$-transitions
%% \item $\N.\epssrc, \N.\epsdst : \N.\epstransitions \to \N.\states$ --- mappings
%%         from $\N.\epstransitions$ to the source and target states of an $\epsilon$-transition
%% \end{itemize}

In \cref{fig:nfatrace} we define a linear type of traces through an
NFA $\N$. $\Trace_{\N}$ is an inductive type indexed by the starting
state of the trace $\s : \N.\states$. Traces in $\N$ may be built
through one of three constructors. We may terminate a trace at an
accepting state with the constructor $\nil$. Here we use an Agda-style
\DIFdelbegin \DIFdel{unicode }\DIFdelend \DIFaddbegin \DIFadd{Unicode }\DIFaddend syntax for $\bigamp$, as well as using the function arrow to
mean a non-dependent version of $\bigamp$. If we had a trace beginning
at the destination state of a transition, then we may use the $\cons$
constructor to linearly combine that trace with a parse of the label
of the transition to build a trace beginning at the source of the
transition.  Finally, if we had a trace beginning at the destination
of an $\epsilon$-transition then we may use $\epscons$ to pull it back
along the $\epsilon$-transition and construct a trace beginning at the
source of the $\epsilon$-transition. As a shorthand, write $Parse_{N}$
for the accepting traces out of $N.init$.
\newcommand{\D}{\texttt{D}}
\begin{figure}
\DIFmodbegin
\begin{lstlisting}[alsolanguage=DIFcode]
data Trace(*@$_{\N}$@*) : (s : N.states) (*@$\to$@*) L where
%DIF <   nil : (*@$\uparrow$@*)(&[ s : N.states ] N.isAcc (*@$\to$@*) Trace(*@$_{\N}$@*) s)
%DIF >   nil : (*@$\uparrow$@*)(&[ s : N.states ] N.isAcc s (*@$\to$@*) Trace(*@$_{\N}$@*) s)
  cons : (*@$\uparrow$@*)(&[ t : N.transitions ]
    ('N.label t' (*@$\lto$@*) Trace(*@$_{\N}$@*) (N.dst t) (*@$\lto$@*) Trace(*@$_{\N}$@*) (N.src t)))
  (*@$\epsilon$@*)cons : (*@$\uparrow$@*)(&[ t : N.(*@$\epsilon$@*)transitions ]
    (Trace(*@$_{\N}$@*) (N.(*@$\epsilon$@*)dst t) (*@$\lto$@*) Trace(*@$_{\N}$@*)(N.(*@$\epsilon$@*)src t)))

data Trace(*@$_{\D}$@*) : (s : D.states) (b : Bool) (*@$\to$@*) L where
  nil : (*@$\uparrow$@*)(&[ s : D.states ] Trace(*@$_{\D}$@*) s D.isAcc s)
  cons : (*@$\uparrow$@*)(&[ c : Char ] &[ s : D.states ] &[ b : Bool ]
  ('c' (*@$\lto$@*) Trace(*@$_{\D}$@*) (D.(*@$\delta$@*) c s) b (*@$\lto$@*) Trace(*@$_{\D}$@*) s b))
\end{lstlisting}
\DIFmodend
\caption{Traces of an NFA $N$ and a DFA $D$}
\label{fig:nfatrace}
\end{figure}

%% \paragraph{Deterministic Finite Automata}
%% Just as the above defines an NFA, we provide a specification for an arbitrary
%% deterministic finite automaton.\ A DFA $D$ is
%% given by the following data,

%% \begin{itemize}
%% \item $\D.\states$ --- a finite set of states
%% \item $\D.\init : \D.\states$ --- the initial state
%%   \item $\D.\isAcc : \D.\states \to \Bool$ --- a mapping marking which states
%%         are accepting
%% \item $\D.\delta : \Sigma \to \D.\states \to \D.\states$ --- a deterministic transition function. Given a character $\c$ and a state
%%         $\s$, $\D.\delta~\c~\s$ is the state for which there is a transition $\s \overset{\stringquote{c}}{\to} (\D.\delta~\c~\s)$
%% \end{itemize}

%% \begin{figure}

%% \caption{Traces through the DFA $D$ as an indexed inductive type}
%% \label{fig:dfatrace}
%% \end{figure}

$\Trace_{\D}$, the linear type of traces through $\D$, is given
next. Unlike traces for an NFA, we parameterize this type additionally
by a boolean which says whether the trace is accepting or
rejecting. These traces may be terminated in an accepting state $\s$
with the $\nil$ constructor.  The $\cons$ constructor builds a trace
out of state $\s$ by linearly combining a parse of some character $\c$
with a trace out of the state $\D.\delta~\c~\s$. The trace built with
$\cons$ is accepting if and only if the trace out of $D.\delta~c~s$ is
accepting.

Because DFAs are deterministic, we are able to prove that their type
of traces are unambiguous and define a parser for them. In particular
we show that for any start state $s$, ${\LinSigTy {b} {\Bool}
  \Trace_{D}~s~b}$ is a retract of \DIFdelbegin \DIFdel{$\stringg$}\DIFdelend \DIFaddbegin \DIFadd{$\StringGram$ and apply \mbox{%DIFAUXCMD
\cref{lem:retract} }\hskip0pt%DIFAUXCMD
to
derive unambiguity}\DIFaddend . That is, first we
construct a function $\pparse_{D}$ that is a parser for
$\Trace_D~{s}~{true}$, with $\Trace_D~{s}~{false}$ being the disjoint
type used. The fact that these \DIFaddbegin \DIFadd{trace }\DIFaddend types are disjoint follows \DIFdelbegin \DIFdel{by showing
that this function is part of a retraction, i.e., that there is only
one way to trace through a deterministic automaton}\DIFdelend \DIFaddbegin \DIFadd{from the
unambiguity of $\LinSigTy {b} {\Bool} {\Trace_{D}~s~b}$ by \mbox{%DIFAUXCMD
\cref{lem:unambig-sum}}\hskip0pt%DIFAUXCMD
}\DIFaddend .

The parser,
$\pparse_{D}$,
is defined by recursion on strings in \cref{fig:printdfa}. If this string is empty, then $\pparse_{D}$
defines a linear function that terminates a trace at the input state $s$. If the
string is nonempty, then $\pparse_{D}$ walks forward in $D$ from the input state $s$ by
the character at the head of the string.
%
The inverse, $\print_{D}$ is defined by recursion on traces. If the trace is defined
via $\nil$, then $\print_{D}$ returns the empty string. Otherwise, if the trace
is defined by $\cons$ then $\pparse_{D}$ appends the character from the most
recent transition to the output
string and recurses.
%
We prove this is a retraction by induction on traces.

\DIFdelbegin %DIFDELCMD < \begin{theorem}[\agdalogo]
%DIFDELCMD <   %%%
\DIFdelend \DIFaddbegin \begin{theorem}[\Agda{\fillerlink}]
  \label{thm:dfa-parser}
  \DIFaddend $\pparse_D s$ is a parser for $\Trace_D~s~\true$.
\end{theorem}

\begin{figure}
\DIFmodbegin
\begin{lstlisting}[alsolanguage=DIFcode]
parse(*@$_D$@*) : (*@$\uparrow$@*)(String (*@$\lto$@*) &[ s : D.states ] (*@$\oplus$@*)[ b : Bool ] Trace(*@$_{\D}$@*) s b)
parse(*@$_D$@*) String.nil s = (*@$\sigma$@*) (D.isAcc s) (Trace(*@$_{\D}$@*).nil s)
%DIF < parse(*@$_D$@*) (String.cons ((*@$\sigma$@*) c a) w) s =
%DIF <    let (*@$\sigma$@*) b t = parse w (D.(*@$\delta$@*) c s) in
%DIF > parse(*@$_D$@*) (String.cons ((*@$\sigma$@*) c a) w) s = let (*@$\sigma$@*) b t = parse w (D.(*@$\delta$@*) c s) in
                                   (*@$\sigma$@*) b (Trace(*@$_{\D}$@*).cons c s b a t)

print(*@$_D$@*) : (s : D.states) (*@$\to$@*) (*@$\uparrow$@*)(((*@$\oplus$@*)[ b : Bool ] Trace(*@$_{\D}$@*) s b) (*@$\lto$@*) String)
print(*@$_D$@*) s ((*@$\sigma$@*) b (Trace(*@$_{\D}$@*).nil .s)) = String.nil
print(*@$_D$@*) s ((*@$\sigma$@*) b (Trace(*@$_{\D}$@*).cons c (D.(*@$\delta$@*) c .s) b a trace)) =
  String.cons ((*@$\sigma$@*) c a) (print(*@$_D$@*) (D.(*@$\delta$@*) c s) ((*@$\sigma$@*) b trace))
\end{lstlisting}
\DIFmodend
\caption{Parser/printer for DFA traces}
\label{fig:printdfa}
\end{figure}

Working backwards, we can then show the traces of an NFA are weakly
equivalent to the traces of a DFA implementing a variant of Rabin and
Scott's classic powerset construction
\cite{rabinFiniteAutomataTheir1959}. Here we note that this is
\emph{only} a weak equivalence and not a strong equivalence, as the
DFA is unambiguous even if the NFA is not.
\DIFdelbegin %DIFDELCMD < \begin{theorem}[Determinization, \Agda]
%DIFDELCMD <   \label{thm:determinization}
%DIFDELCMD <   %%%
\DIFdelend \DIFaddbegin \begin{construction}[Determinization, \Agda{\fillerlink}]
  \label{cons:determinization}
  \DIFaddend Given an NFA $N$, \DIFdelbegin \DIFdel{there exists }\DIFdelend \DIFaddbegin \DIFadd{we construct }\DIFaddend a DFA $D$ such that
  $Parse_{N}$ is weakly equivalent to $Parse_{D}$.
\DIFdelbegin %DIFDELCMD < \end{theorem}
%DIFDELCMD < %%%
\DIFdelend \DIFaddbegin \end{construction}
\DIFaddend \newcommand{\X}{\texttt{X}}
\newcommand{\Y}{\texttt{Y}}
\newcommand{\x}{\texttt{x}}
\newcommand{\y}{\texttt{y}}
\begin{proof}
  Define the states of $\D$ to be the \DIFdelbegin \DIFdel{$\mathbb{P}_{\varepsilon}(\N.\states)$ }\DIFdelend \DIFaddbegin \DIFadd{$\mathbb{P}_{\epsilon}(\N.\states)$ }\DIFaddend --- the type
  of $\epsilon$-closed\footnote{A subset of states $\X$ is $\epsilon$-closed if
    for every $\s \in \X$ and $\epsilon$-transition $\s \overset{\epsilon}{\to} \s'$ we have $\s' \in \X$.} subsets of
  $\N.\states$. A subset is accepting in $\D$ if it contains an accepting state
  from $\N$. Construct the initial state of $\D$ as the
  $\epsilon$-closure of $\N.\init$. Lastly define the transition function
  of $\D$ to send the subset $X$ under the character $\c$ to the
  $\epsilon$-closure of all the states reachable from $X$ via a transition
  labeled with the character $\c$.

  We demonstrate the weak equivalence between $Parse_{N}$ and
  $Parse_{D}$ by constructing parse transformers between the two
  grammars. To build the parse transformer
  $\ltonl{(Parse_{N} \lto Parse_{D})}$, we strengthen our inductive hypothesis \DIFaddbegin \DIFadd{to quantify over every start state
  }\DIFaddend and build a term
\newcommand{\NtoD}{\texttt{NtoD}}
\newcommand{\DtoN}{\texttt{DtoN}}
  \DIFdelbegin \begin{displaymath}%DIFAUXCMD
%DIFDELCMD < \[
%DIFDELCMD <     \NtoD %%%
\DIFdel{: \ltonl {\left(\Trace_{\N}~\s~\true \lto \bigwith\limits_{\X : \D.\states} \bigwith\limits_{\texttt{sInX} : \X \ni \s} \Trace_{\D}~\X~\true \right)}
  }
\end{displaymath}%DIFAUXCMD
%DIFDELCMD < \]
%DIFDELCMD <   %%%
\DIFdelend \DIFaddbegin \DIFadd{\(
    \NtoD : \ltonl {\left(\Trace_{\N}~\s~\true \lto \LinPiTy{\X}{\D.\states}{\LinPiTy{\texttt{sInX}}{\X \ni \s}{ \Trace_{\D}~\X~\true}} \right) }
  \)
  }\DIFaddend that maps a trace in $\N$ from an arbitrary state $\s$ to a trace in $\D$
  that may begin at any subset of states $\X$ that contains $\s$. $\NtoD$ may then be instantiated at
  $\s = \N.\init$ and $\X = D.\mathsf{init}$ to get the desired
  parse transformer.

%% \begin{figure}
%% \begin{lstlisting}
%% NtoD : (s : N.states) (*@$\to$@*)
%%        (*@$\uparrow$@*)(Trace(*@$_{\N}$@*) s true (*@$\lto$@*)
%%            &[ X : D.states ] &[ sInX : (X (*@$\ni$@*) s) ] Trace(*@$_{\D}$@*) X true)
%% NtoD .s (nil s) X sInX = nil s
%% NtoD .(N.src t) (N.cons trans .true a dstTrace) X sInX =
%%   D.cons (N.label t) X true a
%%     (((NtoD s dstTrace).(*@$\pi$@*) (D.(*@$\delta$@*) (N.label t) X)).(*@$\pi$@*) (N.dst t))
%% NtoD .(N.(*@$\epsilon$@*)src t) ((*@$\epsilon$@*)cons (*@$\epsilon$@*)trans .true (*@$\epsilon$@*)DstTrace) X sInX =
%%   ((NtoD s (*@$\epsilon$@*)DstTrace).(*@$\pi$@*) X).(*@$\pi$@*) (N.(*@$\epsilon$@*)dst t)
%% \end{lstlisting}
%% \caption{Parse transformer from NFA traces to Powerset DFA traces}
%% \label{fig:NtoD}
%% \end{figure}

%% Because the state $\s$ is now arbitrary, we can define $\NtoD$ via
%% recursion on the trace through the NFA. Code for this is given in \cref{fig:NtoD}.
%% If the NFA trace is terminating at the accepting state $\s$ and $\s \in \X$,
%% then we return a trace in $\D$ that terminates at
%% $\X$, which is accepting because it contains the accepting state $\s$.

%% If the NFA trace is defined via $\cons$ for a labeled transition
%% $t : N.transitions$ such that $N.src~t \in \X$, then
%% we first decompose the trace into a parse of $N.label~t$ and a
%% trace starting from $N.dst~t$. We then recursively call $\NtoD$
%% on this trace which returns a family of traces parameterized by $\epsilon$-closed
%% subsets containing $N.\dst~t$. We instantiate this family at the
%% set of states
%% $\D.\delta~(N.label~t)~X$. Because $\N.\dst~t$ is
%% reachable from $\N.\src~\texttt{trans}$ by a single transition labeled by
%% $N.label~t$ and $N.src~t \in X$ we have that
%% $N.dst~t \in \D.\delta~(N.label~t)~X$. Therefore
%% we can linearly combine the parse of the label and the trace built from the
%% recursive call to get a trace in $\D$ rooted at $X$.

%% Lastly, if the NFA trace is defined via $\epsilon\cons$ for an $\epsilon$-transition
%% $t : \N.\epstransitions$ such that
%% $\N.\epssrc~t \in \X$, then the construction is quite
%% similar. In this case we again recursively call $\NtoD$, except afterwards we
%% instantiate the family of grammars at the set $X$ itself. We may do this because
%% all states in $\D$ are taken to be $\epsilon$-closed, therefore
%% $\N.\epsdst~t \in \X$ because
%% $\N.\epssrc~t$ is in $X$. This concludes the construction of $\NtoD$.

To construct a term from DFA traces to NFA traces, we similarly strengthen our
\DIFdelbegin \DIFdel{induction }\DIFdelend \DIFaddbegin \DIFadd{inductive }\DIFaddend hypothesis and build a parse transformer \DIFdelbegin \begin{displaymath}%DIFAUXCMD
%DIFDELCMD < \[
%DIFDELCMD <   \DtoN %%%
\DIFdel{: \ltonl {\left( \Trace_{\D}~\X~\true \lto \bigoplus\limits_{s : \N.\mathsf{states}}\bigoplus\limits_{\texttt{sInX} : \X \ni \s} \Trace_{\N}~s~\true \right)}
}
\end{displaymath}%DIFAUXCMD
%DIFDELCMD < \]
%DIFDELCMD < %%%
\DIFdelend \DIFaddbegin \DIFadd{that ranges over arbitrary $\epsilon$-closed subsets $X : \mathbb{P}_{\epsilon}{\N.\states}$,
\(
  \DtoN : \ltonl {\left( \Trace_{\D}~\X~\true \lto \LinSigTy{s}{\N.\states}{\LinSigTy{\texttt{sInX}}{\X \ni \s}{ \Trace_{\N}~s~\true}} \right)}
\).
Because the states of $D$ are $\epsilon$-closed subsets of $N.\states$, if two
states $s$ and $s'$ belong to the the same $\epsilon$-closed subset
$X : \mathbb{P}_{\epsilon}(N.\states)$ then there exists a $\epsilon$-path in
$N$ between the two, but we do not necessarily know which one. Similarly, the
data contained in the type $\Trace_{\D}~X~true$ is the }\emph{\DIFadd{existence}} \DIFadd{of an accepting trace in
$\N$ beginning at some state in $\D$.
}\DIFaddend 

\DIFaddbegin \DIFadd{To define $\DtoN$, we need a choice function that extracts out a trace in $N$
from the mere existence of one. We achieve this by choosing the smallest trace
through $\N$ subject to an ordering on the non-linear types $N.\states$,
$N.\transitions$, and $N.\epstransitions$. In essence, the given orderings
specify a global disambiguation strategy.
}\end{proof}
\DIFaddend %% For a moment, let us turn our attention to the content behind the traces in $\D$.
%% Traces from subset $\X$ to subset $\Y$ describes the \emph{existence} of some trace
%% in the $\N$ from some $\x \in \X$ to some $\y \in \Y$. In general it is not clear how
%% to extract out a choice of trace in $\N$, which is what $\DtoN$ is asking of us.
%% However, we may further take the states and transitions of $\N$ to be \emph{ordered}.
%% This allows us to
%% induce an order on traces in $\D$ from $\X$ to $\Y$. From these orderings we can then
%% define the necessary choice functions on each type by taking the smallest with
%% respect to the ordering.

%% We define $\DtoN$ by recursion on traces through $\D$. First if the trace stops at
%% some accepting $\epsilon$-closed subset $\X$, then we may choose some state
%% $\s \in X$ such that $\s$ is accepting and terminate the trace in $\N$ at $\s$.

%% Next if the trace in $\D$ is defined via transition
%% $\X \overset{\stringquote{c}}{\to} (\D.\delta~\c~\X)$ then we may choose some state in
%% $\s \in \D.\delta~\c~\X$. Recursively, $\DtoN$ sends the trace rooted at
%% $\D.\delta~\c~\X$ to one rooted at $\s$. By definition of $\D.\delta~\c~\X$,
%% there exists some $\s' \in \X$ such that $\s'$ transitions to $\s$ by some number of
%% $\epsilon$-transitions followed by a transition labeled by $\c$.
%% We order all
%% such traces of this form and choose the smallest one, which is then linearly
%% combined with the parse of $\c$ to build an $\N$ trace. This concludes the
%% definition of $\DtoN$, thus showing that $Parse_{N}$ and $Parse_{D}$ are
%% weakly equivalent.
\DIFdelbegin %DIFDELCMD < \end{proof}
%DIFDELCMD < %%%
\DIFdelend 

Finally, given any regular expression we can construct a
\emph{strongly} equivalent NFA. While only weak equivalence is
required to construct a parser, proving the strong equivalence shows
that other aspects of formal grammar theory are also verifiable in
\theoryabbv.
\newcommand{\R}{\texttt{R}}
\DIFdelbegin %DIFDELCMD < \begin{theorem}[Thompson's Construction, \Agda]
%DIFDELCMD <   %%%
\DIFdelend \DIFaddbegin \begin{construction}[Thompson's Construction, \Agda{\fillerlink}]
  \label{cons:thompson}
  \DIFaddend Given a regular expression $\R$, \DIFdelbegin \DIFdel{there exists }\DIFdelend \DIFaddbegin \DIFadd{we build }\DIFaddend an NFA $\N$ such that $\R$ is
  strongly equivalent to $\Trace_{N}(\N.\init)$.
\DIFdelbegin %DIFDELCMD < \end{theorem}
%DIFDELCMD < %%%
\DIFdelend \DIFaddbegin \end{construction}
\DIFaddend \begin{proof}
We use a variant of Thompson's construction
\cite{thompsonProgrammingTechniquesRegular1968}, showing that NFAs are, up to strong equivalence, closed under each type operation for regular expressions.
\DIFdelbegin %DIFDELCMD < 

%DIFDELCMD < %%%
\DIFdelend \DIFaddbegin \end{proof}
\DIFaddend %% In this paper we will walk through
%% some of the cases of this proof. The remaining cases
%% follow
%% similar reasoning and the details may be found in the formalized artifact.

%% \begin{figure}
%%   \begin{tikzpicture}[node distance = 17mm ]
%%     \node[state, initial] (0) {$\0$};
%%     \node[state, right of=0, accepting] (1) {$\1$};

%%     \path[->] (0) edge[above] node{$\stringquote{c}$} (1);
%%   \end{tikzpicture}
%%   \caption{NFA $\N_{\c}$ for a single character $\c$}
%%   \label{fig:litnfa}
%% \end{figure}

%% First handle the case where $R$ is a literal character $\literal c$. The NFA $N_{c}$ pictured in
%% \cref{fig:litnfa} is strongly equivalent to the grammar $c$. To demonstrate this
%% strong equivalence, we build functions between $\literal c$ and $Parse_{N_{c}}$
%% and show they are mutually inverse.

%% \begin{figure}
%% \begin{lstlisting}
%% A : (s : Fin 2) (*@$\to$@*) L
%% A 0 = 'c'
%% A 1 = I

%% toN(*@$_c$@*) : (s : Fin 2) (*@$\to$@*) (*@$\uparrow$@*)(A s (*@$\lto$@*) Trace(*@$_{N_c}$@*) s true)
%% toN(*@$_c$@*) 0 c = cons t true c (nil 1)
%% toN(*@$_c$@*) 1 () = nil 1

%% fromN(*@$_c$@*) : (s : Fin 2) (*@$\to$@*) (*@$\uparrow$@*)(Trace(*@$_{N_c}$@*) s true (*@$\lto$@*) A s)
%% fromN(*@$_c$@*) 0 (cons t .true c tr) = let a (*@$\otimes$@*) () = c (*@$\otimes$@*) fromN(*@$_c$@*) 1 tr in a
%% fromN(*@$_c$@*) 1 (nil .1) = ()
%% \end{lstlisting}
%% \caption{Parse transformer out of $Trace_{N_{c}}~s~true$}
%% \label{fig:litNFAterms}
%% \end{figure}

%% In \cref{fig:litNFAterms} we define a family of grammars $A$ that serve as
%% interpretations for each state in $N_{c}$. We then prove that each $A~s$ is
%% equivalent to the accepting traces through $N_{c}$ out of state $s$.

%% When
%% defining $fromN_{c}$ we pattern match on the accepting traces beginning in each
%% state. Out of state $0$ the only accepting traces are built via $cons$ over the
%% single transition $t$, and out
%% of state $1$ the only accepting trace is built via $nil$.

%% Each $A~s$ is unambiguous, so $(fromN_{c}~s) \circ (toN_{c}~s)$ is equal to the
%% identity on each $A~s$. To complete the proof of strong equivalence it remains
%% to show that $(toN_{c}~s) \circ (fromN_{c}~s)$ is equal to the identity on
%% $Trace~s~true$, which we show by using the equalizer type.

%% \begin{figure}
%% \begin{lstlisting}
%% eqHom(*@$_{N_c}$@*) : (s : N(*@$_c$@*).state) (*@$\to$@*)
%%       (*@$\uparrow$@*)(Trace(*@$_{N_c}$@*) s true (*@$\lto$@*) {tr | (toN(*@$_c$@*) s (*@$\circ$@*) fromN(*@$_{c}$@*) s) tr = id tr})
%% \end{lstlisting}
%% \caption{Signature for the equalizer term}
%% \label{fig:litequalizer}
%% \end{figure}

%% The type signature for the family of functions, $eqHom_{N_{c}}$, into the equalizer type is given in
%% \cref{fig:litequalizer}. By defining an $Trace_{N_{c}}$-algebra homomorphism
%% into the equalizer, we enable a
%% proof strategy for equality by induction on traces through $N_{c}$. First consider the traces
%% out of state $1$. The only case to handle here is $nil 1$, and by definition
%% $toN_{c}~1$ and $fromN_{c}~1$ invert each other.
%% When defining $eqHom_{N_{c}}$ for traces out of state $0$, we must leverage an
%% inductive hypothesis,

%% \begin{align*}
%%   (toN_{c}~0 \circ fromN_{c}~0) (cons~t~true~c~tr)
%%   & \equiv (toN_{c}~0) (let~a~\otimes~()~=~c~\otimes~fromN_{c}~1~tr~in~a) \\
%%   & \equiv cons~t~true~(let~a~\otimes~()~=~c~\otimes~fromN_{c}~1~tr~in~a)~(nil~1) \\
%%   & \equiv cons~t~true~c~(nil~1) \\
%%   & \equiv cons~t~true~c~tr \\
%% \end{align*}

%DIF < % The first two equalities are by defintion. The third equality holds because
%DIF > % The first two equalities are by definition. The third equality holds because
%% $\literal c$ is unambiguous and
%% $let~a~\otimes~()~=~c~\otimes~fromN_{c}~1~tr~in~a$ and $c$ have type
%% $\literal c$. The final equality holds because our inductive hypothesis states
%% that $nil~1 \equiv tr$. Thus, each $A s$ and $Trace_{N_{c}}~s~true$ are strongly
%% equivalent --- in particular, $\literal c$ and $Parse_{N_{c}}$.

%% \steven{Rewrite the disjunction case with equalizers}
%% Now consider the case where $R$ is the disjunction of two regular expressions
%% $R_{1}$ and $R_{2}$. Inductively, we may build NFAs $N_{1}$ and $N_{2}$ that are
%% strongly equivalent to $R_{1}$ and $R_{2}$, respectively. We define $N_{\oplus}$
%% as a new NFA built as the disjunction of $N_{1}$ and $N_{2}$.
%% $N_{\oplus}.\states$ comprises the states from $N_{1}$, the states from
%% $N_{2}$, and a new marked initial state. The accepting states are precisely
%% those that were accepting in the subautomata. The initial state has an
%% $\varepsilon$-transition to each of the initial states of the subautomata. Once in
%% $N_{1}.\init$, the downstream transitions are a copy of those in $N_{1}$,
%% likewise for $N_{2}.\init$.  That is, $N_{\oplus}$ contains a copy of each
%% $N_{1}$ and $N_{2}$, and we choose which copy to inhabit by deciding which
%% $\varepsilon$-transition out of the initial state to take. A schematic
%% representation of this automaton is given in \cref{fig:disjunctionnfa}.

%% \begin{figure}
%%   \begin{tikzpicture}[node distance = 17mm ]
%%     \node[state, initial] (init) {$\init$};
%%     \node[state, above right of=init] (init1) {$N_{1}.\init$};
%%     \node[state, below right of=init] (init2) {$N_{2}.\init$};
%%     \node[right of=init1] (dots1) {$\dots$};
%%     \node[right of=init2] (dots2) {$\dots$};
%%     \node[state, accepting, right of=dots1] (acc1) {$s_{1}$};
%%     \node[state, accepting, right of=dots2] (acc2) {$s_{2}$};

%%     \path[->] (init) edge[below] node{$\varepsilon$} (init1)
%%               (init) edge[above] node{$\varepsilon$} (init2)
%%               (init1) edge[above] node{} (dots1)
%%               (init2) edge[above] node{} (dots2)
%%               (dots1) edge[above] node{} (acc1)
%%               (dots2) edge[above] node{} (acc2);
%%   \end{tikzpicture}
%%   \caption{NFA $N_{\oplus}$ as a disjunction of $N_{1}$ and $N_{2}$}
%%   \label{fig:disjunctionnfa}
%% \end{figure}

%% Because $N_{1}$ and $N_{2}$ are each strongly equivalent to $R_{1}$ and $R_{2}$,
%% it suffices to show that
%% $Parse_{N_{1}} \oplus Parse_{N_{2}}$ is strongly
%% equivalent to $Parse_{N_{\oplus}}$.

%% Just like the case of a single character, when building the maps in either
%% direction, we strengthen the induction hypothesis by mapping out of traces
%% beginning with an arbitrary state then proceed by induction on traces.

%% First, define the
%% mapping out of $\Trace_{N_{\oplus}}(s)$. If $s$ is a state from $N_{1}$ then
%% send the trace to a trace in $N_{1}$ beginning at $s$. Likewise, if $s$ is from
%% $N_{2}$, then we map to a trace in $N_{2}$ beginning at $s$. If $s$ is
%% $N_{\oplus}.\init$, then we map the trace to a disjunction of traces
%% $\Trace_{N_{1}}(N_{1}.\init) \oplus \Trace_{N_{2}}(N_{2}.\init)$.

%% In the other direction, we need to map a disjunction of traces through $N_{1}$
%% or $N_{2}$ to a trace in $N_{\oplus}$. Without loss of generality, assume that
%% the trace came from $N_{1}$. That is, assume we currently have a term of type
%% $\Trace_{{N_{1}}}(N_{1}.\init)$ and we aim to construct a term of type
%% $\Trace_{N_{\oplus}}(N_{\oplus}.\init)$. We induct on the trace $N$ trace an map
%% it over to a trace beginning at the copy of $N_{1}.\init$ in $N_{\oplus}$, and
%% then we conclude by using the $\epscons$ constructor to turn this trace into one
%% beginning at $N_{\oplus}.\init$.

%% We show that these maps mutually invert each other by the same sort of argument
%% we used with the single character NFA.\ That is, we prove that the each map is a
%% homomorphism of algebras and then appeal to the initiality of each of
%% $\Trace_{N_{1}}$, $\Trace_{N_{2}}$, and $\Trace_{N_{\oplus}}$ to prove that the
%% relevant maps compose to the identity.

%% In this sketch, we elide the proof that these maps do indeed form homomorphisms.
%% Further, we are eliding the needed NFAs for the cases where the regular
%% expression is $I$, $0$, a Kleene star, or a concatenation. All of these
%% elided details may be found in the Agda formalization.
\DIFaddbegin 

\begin{corollary}[\Agda{\fillerlink}]
  \DIFadd{We may build a parser for every regular expression $\R$.
}\end{corollary}
\begin{proof}
\DIFadd{We combine the strong equivalence of \mbox{%DIFAUXCMD
\cref{cons:thompson} }\hskip0pt%DIFAUXCMD
with the weak
equivalence of \mbox{%DIFAUXCMD
\cref{cons:determinization} }\hskip0pt%DIFAUXCMD
to show that $\R$ is weakly
equivalent to the traces of its determinized automaton. Then, we use
\mbox{%DIFAUXCMD
\cref{lem:wk-eqv-parse} }\hskip0pt%DIFAUXCMD
to extend the parser from \mbox{%DIFAUXCMD
\cref{thm:dfa-parser}
}\hskip0pt%DIFAUXCMD
with respect to this weak equivalence.
}\DIFaddend \end{proof}

\subsection{Context-free grammars}

Next, we give two examples for parsing context-free
grammars. Context-free grammars (CFG) can be encoded in our type
theory in a similar way to regular expressions, as CFGs are equivalent
to the formalism of \emph{$\mu$}-regular expressions, where the Kleene
star is replaced by an arbitrary fixed point
operation \cite{leis_towards_1992}.

\DIFdelbegin %DIFDELCMD < \newcommand{\balanced}{\mathtt{balanced}}
%DIFDELCMD < \newcommand{\Dyck}{\mathtt{Dyck}}
%DIFDELCMD < %%%
\DIFdelend A simple example of a CFG is the Dyck grammar of balanced
parentheses, which we define in \Cref{fig:dyckinductive}\DIFaddbegin \DIFadd{.
}\DIFaddend $\Dyck$ is a grammar over the alphabet $\{ \stringquote{(}, \stringquote{)} \}$. The $\nil$ constructor shows that
the empty string is balanced, and the $bal$ constructor builds a balanced
parse by wrapping an already balanced parse in an additional set of parentheses
then following it with another balanced parse. We construct a parser for $\Dyck$ by building a deterministic automaton $M$ such
that $Parse_{M}$ is strongly equivalent to $\Dyck$.
\begin{figure}
\begin{lstlisting}
data Dyck : L where
  nil : (*@$\uparrow$@*) Dyck
  bal : (*@$\uparrow$@*)('(' (*@$\lto$@*) Dyck (*@$\lto$@*) ')' (*@$\lto$@*) Dyck (*@$\lto$@*) Dyck)
\end{lstlisting}
\caption{The Dyck grammar as an inductive linear type}
\label{fig:dyckinductive}
\end{figure}

The Dyck language is an example of an $\textrm{LL}(0)$ language, one
that can be parsed top-down with no
lookahead\cite{rosenkrantz_properties_1970}. This means we can
implement it simply as an \emph{infinite} state deterministic
automaton, in \Cref{fig:DyckAutomaton}. Here the state is a ``stack''
counting how many open parentheses have been seen so far. Functions
$parse_{M}$ and $print_{M}$ for this automaton can be defined
analogously to the parser and printer for DFAs, and so $\LinSigTy {s}
{M.states} {\LinSigTy {b}{Bool} {Trace_{M}~s~b}}$ is likewise
unambiguous.

\DIFdelbegin %DIFDELCMD < \begin{theorem}[\Agda]
%DIFDELCMD <   %%%
\DIFdelend \DIFaddbegin \begin{theorem}[\Agda{\fillerlink}]
  \DIFaddend \label{thm:dyck}
  $Dyck$ and $Parse_{M}$ are strongly equivalent. And therefore we can construct a parser for $\Dyck$.
\end{theorem}

%% We can build a $Dyck$ parser by utilizing
%% this artifact, and the
%% runtime of the $Dyck$ parser is about 30 seconds for input that is 25,000
%% characters long.
%% \pedro{It might be a good idea to mention explicitly in the introduction
%%   that we are not claiming that our verified parsers are competitive with
%%   the state of the art.}

\begin{figure}
  \DIFdelbeginFL %DIFDELCMD < \begin{tikzpicture}[node distance = 17mm ]
%DIFDELCMD <     \node[state, initial, accepting] (0) {$0$};
%DIFDELCMD <     \node[state, right of=0] (1) {$1$};
%DIFDELCMD <     \node[state, right of=1] (2) {$2$};
%DIFDELCMD <     \node[right of=2] (3) {$\dots$};
%DIFDELCMD <     \node[state, below of=0] (fail) {$fail$};
%DIFDELCMD < 

%DIFDELCMD <     \path[->] (0) edge[above, bend left] node{\stringquote{(}} (1)
%DIFDELCMD <               (1) edge[below, bend left] node{\stringquote{)}} (0)
%DIFDELCMD <               (1) edge[above, bend left] node{\stringquote{(}} (2)
%DIFDELCMD <               (2) edge[below, bend left] node{\stringquote{)}} (1)
%DIFDELCMD <               (2) edge[above, bend left] node{\stringquote{(}} (3)
%DIFDELCMD <               (3) edge[below, bend left] node{\stringquote{)}} (2)
%DIFDELCMD <               (fail) edge[loop right] node{\stringquote{(}, \stringquote{)}} (fail)
%DIFDELCMD <               (0) edge[left] node{\stringquote{)}} (fail);
%DIFDELCMD <   \end{tikzpicture}
%DIFDELCMD <   %%%
\DIFdelendFL \DIFaddbeginFL \footnotesize
  \begin{tikzpicture}[node distance = 17mm ]
    \node[state, initial above, accepting] (0) {$0$};
    \node[state, right of=0] (1) {$1$};
    \node[state, right of=1] (2) {$2$};
    \node[right of=2] (3) {$\dots$};
    \node[state, left of=0] (fail) {$fail$};

    \path[->] (0) edge[above, bend left] node{\stringquote{(}} (1)
              (1) edge[below, bend left] node{\stringquote{)}} (0)
              (1) edge[above, bend left] node{\stringquote{(}} (2)
              (2) edge[below, bend left] node{\stringquote{)}} (1)
              (2) edge[above, bend left] node{\stringquote{(}} (3)
              (3) edge[below, bend left] node{\stringquote{)}} (2)
              (fail) edge[loop left] node{\stringquote{(}, \stringquote{)}} (fail)
              (0) edge[above] node{\stringquote{)}} (fail);
  \end{tikzpicture}
  \DIFaddendFL \caption{Automaton $M$ for the Dyck grammar}
  \label{fig:DyckAutomaton}
\end{figure}

Our final example is of a simple grammar of arithmetic expressions
with an associative operation. Here we take the alphabet to be $\{
\stringquote{(}, \stringquote{)}, \stringquote{+} , \stringquote{NUM}
\}$. In \Cref{fig:binop-inductive} we define \DIFdelbegin \DIFdel{it }\DIFdelend \DIFaddbegin \DIFadd{the expression grammar
}\DIFaddend using two mutually
recursive types, corresponding to the two non-terminals we would use
in a CFG syntax. The syntactic structure encodes that the binary
operation is right associative. In the same figure, we define the
traces of an automaton with one token of lookahead. The automaton\DIFaddbegin \footnote{\DIFadd{In
  the automaton definition,
  $\texttt{NotStartsWithLP}$ is defined as
  \(I \oplus
  (\literal{)} \oplus \literal{+} \oplus \mathtt{NUM}) \otimes \top
  \). Similarly,
  $\texttt{NotStartsWithRP}$ is defined as
  \(I \oplus
  (\literal{(} \oplus \literal{+} \oplus \mathtt{NUM}) \otimes \top
  \).
}} \DIFaddend has
four different ``states'', each with access to a natural number
``stack''. The ``opening'' state $O$ expects either \DIFdelbegin \DIFdel{an open }\DIFdelend \DIFaddbegin \DIFadd{a left }\DIFaddend paren, in
which case it increments the stack and stays in the opening state, or
sees a number and proceeds to the $D$ state. The ``done opening''
state $D$ is where lookahead is used: if the next token will be a
right paren, then we proceed to $C$\DIFdelbegin \DIFdel{, otherwise}\DIFdelend \DIFaddbegin \DIFadd{; otherwise, }\DIFaddend we proceed to \DIFdelbegin \DIFdel{$M$.
Here
$\mathsf{NotStartsWithRP}$ is defined as
$\I \oplus (({\literal{(}} \oplus {\literal {+}}
\oplus \mathsf{NUM}) \otimes \top)$. }\DIFdelend \DIFaddbegin \DIFadd{$A$.
}\DIFaddend In the ``closing'' state $C$ if
we observe a \DIFdelbegin \DIFdel{close paren}\DIFdelend \DIFaddbegin \DIFadd{right paren, then }\DIFaddend we decrement the count and continue to the
$D$ state. In the ``\DIFdelbegin \DIFdel{multiplying'' state $M$}\DIFdelend \DIFaddbegin \DIFadd{adding'' state $A$}\DIFaddend , we succeed if the string
ends and the count is $0$\DIFdelbegin \DIFdel{, and }\DIFdelend \DIFaddbegin \DIFadd{; otherwise, }\DIFaddend if we see a plus we continue to the $O$
state. Additionally, since the automaton need \DIFdelbegin \DIFdel{also }\DIFdelend parse all of the
incorrect strings, we add \DIFdelbegin \DIFdel{additional }\DIFdelend \DIFaddbegin \DIFadd{all of the }\DIFaddend failing cases.
\DIFdelbegin \DIFdel{It is }\DIFdelend \DIFaddbegin 

\begin{theorem}[\Agda{\fillerlink}]
  \label{thm:exp-parser}
  \DIFadd{We construct a parser for $\mathsf{Exp}$ by showing it is weakly
  equivalent to $\mathsf{O}~0~\true$.
}\end{theorem}

\DIFadd{With \mbox{%DIFAUXCMD
\cref{ax:dist}}\hskip0pt%DIFAUXCMD
, it is }\DIFaddend straightforward to implement a parser for this lookahead automaton,
generalizing the approach for deterministic automata. \DIFaddbegin \DIFadd{Without access to
distributivity, we may define a rudimentary lookahead operation via the chain of
equivalences,
}\[
  \DIFadd{A~n~b \cong (A~n~b) \& \top \cong (A~n~b)\,\& }\left(\left(\DIFadd{\literal{)} \otimes \top }\right) \DIFadd{\oplus }\texttt{\DIFadd{NotStartsWithRP}}\right)
\]
\DIFadd{When defining a parser, if the lookahead character is a right paren we would
like to apply $\texttt{lookAheadRP}$; otherwise, apply $\texttt{lookAheadNot}$.
However, without distributivity we have no means of relating the bit of
information learned by looking ahead to the input $A~n~b$ parse.
That is, to choose which constructor to apply we would need the end of this
chain of equivalences to be a binary sum rather than a binary product,
necessitating the distributivity axiom.
}\DIFaddend 

\DIFdelbegin %DIFDELCMD < \begin{theorem}[\agdalogo]
%DIFDELCMD <   %%%
\DIFdel{We construct a parser for $\mathsf{Exp}$ by showing it isweakly
  equivalent to $\mathsf{O}~0~\true$.
}%DIFDELCMD < \end{theorem}
%DIFDELCMD < %%%
\DIFdelend \begin{figure}
\DIFmodbegin
\begin{lstlisting}[alsolanguage=DIFcode]
data Exp : L where
  done : (*@$\uparrow$@*)(Atom (*@$\lto$@*) Exp)
  add : (*@$\uparrow$@*)(Atom (*@$\lto$@*) '+' (*@$\lto$@*) Exp (*@$\lto$@*) Exp)
data Atom : L where
  num : (*@$\uparrow$@*)(NUM (*@$\lto$@*) Atom)
  parens : (*@$\uparrow$@*)('(' (*@$\lto$@*) Exp (*@$\lto$@*) ')' (*@$\lto$@*) Atom)

data O : Nat (*@$\to$@*) Bool (*@$\to$@*) L where
  left : (*@$\uparrow$@*)(&[ n ] &[ b ] '(' (*@$\lto$@*) O (n + 1) b (*@$\lto$@*) O n b)
%DIF <   num : (*@$\uparrow$@*)(&[ n ] &[ b ] NUM (*@$\lto$@*) DO n b (*@$\lto$@*) O n b)
%DIF <   done : (*@$\uparrow$@*)(&[ n ] O n false)
%DIF <   unexpected : (*@$\uparrow$@*)(&[ n ] (')' (*@$\oplus$@*) '+') (*@$\lto$@*) (*@$\top$@*) (*@$\lto$@*) O n false)
%DIF >   num : (*@$\uparrow$@*)(&[ n ] &[ b ] NUM (*@$\lto$@*) D n b (*@$\lto$@*) O n b)
%DIF >   unexpected : (*@$\uparrow$@*)(&[ n ] (NotStartsWithLP (*@$\lto$@*) O n false)
data D : Nat (*@$\to$@*) Bool (*@$\to$@*) L where
  lookAheadRP : (*@$\uparrow$@*)(&[ n ] &[ b ] ((')' (*@$\otimes$@*) (*@$\top$@*)) (*@$\&$@*) C n b) (*@$\lto$@*) D n b)
%DIF <   lookAheadNot  : (*@$\uparrow$@*)(&[ n ] &[ b ] (NotStartsWithRP (*@$\&$@*) M n b) (*@$\lto$@*) D n b)
%DIF >   lookAheadNot : (*@$\uparrow$@*)(&[ n ] &[ b ] (NotStartsWithRP (*@$\&$@*) A n b) (*@$\lto$@*) D n b)
data C : Nat (*@$\to$@*) Bool (*@$\to$@*) L where
  closeGood : (*@$\uparrow$@*)(&[ n ] &[ b ] ')' (*@$\lto$@*) D n b (*@$\lto$@*) C (n + 1) b)
%DIF <   closeBad : (*@$\uparrow$@*)(&[ n ] ')' (*@$\lto$@*) C 0 b)
%DIF <   done : (*@$\uparrow$@*)(&[ n ] C n false)
%DIF <   unexpected : (*@$\uparrow$@*)(&[ n ] ('(' (*@$\oplus$@*) '+' (*@$\oplus$@*) NUM) (*@$\lto$@*) (*@$\top$@*) (*@$\lto$@*) C n false)
%DIF < data M : Nat (*@$\to$@*) Bool (*@$\to$@*) L where
%DIF <   doneGood : (*@$\uparrow$@*)(M 0 true)
%DIF <   doneBad : (*@$\uparrow$@*)(&[ n ] M (n + 1) false)
%DIF <   add : (*@$\uparrow$@*)(&[ n ] &[ b ] '(' (*@$\lto$@*) O n b (*@$\lto$@*) M n b)
%DIF <   unexpected : (*@$\uparrow$@*)(&[ n ] ('(' (*@$\oplus$@*) ')'(*@$\oplus$@*) NUM) (*@$\lto$@*) (*@$\top$@*) (*@$\lto$@*) M n false)
%DIF >   closeBad : (*@$\uparrow$@*)(')' (*@$\lto$@*) C 0 false)
%DIF >   unexpected : (*@$\uparrow$@*)(&[ n ] NotStartsWithRP (*@$\lto$@*) C n false)
%DIF > data A : Nat (*@$\to$@*) Bool (*@$\to$@*) L where
%DIF >   doneGood : (*@$\uparrow$@*)(A 0 true)
%DIF >   doneBad : (*@$\uparrow$@*)(&[ n ] A (n + 1) false)
%DIF >   add : (*@$\uparrow$@*)(&[ n ] &[ b ] '+' (*@$\lto$@*) O n b (*@$\lto$@*) A n b)
%DIF >   unexpected : (*@$\uparrow$@*)(&[ n ] ('(' (*@$\oplus$@*) ')'(*@$\oplus$@*) NUM) (*@$\lto$@*) (*@$\top$@*) (*@$\lto$@*) A n false)
\end{lstlisting}
\DIFmodend
\caption{Associative arithmetic expressions and a corresponding lookahead automaton}
\label{fig:binop-inductive}
\end{figure}
% \subsection{LL(1)}
% %% 4. LL(1) parser

% \steven{TODO impute LL(1) example, mention that its weak equivalence is mechanized}

\subsection{Unrestricted Grammars}
\DIFdelbegin %DIFDELCMD < \newcommand{\stringNL}{String'}
%DIFDELCMD < %%%
\DIFdelend 

While we have shown only examples for context-free grammars, in fact
arbitrarily complex grammars are encodable in \theoryabbv. To
demonstrate this, we show that for any \emph{non-linear} function \DIFdelbegin \DIFdel{$P :
\stringNL \to U$}\DIFdelend \DIFaddbegin \DIFadd{$P :
\StringSem \to U$}\DIFaddend , where here \DIFdelbegin \DIFdel{$\stringNL$ }\DIFdelend \DIFaddbegin \DIFadd{$\StringSem$ }\DIFaddend is the \emph{non-linear} type
of strings over the alphabet, we can construct a grammar whose parses
correspond to $P$.
%DIF < % Define $\stringNL$ to be the non-linear list type over the alphabet $\Sigma$ and
%DIF > % Define $\StringSem$ to be the non-linear list type over the alphabet $\Sigma$ and
%% let  be a mapping from the set of strings to the universe of
%% nonlinear types. The function $P$ describes a family of nonlinear types
%DIF < % dependent on $\stringNL$.
%DIF > % dependent on $\StringSem$.
%% We view the image of $P$ as a set of parse trees for each input string, and thus
%% $P$ carries the structure of a formal grammar. However, even though $P$ describes this grammatical data, it is not
%% a first-class linear type in \theoryabbv. 
\DIFdelbegin \begin{displaymath}%DIFAUXCMD
%DIFDELCMD < \[
%DIFDELCMD < %%%
\DIFdel{Reify~P = }%DIFDELCMD < \LinSigTyLimit {w} {\stringNL} {\LinSigTyLimit {x} {P~w} {~\lceil w \rceil}}
\end{displaymath}%DIFAUXCMD
%DIFDELCMD < \]
%DIFDELCMD < %%%
\DIFdelend \DIFaddbegin \DIFadd{\(
Reify~P = \LinSigTy{w} {\StringSem} {\LinSigTy {x} {P~w} {~\lceil w \rceil}}
\)
}\DIFaddend %% \steven{Probably put this ceiling business around the discussion of the linear
%%   type of strings. Is this in section 3?}
where $\lceil \stringquote{} \rceil = I$ and
$\lceil c :: w \rceil = \literal c \otimes \lceil w \rceil$.

This reification operation on functions \DIFdelbegin \DIFdel{$\stringNL \to U$ }\DIFdelend \DIFaddbegin \DIFadd{$\StringSem \to U$ }\DIFaddend is incredibly expressive,
as it allows to sidestep our linear typing connectives and utilize the whole of nonlinear
dependent type theory to define a grammar. For example, given a
Turing machine \DIFdelbegin \DIFdel{$Tur$ }\DIFdelend \DIFaddbegin \DIFadd{$T$ }\DIFaddend one may
define a non-linear predicate \DIFdelbegin \DIFdel{$accepts : \stringNL \to U$ such that $accept~w$ is equal to $\top$ if
}\DIFdelend \DIFaddbegin \DIFadd{$accepts : \StringSem \to U$ that encodes that }\DIFaddend $T$
\DIFdelbegin \DIFdel{accepts $w$ and equal to $0$ otherwise}\DIFdelend \DIFaddbegin \DIFadd{halts and accepts an input string}\DIFaddend . Then, $Reify~accepts$ is a linear type that
captures precisely the \DIFdelbegin \DIFdel{strings accepted by $Tur$}\DIFdelend \DIFaddbegin \DIFadd{string the same language as $T$}\DIFaddend . That is,
$Lang(Reify~accepts)$ is recursively enumerable --- the most general class of
languages in the Chomsky hierarchy.
\DIFdelbegin %DIFDELCMD < \begin{theorem}[\Agda]
%DIFDELCMD <   %%%
\DIFdelend \DIFaddbegin \begin{construction}[\Agda{\fillerlink}]
  \label{cons:turing}
  \DIFaddend For any Turing machine \DIFaddbegin \DIFadd{$T$}\DIFaddend , we can construct a grammar in \theoryabbv
  that accepts the same language as \DIFdelbegin \DIFdel{the Turing machine.
}%DIFDELCMD < \end{theorem}
%DIFDELCMD < %%%
\DIFdelend \DIFaddbegin \DIFadd{$T$.
}\end{construction}
\DIFaddend 

\section{Denotational Semantics and Implementation}
\label{sec:semantics-and-metatheory}

To justify our assertion that \theoryabbv is a syntax for formal
grammars and parse transformers, we will now define a
\emph{denotational semantics} that makes this mathematically precise
by defining a notion of formal grammar and parse transformer then
showing that our type theory can be soundly interpreted in this
model. We then discuss how this denotational semantics provides the
basis for our prototype implementation in Agda.

\subsection{Formal Grammars and Parse Transformers}

The most common definition of a formal grammar is as generative
grammars, defined by a set of non-terminals, a specified start symbol
and set of production rules. We instead use a more abstract
formulation that is closer in spirit to the standard definition of a
formal \emph{language} \cite{elliottSymbolicAutomaticDifferentiation2021}:
\begin{definition}
  A \emph{formal language} $L$ is a function from strings to propositions.
  A (small) \emph{formal grammar} $A$ is a function from strings to (small) sets.
\end{definition}
We think of the grammar $A$ as taking a string to the set of all parse
trees for that string. However since $A$ could be any function
whatsoever there is no requirement that an element of $A(w)$ be a
``tree'' in the usual sense. This definition provides a simple,
syntax-independent definition of a grammar that can be used for any
formalism: generative grammars, categorial grammars, or our own
type-theoretic grammars. Note that the definition of a formal grammar
is a generalization of the usual notion of formal language since a
proposition can be equivalently defined as a subset of a one-element
set. Then the difference between a formal grammar and a formal
language is that formal grammars can be \emph{ambiguous} in that there
can be more than one parse of the same string. Even for unambiguous
grammars, we care not just about \emph{whether} a string has a parse
tree, but \emph{which} parse tree it has, i.e., what the structure of
the element of $A(w)$ is.  To interpret our universes $U , L$ we
assume we have a universe of \emph{small} sets. In the remainder, all
formal grammars are assumed to be small.

We then interpret linear \emph{terms} as \emph{parse transformers}:
\begin{definition}
  Let $A_1,A_2$ be formal grammars. Then a parse transformer $f$ from
  $A_1$ to $A_2$ is a function assigning to each string $w$ a function
  $f_w : A_1(w) \to A_2(w)$.
\end{definition}
Just as formal grammars generalize formal languages, parse
transformers generalize formal language inclusion: if $A_1(w), A_2(w)$
are all subsets of a one-element set, then a parse transformer is
equivalent to showing that $A_1(w) \subseteq A_2(w)$. In our
denotational semantics, linear terms will be interpreted as such parse
transformers, and the notions of unambiguous grammar, parsers,
disjointness, etc, introduced in \Cref{sec:applications} can be
verified to correspond to their intended meanings under this
interpretation.

Parse transformers can be composed: given two parse transformers $f$ and $g$,
their composition is defined pointwise, i.e. $(f\circ g)_w = f_w \circ g_w$.
Furthermore, given a formal grammar $A$, its identity transformer is $id_w =
id_{A(w)}$, where $id_{A(w)}$ is the identity function on the set $A(w)$. This
defines a \emph{category}.
\begin{definition}
  Define $\Grammar$ to be the category whose objects are formal
  grammars and morphisms are parse transformers.
\end{definition}
This category is equivalent to the slice category $\Set/\Sigma^*$ and
as such is very well-behaved. It is complete, co-complete, Cartesian
closed and carries a monoidal biclosed structure. We will use these
structures to model the linear types, terms and equalities in
\theoryabbv. These categorical properties are precisely what is
required to interpret all of the linear type and term constructors.
%% \begin{theorem}
%%   The category $\Grammar$ is complete, co-complete and cartesian
%%   closed. Furthermore, it is monoidal biclosed with the monoidal
%%   product given by
%%   \[ (A \otimes B) w = \{ (w_1,w_2,a,b) \pipe w_1w_2 = w \wedge a \in A w_1 \wedge b \in B w_2\} \]
%%   and unit by
%%   \[ I w = \{ () \pipe w = \varepsilon \} \]
%% \end{theorem}

%% \max{todo: need to decide if we need this section here or if it's in the intro already}.
% The linear typing judgment in our syntax takes on the following schematic form
% $\Gamma ; \Delta \vdash a : A$. First, $A$ represents a \emph{linear type} in
% our syntax. The intended semantics of these linear types are formal grammars.
% That is, the linear typing system is designed to syntactically reflect the
% behavior of formal grammars. For this reason, we may often interchangeably use
% the terms ``linear type'' and ``grammar''.

% The term $a$ is an inhabitant of type $A$, which is thought of as a parse tree of
% the grammar $A$. The core idea of this entire paper follows precisely from this
% single correspondence: grammars are types, and the inhabitants of these types
% are parse trees for the grammars.

% $\Gamma$ represents a non-linear context, while $\Delta$ represents a
% \emph{linear context} dependent on $\Gamma$. These linear contexts behave
% substructurally. As stated earlier, they are linear --- they do not obey
% weakening or contraction --- because a character is exhausted once read by a
% parsing procedure. Moreover, the characters in strings appear in the order in
% which we read them. We do not have the freedom to freely permute characters,
% therefore any type theory that is used to reason about formal grammars ought to
% omit the structural rule of exchange as well. This means that every variable
% within $\Delta$ must be used \emph{exactly once} and \emph{in order of
%   occurrence}. Thus, we can think of the linear contexts as an ordered list of
% limited resources. Once a resource is consumed, we cannot make reference to it
% again. Variables in a linear context then act like building
% blocks for constructing patterns over strings.

% We give the base types and type constructors for linear terms. As the
% interpretation of types as grammars in $\Grammar$ serves as our intended
% semantics, we simultaneously give the interpretation $\sem \cdot$ of the
% semantics as grammars.

% \pedro{The paragraphs above are a bit circular, what about the following alternative?}
% \steven{I am in favor of cutting what's above, but we haven't introduced the
%   syntax in the main body of the paper yet, even if the figures appeared
%   earlier. So what you write needs to take that into account}
% \pedro{In which case we should introduce it in the previous section. I suggest
%   adding this right after ``and the intuitionistic typing rules in fig 3''}
\subsection{Semantics}

We now define our denotational semantics.
\begin{definition}[Grammar Semantics]
  We define the following interpretations by mutual recursion on the
  judgments of \theoryabbv:
  \begin{enumerate}
  \item For each non-linear context $\Gamma \isCtx$, we define a set $\sem \Gamma$.
  \item For each non-linear type $\Gamma \vdash X \isTy$, and element
    $\gamma \in \sem\Gamma$, we define a set $\sem X \gamma$.
  \item For each linear type \DIFdelbegin \DIFdel{$\Gamma A \isLinTy$ }\DIFdelend \DIFaddbegin \DIFadd{$\Gamma \vdash A \isLinTy$ }\DIFaddend and element $\gamma
    \in \sem\Gamma$, we define a formal grammar $\sem{A}\gamma$. We
    similarly define a formal grammar $\sem\Delta\gamma$ for each
    linear context $\Gamma \Delta\isLinCtx$.
  \item For each non-linear term $\Gamma \vdash M : X$ and $\gamma \in \sem{\Gamma}$, we define an element $\sem{M}\gamma \in \sem{X}\gamma$.
  \item For each linear term $\Gamma; \Delta \vdash e : A$ and $\gamma \in \sem{\Gamma}$ we define a parse transformer from $\sem{\Delta}\gamma$ to $\sem{A}\gamma$.
  \end{enumerate}
  And we verify the following conditions:
  \begin{enumerate}
  \item If $\Gamma \vdash X \isSmall$, then $\sem X \gamma$ is a small set.
  \item If $\Gamma \vdash X \equiv X'$ then for every $\gamma$, $\sem{X}\gamma = \sem{X'}\gamma$.
  \item If $\Gamma \vdash A \equiv A'$ then for every $\gamma$, $\sem{A}\gamma = \sem{A'}\gamma$.
  \item If $\Gamma \vdash M \equiv M' : X$ then for every $\gamma$, $\sem{M}\gamma = \sem{M'}\gamma$.
  \item If $\Gamma;\Delta \vdash e \equiv e' : A$ then for every $\gamma$, $\sem{e}\gamma = \sem{e'}\gamma$.
  \end{enumerate}
\end{definition}

The interpretation of dependent types as sets is standard
\cite{Hofmann_1997}. We present the concrete descriptions of the
semantics of linear types, as well as our non-standard non-linear
types in \Cref{fig:semantics}. The grammar for a literal $c$ has a
single parse precisely when the input string consists of the single
character. The grammar for the unit similarly has a single parse for
the empty string.  A parse of the tensor product $A \otimes B$
consists of a \emph{splitting} of the empty string into a prefix $w_1$
and suffix $w_2$ along with an $A$ parse of $w_1$ and $B$ parse of
$w_2$. A parse of $\bigoplus_{x:X} A$ is a pair of an element of the
set $X$ and a parse of $A(x)$, while dually a parse of $\bigwith_{x:X}
A$ is a \emph{function} taking any $x:X$ to a parse of $A(x)$. A
$w$-parse of $A \lto B$ is a function that takes an $A$ parse of some
other string $w'$ to a $B$ parse of $ww'$, and $B \tol A$ is the same
except the $B$ parse is for the reversed concatenation $w'w$.
%
The set $\uparrow A$ is the set of parse for the empty string for
$A$. This definition means that $\sem{\uparrow (A \lto B)}$ (or $\sem{\uparrow (B \tol A)}$) is
equivalent to the set of parse transformers:
\DIFdelbegin \begin{displaymath}%DIFAUXCMD
%DIFDELCMD < \[ \sem{\uparrow (A \lto B)}%%%
\DIFdel{\gamma = }%DIFDELCMD < \sem{A \lto B}%%%
\DIFdel{\gamma\,\varepsilon = \prod_{w'} }%DIFDELCMD < \sem{A}%%%
\DIFdel{\gamma\,w' }%DIFDELCMD < \to \sem{B}%%%
\DIFdel{\gamma\,w'}
\end{displaymath}%DIFAUXCMD
%DIFDELCMD < \]
%DIFDELCMD < %%%
\DIFdelend \DIFaddbegin \DIFadd{\( \sem{\uparrow (A \lto B)}\gamma = \sem{A \lto B}\gamma\,\varepsilon = \prod_{w'} \sem{A}\gamma\,w' \to \sem{B}\gamma\,w'\).
}

\DIFaddend Next, a parse in the equalizer $\equalizer{a}{f}{g}$ is defined as
a parse in $\sem{A}$ that is mapped to the same parse by the parse transformers $\sem{f}$ and
$\sem{g}$.
%
The universe $L$ of linear types is interpreted as the set of all
small grammars.
\begin{figure}
  \DIFaddbeginFL \begin{minipage}[t]{.5\textwidth}
  \DIFaddendFL \begin{footnotesize}
    \DIFdelbeginFL %DIFDELCMD < \begin{align*}
%DIFDELCMD <     %%%
\DIFdelendFL \DIFaddbeginFL \begin{flalign*}
    &\DIFaddendFL \sem{c}\gamma\,w \DIFdelbeginFL %DIFDELCMD < &%%%
\DIFdelendFL = \{ c | w = c\}\\
    \DIFaddbeginFL &\DIFaddendFL \sem{I}\gamma\,w \DIFdelbeginFL %DIFDELCMD < &%%%
\DIFdelendFL = \{ () \pipe w = \varepsilon \}\\
    \DIFaddbeginFL &\DIFaddendFL \sem{A \otimes B}\gamma\,w \DIFdelbeginFL %DIFDELCMD < &%%%
\DIFdelendFL = \{ (w_1,w_2,a,b) \pipe w_1w_2 = w \wedge a \in \sem{A}\gamma\,w_1 \wedge b \in \sem{B}\gamma\,w_2 \}\\
    \DIFaddbeginFL &\DIFaddendFL \sem{A \lto B}\gamma\,w \DIFdelbeginFL %DIFDELCMD < &%%%
\DIFdelendFL = \prod_{w'} \sem{A}\gamma\,w' \to \sem{B}\gamma\,ww'\\
    \DIFaddbeginFL &\DIFaddendFL \sem{B \tol A}\gamma\,w \DIFdelbeginFL %DIFDELCMD < &%%%
\DIFdelendFL = \prod_{w'} \sem{A}\gamma\,w' \to \sem{B}\gamma\,w'w\\
    \DIFaddbeginFL &\DIFaddendFL \sem{\bigoplus_{x:X} A}\gamma\,w \DIFdelbeginFL %DIFDELCMD < &%%%
\DIFdelendFL = \{ (x, a) \pipe x \in \sem{X}\gamma \wedge a \in \sem{A}(\gamma,x)w \}
  \DIFdelbeginFL %DIFDELCMD < \\
%DIFDELCMD <     %%%
\DIFdelendFL \DIFaddbeginFL \end{flalign*}
  \end{footnotesize}
  \end{minipage}%DIF > 
  \begin{minipage}[t]{.5\textwidth}
  \begin{footnotesize}
    \begin{flalign*}
    &\DIFaddendFL \sem{\bigwith_{x:X} A}\gamma\,w \DIFdelbeginFL %DIFDELCMD < &%%%
\DIFdelendFL = \prod_{x\in \sem{X}\gamma} \sem{A}(\gamma, x)\,w \\
    \DIFaddbeginFL &\DIFaddendFL \sem{\uparrow A}\gamma \DIFdelbeginFL %DIFDELCMD < &%%%
\DIFdelendFL = \sem{A}\gamma\,\varepsilon\\
    \DIFaddbeginFL &\DIFaddendFL \sem{\equalizer{a}{f}{g}}\gamma\,w \DIFdelbeginFL %DIFDELCMD < &%%%
\DIFdelendFL = \{ a \in \sem{A}\gamma\,w \pipe \sem{f}\gamma\,w\,a = \sem{g}\gamma\,w\,a \} \\
    \DIFaddbeginFL &\DIFaddendFL \sem{L}\gamma \DIFdelbeginFL %DIFDELCMD < &%%%
\DIFdelendFL = \Grammar_0\\
    \DIFdelbeginFL %DIFDELCMD < \sem{\SPF\,X}%%%
\DIFdelFL{\gamma }\DIFdelendFL &\DIFaddbeginFL \sem{SPF\,X}\DIFaddFL{\gamma }\DIFaddendFL = \textrm{DepPolyFunctor}(\sem{X}\gamma\times \Sigma^*,\Sigma^*)\\
    \DIFaddbeginFL &\DIFaddendFL \sem{\el(F)}\gamma\,G \DIFdelbeginFL %DIFDELCMD < &%%%
\DIFdelendFL = \sem{F}\gamma\,G\\
    \DIFaddbeginFL &\DIFaddendFL \sem{\map(F)}\gamma\,f \DIFdelbeginFL %DIFDELCMD < &%%%
\DIFdelendFL = \sem{F}\gamma\,f\\
    \DIFaddbeginFL &\DIFaddendFL \sem{\mu A}\gamma \DIFdelbeginFL %DIFDELCMD < &%%%
\DIFdelendFL = \mu (\sem{A}\gamma)
  \DIFdelbeginFL %DIFDELCMD < \\
%DIFDELCMD <   \end{align*}
%DIFDELCMD <   %%%
\DIFdelendFL \DIFaddbeginFL \end{flalign*}
  \DIFaddendFL \end{footnotesize}
  \DIFaddbeginFL \end{minipage}
  \DIFaddendFL \caption{Grammar Semantics}
  \label{fig:semantics}
\end{figure}
\DIFdelbegin %DIFDELCMD < 

%DIFDELCMD < %%%
\DIFdelend The most complex part of the semantics is the interpretation of
strictly positive functors and indexed inductive linear types.  We
interpret a strictly positive functor as a \emph{dependent polynomial
functor} on the category of sets, also sometimes called an
\emph{indexed container} \cite{gambino_wellfounded_2004,altenkirch_indexed_2015}.
\begin{definition}
  Let $I$ and $O$ be sets. A (dependent) \emph{polynomial} (of sets)
  from $I$ to $O$ consists of a set of shapes $S$, a set of positions
  $P$ and functions $f : P \to I$, $g : P \to S$ and $h : S \to O$. The \emph{extension}
  of a polynomial is a functor $\Set/I \to \Set/O$ defined as the
  composite
  % https://q.uiver.app/#q=WzAsNCxbMCwwLCJcXFNldC9JIl0sWzMsMCwiXFxTZXQvTyJdLFsxLDAsIlxcU2V0L1AiXSxbMiwwLCJcXFNldC9TIl0sWzAsMiwiZl4qIl0sWzIsMywiXFxQaV9nIl0sWzMsMSwiXFxTaWdtYV9oIl1d
  \[\begin{tikzcd}
	    {\Set/I} & {\Set/P} & {\Set/S} & {\Set/O}
	    \arrow["{f^*}", from=1-1, to=1-2]
	    \arrow["{\Pi_g}", from=1-2, to=1-3]
	    \arrow["{\Sigma_h}", from=1-3, to=1-4]
  \end{tikzcd}\]
  Where $f^*$ is the pullback functor along $f$; $\Pi_g$ is the dependent
  product operation and $\Sigma_h$ is the dependent sum operation, which
  are, respectively, the right and left adjoint of their pullback functors
  $g^*$ and $h^*$. A dependent polynomial functor from $I$ to $O$ is a functor $\Set/I
  \to \Set/O$ that is naturally isomorphic to the extension of a
  dependent polynomial from $I$ to $O$.
\end{definition}
With this interpretation of $F$ as a polynomial functor, $\el(F)$ and
$\map(F)$ are just interpreted as the action of the functor on objects
and morphisms, respectively. We interpret the constructors
$\mathsf{K},\Var$, etc. on functors in the obvious way that matches
the definitional behavior of $\el$ and $\map$. The non-trivial part of
the construction is verifying that such such constructions are closed
under being polynomial. The details are tedious but straightforward
extension of prior work on dependent polynomials and indexed
containers and we have verified the construction in Agda.

We use dependent polynomials functors on sets as these are guaranteed
to have initial algebras. Further, these initial algebras are readily
constructed in our Agda implementation as an inductive type of $IW$
trees which are already available in the \DIFdelbegin \DIFdel{cubical }\DIFdelend \DIFaddbegin \DIFadd{Cubical }\DIFaddend library of Agda
\cite{The_Agda_Community_Cubical_Agda_Library_2024}.  Then an element $F \in \sem{X \to
  \SPF\,X}\gamma$ is an $\sem{X}\gamma$-indexed family of polynomial
functors from $\sem{X}\gamma\times \Sigma^*$ to $\Sigma^*$, and taking
the product of these constructs a polynomial functor from
$\sem{X}\gamma\times \Sigma^*$ to itself. Then $\sem{\mu\,F}\gamma$ is
defined to be the initial algebra of this functor, and the initial
algebra structure is used to interpret $\roll, \fold$ and the
corresponding axioms.

The remaining details of the interpretation of linear terms as parse
transformers and verification of the equational axioms \DIFaddbegin \DIFadd{--- as well as
\mbox{%DIFAUXCMD
\cref{ax:dist,ax:disjointness,ax:string-top} }\hskip0pt%DIFAUXCMD
--- }\DIFaddend is a relatively
straightforward extension of existing semantics of linear logic in
monoidal categories, and is included in the appendix\cite{seely89}.

\subsection{Agda Implementation \DIFaddbegin \Agda{\fillerlink}\DIFaddend }
\DIFdelbegin %DIFDELCMD < 

%DIFDELCMD < %%%
\DIFdelend \DIFaddbegin \label{subsec:impl}
\DIFaddend This denotational semantics in grammars and parse transformers serves
as the basis for our prototype implementation of \theoryabbv in
\DIFdelbegin \DIFdel{cubical }\DIFdelend \DIFaddbegin \DIFadd{Cubical }\DIFaddend Agda. The implementation is a shallow embedding, meaning that
rather than formalizing a syntax of \theoryabbv types and terms, we \DIFdelbegin \DIFdel{work directly with Agdatypes and a definition of a formal grammar as a function $\String \to \Set$. }\DIFdelend \DIFaddbegin \DIFadd{interpret
the non-linear universe $U$ as Cubical Agda's universe of (homotopy) sets (at
some universe level $\ell$), $\hSet~\ell$, and the linear universe $L$ as the
function type
$\StringSem \to \hSet~\ell$. We use $\hSet~\ell$ as it ensures that uniqueness of identity proofs holds for the interpretation of all types in }\theoryabbv\DIFadd{, as it does in any extensional type theory. }\DIFaddend Then we implement each of the type and
term constructors of \theoryabbv as combinators on formal grammars or
parse transformers, and use \DIFdelbegin \DIFdel{cubical }\DIFdelend \DIFaddbegin \DIFadd{Cubical }\DIFaddend Agda's equality type to model the
term equalities. Cubical Agda is convenient for this purpose as it has
built-in support for function extensionality which \DIFdelbegin \DIFdel{we use
extensively}\DIFdelend \DIFaddbegin \DIFadd{is convenient for the verification of equality rules}\DIFaddend . Axioms such as distributivity of $\oplus/\&$ and
disjointness of constructors are then provable directly in Agda, and
we are careful to only construct grammars, terms and proofs using
constructs that are possible in \theoryabbv.
\DIFaddbegin 

\DIFaddend The main difference
between our shallow embedding and \theoryabbv is that our linear terms
are written in a combinator-style, without being able to use named
variables in linear terms. \DIFaddbegin \DIFadd{For example, the function $h$ from
\mbox{%DIFAUXCMD
\cref{fig:kleeneabstractproof} }\hskip0pt%DIFAUXCMD
would be written with the fold combinator applied
to subexpressions for the $nil$ and $cons$ cases,
\(
h = fold~nil~(cons \circ id \otimes cons \circ assoc^{-1})
\). Notice in the $cons$ case of the fold, we manually reassociate
$(\literal{a} \otimes \literal{a}) \otimes \literal{a}^{*}$ to
$\literal{a} \otimes (\literal{a} \otimes \literal{a}^{*})$, then act on the
left by the identity and on the right by $cons$ to get a parse
$\literal{a} \otimes \literal{a}^{*}$, and finally we act again by $cons$ to
finally produce a parse of $\literal{a}^{*}$.
}\DIFaddend A benefit of this shallow embedding is that
the parsers are immediately available to a larger Agda development, as
they are just normal Agda code. \DIFaddbegin \DIFadd{Although, even in the small example from
\mbox{%DIFAUXCMD
\cref{fig:kleeneabstractproof} }\hskip0pt%DIFAUXCMD
we must manage additional complexity --- such as
manually reassociations --- and
this only grows more complex in larger programs. }\DIFaddend In future work we will look to
implement a type checker for a syntax closer to the presentation in
this paper, \DIFdelbegin \DIFdel{but with }\DIFdelend \DIFaddbegin \DIFadd{while maintaining }\DIFaddend the goal of \DIFdelbegin \DIFdel{still making it integrate }\DIFdelend \DIFaddbegin \DIFadd{integration }\DIFaddend with an
existing proof assistant.

%% That is, strings match $A \lto B$ if when prepended with a parse of $A$ they
%% complete to parses of $B$. In this manner, the linear function types generalize
%% Brzozowksi's notion of derivative
%% \cite{brzozowskiDerivativesRegularExpressions1964}.
%% Brzozowski initially only gave an accounting of this operation for
%% generalized regular expressions, but later work from Might et al.\ demonstrates that the same
%% construction can be generalized to context free grammars
%% \cite{mightParsingDerivativesFunctional2011}. Here, via the linear function
%% types, the same notion of derivative extends to the grammars of \theoryabbv.

%% Note, to ensure that the linear function types do indeed generalize Brzozowski
%% derivatives, we must include the equalities in \cref{fig:brzozowskideriv} as axioms\max{we must? why must we}.
%% \pedro{We should further explain why these equations are required, or at the very
%% least give some intuition}

%% Of course, all the above statements for the left function type also have
%% corresponding analogues for the
%% right-handed counterpart.

%% \begin{figure}
%% \begin{align*}
%%   c\lto c &\equiv I\\
%%   c\lto d &\equiv 0\\
%%   c\lto I &\equiv 0\\
%%   c\lto 0 &\equiv 0\\
%%   c\lto (A \otimes B) & \equiv (c\lto A) \otimes B + (A \amp I) \otimes (c\lto B)\\
%%   c\lto A^* &\equiv (A \amp I)^{*} \otimes (c \lto A) \otimes A^* \\
%%   c\lto (A + B) &\equiv (c\lto A) + (c\lto B)
%% \end{align*}
%% \caption{Equality for Brzozowski Derivatives}
%% \label{fig:brzozowskideriv}
%% \end{figure}

%% \paragraph{LNL Dependent Types}
%% \steven{talk about how binary sum and binary with are definable over $\Bool$}
%% Given a non-linear type $X$, we may form both the dependent product type
%% $\LinPiTy x X A$ and the dependent pair type $\LinSigTy x X A$ as linear types
%% where $x$ is free in $A$.

%% $\sem{\LinPiTy x X A} \gamma w = \Pi(x:\sem{X}\gamma) \sem{A}(\gamma,x) w$

%% $\sem{\LinSigTy x X A} \gamma w = \Sigma(x:\sem{X}\gamma) \sem{A}(\gamma,x) w$

%% The grammar semantics of the linear product type is
%% indeed a dependent function out of the
%% semantics of $X$. Likewise, the grammar semantics of the linear dependent pair type is a dependent
%% pair in $\Set$.\pedro{The connection to grammar semantics needs to be better explained. For instance,
%% we should explain that disjunction corresponds to ``or'' of grammars}

%% Note that even though we do not take the binary additive conjunction and
%% disjunction as primitive, we may define them via these LNL dependent types. In
%% particular, via a dependence on $\Bool$.
%% %
%% \[
%%   A \amp B := \LinPiTy b \Bool {C(b)}
%% \]
%% \[
%%   A \oplus B := \LinSigTy b \Bool {C(b)},
%% \]
%% where $C(\true) = A, C(\false) = B$.

%% \paragraph{Universal Type}\max{this is absolutely not the right terminology. Universal type means multiple other completely different things}
%% \steven{Definable as nullary product}
%% The universal type $\top$ may be formed in any context.
%% %
%% \[ \sem{\top} \gamma w = \{ \ast \}\]
%% %
%% Its grammar semantics in set outputs the unit type in $\Set$ for all input strings in all contexts.

%% \paragraph{Empty Type}
%% \steven{Definable as nullary sum}
%% The empty type $0$ has no inhabitants. The elimination for the empty type
%% witnesses the principle of explosion --- i.e.\ from a term of type $0$ we may
%% introduce a term $\mathsf{absurd}_{A} : A$ for any type $A$.
%% %
%% \[ \sem {0} \gamma w = \emptyset \]
%% %
%% \paragraph{The $G$ Modality}
%% \steven{Rename this}
%% \steven{Compare to persistence modalities from separation logic}
%% The $G$ modality provides the embedding of linear types in the empty context
%% into non-linear types. The introduction and elimination forms for $G A$ obey the
%% same rules as given in \cite{bentonMixedLinearNonlinear1995} and \cite{krishnaswami_integrating_2015}.

%% The left adjoint $F$ from non-linear types back to linear types may be defined
%% as,
%% %
%% \[
%%   F X := \LinSigTy a A I
%% \]
%% %
%% \steven{Elaborate on $G$ more and rename $G A$to $\ltonl A$ in this section}

%% \pedro{Agreed :) maybe mention connections to persistent propositions from the
%% separation logic literature}

%% The semantics of $G A$ are exactly the semantics of $A$ at the empty word $\varepsilon$.
%% %
%% \[ \sem{G A} \gamma = \sem{A} \gamma \varepsilon \]
%% %
%% %
%% \paragraph{Recursive Types}
%% \steven{reflavor this section as ``indexed inductive'' instead of ``recursive'',
%% and entirely rework with the new inductive definition}
%% \steven{The non-indexed inductive are definable as being trivially indexed over unit}
%% Recursive linear types may be defined via the least-fixed point operator $\mu$.
% The grammar semantics of which are the fixed-point of sets induced by the
% grammar semantics of $A$.
% %
% \[ \sem{\mu x. A} \gamma = \mu (x:\Gr_i). \sem{A}(\gamma,x) \]
% %
% \pedro{We should justify, even if briefly, the existence of this
%   fixed-point}

% Observe that we need not take the Kleene star as a primitive
% grammar constructor, as it is definable as a fixed point.
% The Kleene star of a grammar $g$ is given as,

% \[
%   g^{*} := \mu X . I \oplus (g \otimes X)
% \]

% \begin{figure}[h!]
% \begin{mathpar}
%   \inferrule
%   {\Gamma ; \Delta \vdash p : I}
%   {\Gamma ; \Delta \vdash \mathsf{nil} : g^{*}}

%   \and

%   \inferrule
%   {\Gamma ; \Delta \vdash p : g \\ \Gamma ; \Delta' \vdash q
%   : g^{*}}
%   {\Gamma ; \Delta \vdash \mathsf{cons}(p , q) : g^{*}}

%   \\

%   \inferrule
%   {
%     \Gamma ; \Delta \vdash p : g^{*} \\
%     \Gamma ; \cdot \vdash p_{\varepsilon} : h \\
%     \Gamma ; x : g , y : h \vdash p_{\ast} : h
%   }
%   {\Gamma ; \Delta \vdash \mathsf{foldr}(p_{\varepsilon} , p_{\ast}) : g^{*}}
% \end{mathpar}
% \caption{Kleene Star Rules}
% \label{fig:star}
% \end{figure}

% Likewise, $g^{*}$ has admissible introduction and
% elimination rules, shown in \cref{fig:star}. Note that this
% definition of $g^{*}$ and these
% rules arbitrarily assigns a handedness to the Kleene star.
% We could have just as well took it to be a fixed point of
% $I \oplus (X \otimes g)$. In fact, the definitions are
% equivalent, as the existence of the $\mathsf{foldl}$ term below
% shows that $g^{*}$ is also a fixed point of
% $I \oplus (X \otimes g)$.
% This is a marked point of difference from Kleene
% algebra with recursion, where the fixed points for the left and right variants
% of Kleene star need not agree \cite{leiss}.

% \begin{equation}
%   \label{eq:foldl}
%   \inferrule
%   {
%     \Gamma ; \Delta \vdash p : g^{*} \\
%     \Gamma ; \cdot \vdash p_{\varepsilon} : h \\
%% %     \Gamma ; y : h , x : h \vdash p_{\ast} : h
%% %   }
%% %   {\Gamma ; \Delta \vdash \mathsf{foldl}(p_{\varepsilon} , p_{\ast}) : g^{*}}
%% % \end{equation}

%% % In fact, the $\mathsf{foldl}$ term is defined using
%% % $\mathsf{foldr}$ --- much in the same way one
%% % may define a left fold over lists in terms of a right fold
%% % in a functional programming language.
%% % The underlying trick is to fold over a list of linear functions
%% % instead of the original string. We curry each character $c$
%% % of the string into a function that concatenates $c$, and
%% % right fold over this list of linear functions. Because function
%% % application is left-associative, this results in a left
%% % fold over the original string.

%% % We only take fixed points of a single variable as a
%% % primitive operation in the type theory, but we may apply
%% % Beki\`c's theorem \cite{Beki1984} to define an admissible
%% % notion of multivariate fixed point. This is particularly
%% % useful for defining grammars that encode the states of an
%% % automaton, as we will see in \cref{sec:automata}. In \cref{fig:multifix} we provide the
%% % introduction and elimination principles for such a fixed
%% % point, where $\sigma$ is the substitution that unrolls the
%% % mutually recursive definitions one level. That is,

%% % \begin{align*}
%% %   \sigma = \{ & \mu(X_{1} = A_{1} \dots, X_{n} = A_{n}).X_{1} / X_{1} , \dots, \mu(X_{1} = A_{1}, \dots, X_{n} = A_{n}).X_{n} / X_{n} \}
%% % \end{align*}

%% % \begin{figure}
%% % \begin{mathpar}
%% %   \inferrule
%% %   {\Gamma ; \Delta \vdash e : \simulsubst {A_{k}} {\sigma}}
%% %   {\Gamma ; \Delta \vdash \mathsf{cons}~e : \mu(X_{1} = A_{1}, \dots, X_{n} = A_{n}).X_{k}}

%% %   \and

%% %   \inferrule
%% %   {\Gamma ; \Delta \vdash e : \mu(X_{1} = A_{1}, \dots, X_{n} = A_{n}).X_{k} \\
%% %              \Gamma ; x_{j} : \simulsubst {A_{j}}{\sigma} \vdash e_{j} : B_{j}\quad \forall j
%% %   }
%% %   {\Gamma; \Delta \vdash \mathsf{mfold}(x_{1}.e_{1}, \dots, x_{n}.e_{n})(e) : B_{k}}
%% % \end{mathpar}
%% % \caption{Multi-fixed Points}
%% % \label{fig:multifix}
%% % \end{figure}

%% %% \section{Categorical Semantics of \theoryabbv}
%% %% \label{sec:categorify}

%% As described in previous sections, \theoryabbv has a ``standard''
%% interpretation where non-linear types denote sets, linear types denote
%% formal grammars, non-linear terms denote functions and linear terms
%% denote parse transformers.
%% %
%% Further, as is typical for type theory, every type constructor in the
%% calculus is interpreted by using a \emph{universal construction} in
%% either the category of sets or the category of grammars\footnote{the
%% only exception is that the axioms for Brzozowski derivatives we
%% considered do not follow solely from universal properties. In this
%% section we focus on models that do not necessarily satisfy these
%% axioms.}.
%% %
%% This leads to an immediate opportunity for generalization: the type
%% theory has in addition to the standard grammar-theoretic
%% interpretation, an interpretation in any categorical structure that
%% exhibits the same universal constructions.
%% %
%% On the one hand this gives us a structured way to formalize the
%% standard semantics precisely, but additionally it enables us to
%% consider \emph{non-standard} models in the next section that point to
%% further applications as well as meta-theoretic results.

%% In developing this categorical semantics, we will start with the
%% closest analogue from formal language theory: Kleene algebra.  Kleene
%% algebras are an important tool in the theory of regular languages
%% serving as a bridge between algebraic reasoning and equivalence of
%% regular expressions. More broadly, through various extensions, they
%% serve as a theoretical substrate to studying different kinds of formal
%% languages.
%% %
%% We can then see that the ``regular fragment'' of our type theory
%% (i.e., just characters, $\otimes$, $\oplus$, $0,1$ and a Kleene star)
%% has models in what we call a \emph{Kleene category} a categorification
%% of Kleene algebra from posets to categories.
%% %
%% We then further develop this into our final notion of model, which we
%% call a \emph{Chomsky category} as it can model not just regular
%% languages, but the full Chomsky hierarchy.

%% \subsection{Comparison to Kleene Algebra}
%% \subsection{Kleene Algebra and Kleene Categories}
%% A Kleene algebra is a tuple $(A, +, \cdot, (-)^*, 1, 0)$, where $A$ is
%% a set, $+$ and $\cdot$ are binary operations over $A$, $(-)^*$ is a
%% function over $A$, and $1$ and $0$ are constants. These structures
%% satisfy the axioms depicted in Figure~\ref{fig:axioms}.

%% \begin{figure}
%%   \begin{align*}
%%     x + (y + z) &= (x + y) + z & x + y &= y + x\\
%%     x + 0 &= x & x + x &= x\\
%%     x(yz) &= (xy)z & x1 &= 1x = x\\
%%     x(y + z) &= xy + xz & (x + y)z &= xz + yz\\
%%     x0 &= 0x = x & & \\
%%     1 + aa^* &\leq a^* & 1 + a^*a &\leq a^*\\
%%      b + ax \leq x &\implies a^*b \leq x &  b + xa \leq x &\implies ba^* \leq x
%%   \end{align*}
%%   \caption{Kleene algebra axioms}
%%   \label{fig:axioms}
%% \end{figure}

%% The addition operation can be used to define the partial order
%% structure $a \leq b$ if, and only if, $a + b = b$. In the theory of
%% formal languages, this order structure can be used to model language
%% containment. In this section, we categorify the concept of Kleene
%% algebra and build on top of it in order to define an abstract theory
%% of parsing. We start by defining \emph{Kleene categories}.

%% \begin{definition}
%%   A Kleene category is a distributive monoidal category $\cat{K}$
%%   such that for every objects $A$ and $B$, the endofunctors $F_{A, B}
%%   = B + A \otimes X$ and $G_{A, B} = B + X \otimes A$ have initial
%%   algebras (denoted $\mu X.\, F_{A, B}(X)$) such that $B \otimes (\mu
%%   X.\, F_{A, 1}) \cong \mu X.\, F_{A, B}(X)$ and the analogous isomorphism
%%   for $G_{A,B}$ also holds.
%% \end{definition}

%% As a sanity check, note that Kleene algebras are indeed examples of
%% Kleene categories.

%% \begin{example}
%%   Every Kleene algebra, seen a posetal category, is a Kleene category.
%%   The product $\cdot$ is a monoidal product and the addition is a
%%   least-upper bound, which corresponds to a coproduct. Lastly, the
%%   axioms of the Kleene star have a direct correspondence to the
%%   coherence conditions postulated by the initial algebras of Kleene
%%   categories.
%% \end{example}

%% This example provides a neat categorical justification to how
%% restrictive Kleene algebras are in terms of reasoning about
%% languages. By only having at most one morphism between objects, there
%% is not a lot of information they can convey. In this case, the only
%% information you get is language containment. As demonstrated by
%% $\mathbf{Gr}$, the extra degrees of freedom granted by having more
%% morphisms gives you more algebraic structure for reasoning about
%% languages.

%% For the next example, we see an unexpected connection with the theory
%% of substructural logics.

%% \begin{example}
%%   The opposite category of every Kleene category is a model of a variant of
%%   conjunctive ordered logic, where the Kleene star plays the role of the ``of
%%   course'' modality from substructural logics which allows hypotheses to
%%   be discarded or duplicated.
%% \end{example}

%% As we have seen, the proposed axioms are a direct translation of the
%% Kleene algebra axioms to a categorical setting. Its most unusual aspect is the
%% axiomatization of the Kleene star as a family of initial algebras
%% satisfying certain isomorphisms. If the Kleene category $\cat{K}$ has
%% more structure, then these isomorphisms hold ``for free''.

%% \begin{theorem}
%%   \label{th:kleeneclosed}
%%   Let $\cat{K}$ be a Kleene category such that it is also monoidal
%%   closed.  Then, the initial algebras isomorphisms hold automatically.
%% \end{theorem}
%% \begin{proof}
%%   We prove this by the unicity (up-to isomorphism) of initial
%%   algebras. Let $[hd, tl]: I + (\mu X.\, F_{A, I}(X)) \otimes A \to
%%   (\mu X.\, F_{A, I}(X))$ be the initial algebra structure of $(\mu
%%   X.\, F_{A, I}(X))$ and consider the map $[hd, tl] : B + B \otimes
%%   (\mu X.\, F_{A, I}(X)) \otimes A \to B\otimes (\mu X.\, F_{A,
%%     I}(X))$.

%%   Now, let $[f,g] : B + A \otimes Y \to Y$ be an $F_{A,B}$-algebra and
%%   we want to show that there is a unique algebra morphism $h : \mu X.\, F_{A,I} \to B \lto Y$. We can show existence and
%%   uniqueness by showing that the diagram on top commutes if, and
%%   only if, the diagram on the bottom commutes:

%% % https://q.uiver.app/#q=WzAsOCxbMCwwLCJCICsgQiBcXG90aW1lcyAoXFxtdSBYLlxcLCAxICsgWCBcXG90aW1lcyBBKSJdLFsyLDAsIkIgKyBZIFxcb3RpbWVzIEEiXSxbMiwyLCJZIl0sWzAsMiwiQiBcXG90aW1lcyAoXFxtdSBYLlxcLCAxICsgWCBcXG90aW1lcyBBKSJdLFswLDMsIjEgKyAoXFxtdSBYLlxcLCAxICsgWCBcXG90aW1lcyBBKSJdLFswLDUsIlxcbXUgWC5cXCwgMSArIFggXFxvdGltZXMgQSJdLFsyLDMsIjEgKyAoIEIgXFxsdG8gWSkgXFxvdGltZXMgQSJdLFsyLDUsIkIgXFxsdG8gWSJdLFswLDEsImlkICsgKGlkIFxcb3RpbWVzIGg7IGV2KSBcXG90aW1lcyBpZF9BIl0sWzEsMiwiW2YsZ10iXSxbMywyLCJpZCBcXG90aW1lcyBoOyBldiIsMl0sWzAsMywiW2lkIFxcb3RpbWVzIGgsIGlkIFxcb3RpbWVzIHRsXSIsMl0sWzQsNiwiaWQgKyAoaCBcXG90aW1lcyBYKVxcb3RpbWVzIGlkIl0sWzUsNywiaCIsMl0sWzYsNywiW2YnLCBnJ10iXSxbNCw1LCJbaGQsIHRsXSIsMl1d
%% \[\begin{tikzcd}
%% 	{B + B \otimes (\mu X.\, I + X \otimes A)} && {B + Y \otimes A} \\
%% 	\\
%% 	{B \otimes (\mu X.\, I + X \otimes A)} && Y \\
%% 	{I + (\mu X.\, I + X \otimes A)} && {I + ( B \lto Y) \otimes A} \\
%% 	\\
%% 	{\mu X.\, I + X \otimes A} && {B \lto Y}
%% 	\arrow["{id + (id \otimes h; ev) \otimes id_A}", from=1-1, to=1-3]
%% 	\arrow["{[f,g]}", from=1-3, to=3-3]
%% 	\arrow["{id \otimes h; ev}"', from=3-1, to=3-3]
%% 	\arrow["{[id \otimes h, id \otimes tl]}"', from=1-1, to=3-1]
%% 	\arrow["{id + (h \otimes X)\otimes id}", from=4-1, to=4-3]
%% 	\arrow["h"', from=6-1, to=6-3]
%% 	\arrow["{[f', g']}", from=4-3, to=6-3]
%% 	\arrow["{[hd, tl]}"', from=4-1, to=6-1]
%% \end{tikzcd}\]
%%   This equivalence follows by using the adjunction structure given
%%   by the monoidal closed structure of $\cat{K}$. A completely analogous
%%   argument for $G_{A,B}$ also holds. Furthermore, by generalizing the
%%   construction of \Cref{sec:formaltype}, we can also show that from
%%   the monoidal closed assumption it follows that $\mu X.\, F_{A, I}(X) \cong \mu X.\, G_{A, I}(X)$
%% \end{proof}

%% Something surprising about this lemma is that it provides an alternative
%% perspective on the observation that if a Kleene algebra has an
%% residuation operation, also called action algebra \cite{kozen1994action},
%% then the Kleene star admits a simpler axiomatization.

%% Since we want Kleene categories to generalize our notion of formal
%% grammars as presheaves $\String \to \Set$, we prove that they do
%% indeed form a Kleene category. We start by presenting a well-known
%% construction from presheaf categories.

%% \begin{definition}
%%   Let $\cat{C}$ be a locally small monoidal category and $F$, $G$ be
%%   two functors $\cat{C} \to \Set$. Their Day convolution tensor
%%   product is defined as the following coend formula:
%%   \[
%%   (F \otimes_{Day} G)(x) = \int^{(y,z) \in \cat{C}\times\cat{C}}\cat{C}(y\otimes z, x) \times F(y) \times G(z)
%%   \]
%%   Dually, its internal hom is given by the following end formula:
%%   \[
%%   (F \lto_{Day} G)(x) = \int_{y} \Set(F(y), G(x \otimes y))
%%   \]
%% \end{definition}

%% \begin{lemma}[Day \cite{day1970construction}]
%%   Under the assumptions above, the presheaf category $\Set^{\cat{C}}$ is
%%   monoidal closed.
%% \end{lemma}

%% %% \begin{theorem}
%% %%   Let $\cat{K}$ be a Kleene category and $A$ a discrete category.
%% %%   The functor category $[A, \cat{K}]$.
%% %%   (HOW GENERAL SHOULD THIS THEOREM BE? BY ASSUMING ENOUGH STRUCTURE,
%% %%   E.G. K = Set, THIS THEOREM BECOMES SIMPLE TO PROVE)
%% %% \end{theorem}
%% \begin{theorem}
%%   If $\cat{C}$ is a locally small monoidal category, then
%%   $\Set^{\cat{C}}$ is a Kleene category.
%% \end{theorem}
%% \begin{proof}

%%   By the lemma above, $\Set^{\cat{C}}$ is monoidal closed, and since it
%%   is a presheaf category, it has coproducts. Furthermore, the tensor
%%   is a left adjoint, i.e. it preserves colimits and, therefore, it is
%%   a distributive category.

%%   As for the Kleene star, since presheaf categories admit small colimits,
%%   the initial algebra of the functors $F_{A,B}$ and $G_{A,B}$ can be
%%   defined as the filtered colimit of the diagrams:

%%   From Theorem~\ref{th:kleeneclosed} it follows that these initial
%%   algebras satisfy the required isomorphisms and this concludes the
%%   proof.
%% \end{proof}

%% \begin{corollary}
%%   For every alphabet $\Sigma$, the presheaf category $\Set^{\cat{\Sigma^*}}$
%%   is a Kleene category.
%% \end{corollary}
%% \begin{proof}
%%   Note that string concatenation and the empty string make the
%%   discrete category $\Sigma^*$ a strict monoidal category.
%% \end{proof}

%% Much like in the posetal case, the abstract structure of a Kleene
%% category is expressive enough to synthetically reason about regular
%% languages. A significant difference between them is that while Kleene
%% algebras can reason about language containment, Kleene categories can
%% reason about \emph{ambiguity}, \emph{strong equivalence} of grammars.

%% \subsection{Lambek Hyperdoctrines and Chomsky Hyperdoctrines}

%% Though Kleene categories are expressive enough to reason about
%% concepts that are outside of reach of Kleene algebras, their
%% simply-typed nature makes them not so expressive from a type theoretic
%% point of view. This is limiting because type theories are successful
%% syntactic frameworks for manipulating complicated categorical
%% structures while avoiding some issues common in category theory, such
%% as coherence issues.

%% With this in mind, we want to design a categorical semantics that
%% builds on top of Kleene categories with the goal of extending them
%% with dependent types and making them capable reasoning about languages
%% and their parsers. This leads us to the abstract notion of model we
%% are interested in capturing with \theoryabbv: a \emph{Chomsky
%% category}.

%% We do this in two stages: first we define a \emph{Lambek hyperdoctrine} to be a
%% notion of model for the judgmental structure of \theoryabbv: that is we have
%% notions of linear and non-linear type, contexts, substitution and terms, but do
%% not assume any particular type constructions exist. Then we will define when a
%% Lambek hyperdoctrine is a Chomsky hyperdoctrine, meaning it can interpret all
%% the type formers of \theoryabbv and therefore arbitrary strength grammars.

%% \begin{definition}[Lambek Hyperdoctrine]
%%   A \emph{Lambek hyperdoctrine} consists of
%%   \begin{enumerate}
%%   \item A category $\mathcal C$ with a terminal object.
%%   \item A category with families structure over $\mathcal C$.
%%   \item A contravariant functor $L : \mathcal C^{o} \to \textrm{MultiCat}$ from
%%     $\mathcal C$ to the category of Multicategories.
%%   \end{enumerate}
%% \end{definition}
%% Here the objects of $\mathcal C$ model the non-linear contexts, and morphisms
%% model non-linear substitutions. The category with families structure over
%% $\mathcal C$ models the dependent non-linear types. Finally the
%% ``hyperdoctrine'' of multicategories $L$ models the linear types and terms. The
%% fact that this is functorial in $\mathcal C$ corresponds to the fact that linear
%% types are all relative to a non-linear context and that linear variables and
%% composition commute with non-linear substitution.

%% We will have three main Lambek hyperdoctrines of interest in this paper: the
%% ``standard model'' of sets and grammars, the ``syntactic model'' given by our
%% type theory itself and lastly a gluing model introduced in
%% Section~\ref{sec:canonicity} to prove the canonicity theorem.
%% \begin{example}
%%   \begin{enumerate}
%%   \item Let $\mathcal C$ be the category of sets, equipped with its usual
%%     category with families structures of families. Define $L : \mathcal C^{o}
%%     \to \textrm{MultiCat}$ to map $L(X)$ to the representable multicategory
%%     $(\Set^{\Sigma^*})^X$ of $X$-indexed families of grammars where the monoidal
%%     structure is given pointwise by the Day convolution monoidal structure.
%%   \item We can build a ``syntactic'' Lambek hyperdoctrine $\Syn(\Sigma)$ from the syntax
%%     itself: the category $\mathcal C$ is given by non-linear contexts and
%%     substitutions, the category with families by the non-linear types and terms
%%     and the hyperdoctrine of multicategories by the linear types and terms.
%%   \end{enumerate}
%% \end{example}

%% Next each type linear and non-linear type constructor corresponds to extra data
%% on a Lambek hyperdoctrine.
%% \begin{definition}
%%   \label{def:chomsky-data}
%%   \begin{enumerate}
%%   \item Dependent type structure: standard (say extensional type theory, give a reference)
%%   \item Universes
%%   \item Inductive grammars?
%%   \item Non-inductive Grammar constructors: standard monoidal category stuff
%%   \end{enumerate}
%% \end{definition}
%% \begin{definition}[Chomsky Hyperdoctrine]
%%   A Chomsky Hyperdoctrine is a Lambek hyperdoctrine equipped with all of the data in Definition~\ref{def:chomsky-data}.
%% \end{definition}
%% \pedro{We should probably define some of the words in this definition}
%% \begin{definition}
%%   A Chomsky category is a locally Cartesian category with two hierarchies of
%%   universes $\{L_i\}_{i\in \nat}$ and $\{U_i\}_{i\in \nat}$ such that
%%   every $L_i$ and $U_i$ are $U_{i+1}$-small. Furthermore, we require
%%   $U_i$ to be closed under dependent products and sum,
%%   $L_i$ to be closed under the Kleene category connectives,
%%   dependent products, left and right closed structures, with
%%   a type constructor $G : L_i \to U_i$ and a linear dependent sum
%%   going the other direction.
%% \end{definition}

%% \steven{Max suggests augmenting the definition of a Chomsky category to
%%   something like two categories $L$ and $U$, and $U$ is Cartesian closed I don't
%%   fully recall the rest.


%%   My best guess is that you take $L$ a Kleene category with a hierarchy of universes
%%   two, and you further require that $L$ is $Psh(U)$-enriched, except he
%%   suggested a further adjective on these presheaves. Perhaps representability?
%% }

%% \begin{theorem}
%%   The presheaf category $\Grammar = \Set^{\cat{\Sigma^*}}$ is a Chomsky category.
%% \end{theorem}


%% Further, the syntactic category of \theoryname is manifestly a Chomsky category.

%% \pedro{This is likely true, but if we explicitly say so, this warrants a proof. I think that
%% if we don't say anything about the syntactic category, reviewers won't mind.}

%% \steven{I agree that we don't want to say anything that opens unnecessary
%%   questions for proofs we haven't written. However, it seems hard to make the
%%   case that we have the right categorical model of the syntax if this statement
%%   isn't true. By restating our definition of Chomsky category, this should be
%%   obvious or a quick proof}

%% \pedro{I agree, then we should add the quick proof :) }

%% \subsection{Concrete Models of \theoryabbv}
%% \label{sec:othermodels}

%% \steven{Because we can define a version of semantic actions internally, we
%%   shouldn't put it as a separate model}

%% One of the powers of type theories is that it can be profitable to interpret them
%% in various models. In this section, by using our just-defined Chomsky categories,
%% we show how other useful concepts from formal language theory can also be organized
%% as models of \theoryabbv. We illustrate this point by providing two examples that
%% are closely related to the theory of formal languages: language equivalence and
%% semantic actions. Furthermore, in order to justify how $\mathbf{Gr}$ relates to
%% more traditional notions of parsing, we define a glued model that proves a
%% canonicity property of grammar terms.


%% \subsection{Language Semantics}
%% Every grammar induces a language semantics. Also languages can be taken as a
%% propositionally truncated view of the syntax. Logical equivalence should induce
%% weak equivalence, and thus even give a syntactic way to reason about language equivalence.
%% \steven{TODO language semantics}

%% % \subsection{Semantic Actions}
%% % \steven{Tentatively planning to cut this subsection for an internal representation
%% %   of semantic actions}
%% % Returning to the problem of parsing, the output of a parse usually is not the
%% % literal parse tree. Rather, the output is the result of some \emph{semantic
%% %   actions} ran on the parse tree, which usually serve to remove some syntactic
%% % details that are unnecessary for later processing.

%% % Given a grammar $G : \String \to \Set$, we define a semantic action to be a set
%% % $X$ with a function $f$ that produces a semantic element from any parse of $G$.

%% % \[
%% %   f : \PiTy w \String {G w \to X}
%% % \]

%% % Further, semantic actions can be arranged into a structured category.
%% % Define $\SemAct$ as the comma
%% % category $\Grammar / \Delta$, where $\Delta : \Set \to \Grammar$ defines a
%% % discrete presheaf. That is, for a set $X$, $\Delta (X)(w) = X$ for all
%% % $w \in \String$. As $\SemAct$ is defined as a comma category, it has a forgetful
%% % functor into $\Grammar$. That is, $\SemAct$ serves as a notion of formal
%% % grammar. Moreover, $\SemAct$ is a model of \theoryabbv.

%% % \steven{It being a notion of formal grammar is distinct from being a model. This
%% % probably warrants a proof}

%% % \steven{TODO semantic actions}

%% % \pedro{This is a very nice opportunity of showing off the supremacy of denotational
%% %   reasoning ;) We should probably prove the gluing lemma in the previous section
%% %   and apply it here and in the canonicity section. The actual proof might have to be
%% %   moved to the appendix, though}

%% \subsection{Parse Canonicity}
%% Canonicity is an important metatheoretic theorem in the type theory
%% literature.  It provides insight on the normal forms of terms and,
%% therefore, on its computational aspects. Frequently, proving
%% canonicity for boolean types, i.e. every closed term of type bool
%% reduces to either true or false, is enough to justify that the type
%% theory being studied is well-behaved. In our case, however, since we
%% want to connect \theoryabbv to parsers, we must provide a more
%% detailed account of canonicity. In particular, we give a nonstandard semantics
%% of \theoryabbv that carries a proof of canonicity along with it.

%% If $\cdot \vdash A$ is a closed linear type then there are
%% two obvious notions of what constitutes a ``parse'' of a string w
%% according to the grammar $A$:
%% \begin{enumerate}
%% \item On the one hand we have the set-theoretic semantics just
%%   defined, $\llbracket A \rrbracket \cdot w$
%% \item On the other hand, we can view the string $w = c_1c_2\cdots$ as
%%   a linear context $\lceil w \rceil = x_1:c_1,x_2:c_2,\ldots$ and
%%   define a parse to be a $\beta\eta$-equivalence class of linear terms $\cdot;
%%   \lceil w \rceil \vdash e : G$.
%% \end{enumerate}
%% It is not difficult to see that at least for the ``purely positive''
%% formulae (those featuring only the positive connectives
%% $0,+,I,\otimes,\mu, \overline\Sigma,c$) that
%% every element $t \in \llbracket A \rrbracket w$ is a kind of tree and
%% that the nodes of the tree correspond precisely to the introduction
%% forms of the type. However it is far less obvious that \emph{every}
%% linear term $\lceil w \rceil \vdash p : \phi$ is equal to some
%% sequence of introduction forms since proofs can include elimination
%% forms as well. To show that this is indeed the case we give a
%% \emph{canonicity} result for the calculus: that the parses for .

%% \begin{definition}
%%   A non-linear type $X$ is purely positive if it is built up using
%%   only finite sums, finite products and least fixed points.

%%   A linear type is purely positive if it is built up using only finite
%%   sums, tensor products, generators $c$, least fixed points and linear
%%   sigma types over purely positive non-linear types.
%% \end{definition}

%% \begin{definition}
%%   %% Let $X$ be a closed non-linear type. The closed elements $\textrm{Cl}(X)$ of $X$ are the definitional equivalence classes of terms $\cdot \vdash e : X$.

%%   Let $A$ be a closed linear type. The nerve $N(A)$ is a presheaf on
%%   strings that takes a string $w$ to the definitional equivalence
%%   classes of terms $\cdot; \lceil w\rceil \vdash e: N(A)$.
%% \end{definition}

%% \begin{theorem}[Canonicity]
%%   Let $A$ be a closed, purely positive linear type. Then there is an
%%   isomorphism between $\llbracket A\rrbracket$ and $N(A)$.
%% \end{theorem}
%% \begin{proof}
%%   We outline the proof here, more details are in the appendix. The
%%   proof proceeds first by a standard logical families construction
%%   that combines canonicity arguments for dependent type theory
%%   TODO cite coquand
%%   % \cite{coquand,etc}
%%   with logical relations constructions for linear
%%   types
%%   TODO cite hylandschalk
%%   % \cite{hylandschalk}
%%   . It is easy to see by induction that the
%%   logical family for $A$, $\hat A$ is isomorphic to $\llbracket A
%%   \rrbracket$ and the fundamental lemma proves that the projection
%%   morphism $p : \hat A \to N(A)$ has a section, the canonicalization
%%   procedure. Then we establish again by induction that
%%   canonicalization is also a retraction by observing that introduction
%%   forms are taken to constructors.
%% \end{proof}


%% \begin{enumerate}
%% \item Every term $\lceil w \rceil \vdash p : G + H$ is equal to $\sigma_1q$ or $\sigma_2 r$ (but not both)
%% \item There are no terms $\lceil w \rceil \vdash p : 0$
%% \item If there is a term $\lceil w \rceil \vdash p : c$ then $w = c$ and $p = x$.
%% \item Every term $\lceil w \rceil \vdash p : G \otimes H$ is equal to $(q,r)$ for some $q,r$
%% \item Every term $\lceil w \rceil \vdash p : \varepsilon$ is equal to $()$
%% \item Every term $\lceil w \rceil \vdash p : c$ is equal to $x:c$
%% \item Every term $\lceil w \rceil \vdash p : \mu X. G$ is equal to $\textrm{roll}(q)$ where $q : G(\mu X.G/X)$
%% \item Every term $\lceil w \rceil \vdash p : (x:A) \times G$ is equal
%%   to $(M,q)$ where $\cdot \vdash M : A$
%% \end{enumerate}

%% To prove this result we will use a logical families model. We give a
%% brief overview of this model concretely:
%% \begin{enumerate}
%% \item A context $\Gamma$ denotes a family of sets indexed by closing substitutions $\hat\Gamma : (\cdot \vdash \Gamma) \Rightarrow \Set_i$
%% \item A type $\Gamma \vdash X : U_i$ denotes a family of sets $\hat X : \Pi(\gamma:\cdot \vdash \Gamma) \hat\Gamma \Rightarrow (\cdot \vdash \simulsubst X \gamma) \Rightarrow \Set_i$
%% \item A term $\Gamma \vdash e : X$ denotes a section $\hat e : \Pi(\gamma)\Pi(\hat\gamma)\hat X \gamma \hat\gamma (\simulsubst e \gamma)$
%% \item A linear type $\Gamma \vdash A : L_i$ denotes a family of grammars $\hat A : \Pi(\gamma:\cdot\vdash\Gamma)\,\hat\Gamma \Rightarrow \Pi(w:\Sigma^*) (\cdot;\lceil w\rceil \vdash A[\gamma])\Rightarrow \Set_i$, and the denotation of a linear context $\Delta$ is similar.
%% \item A linear term $\Gamma;\Delta \vdash e : A$ denotes a function \[\hat e : \Pi(\gamma)\Pi(\hat\gamma)\Pi(w)\Pi(\delta : \lceil w \rceil \vdash \simulsubst \Delta \gamma) \hat\Delta \gamma \hat\gamma \delta \Rightarrow \hat A \gamma \hat\gamma w {(\simulsubst {\simulsubst e \gamma} \delta)}\]
%% \end{enumerate}
%% And some of the constructions:
%% \begin{enumerate}
%% \item $\widehat {(G A)} \gamma \hat\gamma e = \hat A \gamma \hat\gamma \varepsilon (G^{-1}e)$
%% \item $\widehat {(A \otimes B)} \gamma \hat\gamma w e = \Sigma(w_Aw_B = w)\Sigma(e_A)\Sigma(e_B) (e_A,e_B) = e \wedge \hat A \gamma \hat \gamma w_A e_A \times \hat B \gamma \hat \gamma w_B e_B$
%% \item $\widehat {(A \lto B)} \gamma \hat\gamma w e = \Pi(w_A)\Pi(e_A) \hat A \gamma\hat\gamma w_A e_A \Rightarrow \hat B \gamma\hat\gamma (ww_A) (\applto {e_A} e)$
%% \end{enumerate}

%% First, the category with families will be
%% the category of logical families over set contexts/types
%% $\Delta$/$A$. Then the propositional portion will be defined by
%% mapping a logical family $\hat \Gamma \to \Gamma$

%% First, let $L$ be the category of BI formulae and proofs (quotiented
%% by $\beta\eta$ equality). Define a functor $N : L \to \Set^{\Sigma^*}$ by
%% \[ N(\phi)(w) = L(w,\phi) \]

%% Then define the gluing category $\mathcal G$ as the comma category
%% $\Set^{\Sigma^*}/N$. That is, an object of this category is a pair of
%% a formula $\phi \in L$ and an object $S \in \mathcal
%% P(\Sigma^*)/N(\phi)$. We can then use the equivalence $\mathcal
%% P(\Sigma^*)/N(\phi) \cong \mathcal P(\int N(\phi))$ to get a simple
%% description of such an $S$: it is simply a family of sets indexed by
%% proofs $L(w,\phi)$:
%% \[ \prod_{w\in\Sigma^*} L(w,\phi) \to \Set \]
%% This category clearly comes with a projection functor $\pi : \mathcal
%% G \to \mathcal L$ and then our goal is to define a section by using
%% the universal property of $\mathcal L$.

%% To this end we define
%% \begin{enumerate}
%% \item $(\phi, S) \otimes (\psi, T) = (\phi \otimes \psi, S\otimes T)$ where
%%   \[ (S \otimes T)(w, p) = (w_1w_2 = w) \times (q_1,q_2 = p) \times S\,w_1\,q_1 \times T\,w_2\,q_2\]
%% \item $(\phi, S) \multimap (\psi, T) = (\phi \multimap \psi, S \multimap T)$ where
%%   \[ (S \multimap T)(w,p) = w' \to q \to S\,w'\,q \to T (ww') (p\,q) \]
%% \item $\mu X. ??$ ??
%% \end{enumerate}

%% \pedro{We should conclude this section by explaining the relevance of the canonicity
%% theorem. Could also be done before stating the theorem.}

\section{\DIFdelbegin \DIFdel{Discussion }\DIFdelend \DIFaddbegin \DIFadd{Related }\DIFaddend and Future Work}
\label{sec:discussion}
\DIFaddbegin 

\subsection{\DIFadd{Related Work}}
\DIFaddend \paragraph{Grammars as (Linear) Types}
Lambek's original syntactic calculus \cite{lambek58} describes a
logical system for linguistic derivations, and it can be given
semantics inside of a non-commutative biclosed monoidal category
\cite{lambek1988categorial}. This led to many uses of non-commutative
linear logic and lambda calculi in linguistics
\cite{buszkowskiTypeLogicsGrammar2003} --- including mechanized
categorial grammar parsers \cite{Guillaume2024,ranta-2011}. This style
of grammar formalism has gone by the names Lambek calculus or
\emph{categorial grammar}, and it is equal in expressivity to
context-free grammars.  The existing works on categorial grammar are
different in nature to our approach: they are based on non-commutative
linear logic, but their terms do not include elimination rules, and so
can only express parse trees, \DIFdelbegin \DIFdel{and so cannot be used to express
}\DIFdelend \DIFaddbegin \DIFadd{not }\DIFaddend verified parsers.

The most similar prior work to our own is Luo's Lambek calculus with
dependent types \cite{luo}. \DIFdelbegin \DIFdel{Their design differs from our own in
allowing linear
types }\DIFdelend \DIFaddbegin \DIFadd{They present two systems: one like ours where linear
types may only depend on non-linear ones, and another that
allows linear types }\DIFaddend to depend on other linear types and \DIFdelbegin \DIFdel{supporting
}\DIFdelend \DIFaddbegin \DIFadd{supports
}\DIFaddend directed $\Pi$ and $\Sigma$ types that have no analog in our
system. They do not provide a semantics for these connectives, and it
is unclear how to interpret their connectives in our grammar
semantics. Further, we provide several examples showing that
\theoryabbv is a practical system for describing grammar formalisms
and parsers, and it is unclear if these could be implemented in their calculus.

%% \subsubsection{Typed Grammars}
\DIFdelbegin \DIFdel{The usage }\DIFdelend \DIFaddbegin \DIFadd{Frisch and Cardelli introduced the use }\DIFaddend of a simple type system to reason about
regular expressions up to weak equivalence \DIFdelbegin \DIFdel{was
introduced by Frisch and Cardelli }\DIFdelend \cite{frischCardelli}.
Henglein and Nielsen \DIFdelbegin \DIFdel{\mbox{%DIFAUXCMD
\cite{henglein_regular_2011} }\hskip0pt%DIFAUXCMD
later adapt this approach by providing a type
system for regular expressions whose semantics were in one-to-one correspondence
with the set of parse trees.
Elliott \mbox{%DIFAUXCMD
\cite{elliottSymbolicAutomaticDifferentiation2021} }\hskip0pt%DIFAUXCMD
gives a similar
semantic interpretation of regular grammars as type-level predicates on strings. The type system of }\DIFdelend \DIFaddbegin \DIFadd{build on this type-theoretic view of grammars, pointing out that the values of the types correspond precisely to the parse trees \mbox{%DIFAUXCMD
\cite{henglein_regular_2011}}\hskip0pt%DIFAUXCMD
.
Our design of }\DIFaddend \theoryabbv extends \DIFdelbegin \DIFdel{these semantic interpretations of to a }\DIFdelend \DIFaddbegin \DIFadd{this view of grammars as types to a much }\DIFaddend broader class of grammars\DIFdelbegin \DIFdel{and also gives a formal }\DIFdelend \DIFaddbegin \DIFadd{, as well as providing a }\DIFaddend syntax and equational theory for \DIFdelbegin \DIFdel{the parse transformers that these prior works lack}\DIFdelend \DIFaddbegin \emph{\DIFadd{parse transformers}}\DIFadd{, showing that the terms can be viewed not just as parse trees, but as intrinsically verified parsers}\DIFaddend .

\DIFaddbegin \DIFadd{\mbox{%DIFAUXCMD
\citet{elliottSymbolicAutomaticDifferentiation2021} }\hskip0pt%DIFAUXCMD
uses essentially the same denotational semantics as ours, interpreting regular expressions as type-valued functions on strings. Our denotational semantics of }\theoryabbv \DIFadd{extends this to a much broader class of grammars.
}\DIFaddend %% Krishnaswami and Yallop \cite{krishnaswami_typed_2019} provide a type system for
%% demonstrating that carries proves of the LL(1) condition for context-free
%% grammars. Despite being a type system over formal grammars, their interpretation
%% of types is very different from the semantics of \theoryabbv.


%% 1. More practical: integrating it into a larger verified development
%%    - verified imperative implementation: Lambek logic ala separation
%%      logic?
%% 2. Semantic actions
%% 3. Type systems as tree grammars

%% \subsubsection{Kleene Algebra}
%% Since the early works in the theory of formal languages, Kleene
%% algebras have played an important role in its development. They
%% generalize the operations known from regular languages by introducing
%% operations generalizing language composition, language union and the
%% Kleene star.  More generally, they are defined as inequational theory
%% where the inequality is meant to capture language containment.  This
%% theory is extremely successful, having found applications in concurrency theory
%% \cite{hoare2009concurrent}, software-defined networks \cite{anderson2014netkat},
%% theory of programming languages \cite{angus2001kleene}, compiler optimizations
%% \cite{kozen2000certification} and more.

%% A frequently fruitful research direction is exploring varying
%% extensions of Kleene algebras, Kleene algebra with tests (KAT) being
%% one of the most notable ones \cite{kozen1997kleene}. Our approach is
%% radically different from most extensions, which usually aim at
%% modifying or adding new operations to Kleene algebras, but still
%% keeping it as an inequational theory. By adopting a category-theoretic
%% treatment and allowing the ``order structure'' to encode more
%% information than merely inequalities, we were able to extend Kleene
%% algebra to reason about parsing as well.
\DIFaddbegin \paragraph{\DIFadd{Dependent Linear Types}}
\DIFadd{Our syntax for non-commutative dependent linear type theory is based on the
dependent commutative linear type theory of
\mbox{%DIFAUXCMD
\citet{krishnaswami_integrating_2015}}\hskip0pt%DIFAUXCMD
, itself an extension of Benton's
linear-non-linear calculus \mbox{%DIFAUXCMD
\cite{benton1994}}\hskip0pt%DIFAUXCMD
. A distinct feature of these
systems is that linear types and non-linear types are distinct sorts, so the
linear logic $!$ operator is not a primitive, but is instead definable, for
instance in }\theoryabbv \DIFadd{as $!A = \LinSigTy{\_}{\ltonl A}{I}$.  V\'ak\'ar
develops a dependent linear type theory where instead the non-linear types are
accessed using the $!$ modality, similar to Girard's original approach
\mbox{%DIFAUXCMD
\cite{vakar2015,girard_linear_1987}}\hskip0pt%DIFAUXCMD
. Additionally, V\'ak\'ar develops a general
categorical semantics for their system, whereas we have only developed a single
denotational model. It appears straightforward to adapt V\'ak\'ar's general
semantics approach to a non-commutative variant that would apply to
}\theoryabbv\DIFadd{. This may have applications in finding alternative models, or
developing logical relations proofs categorically.
}\DIFaddend 

%%    - verified imperative implementation: Lambek logic ala separation
%%      logic?
\paragraph{Relation to Separation Logic}

\theoryabbv is similar in spirit to separation logic
\cite{reynolds_separation_2002}. Semantically, they are closely
related: linear types in \theoryabbv denote families of sets indexed
by a monoid of strings, whereas separation logic formulae typically
denote families of predicates indexed by an ordered partial
commutative monoid of worlds \cite{jung_higher-order_2016}. The
monoidal structure in both cases is are instances of the
category-theoretic notion of Day convolution monoidal structure
\cite{day1970construction}. From a separation-logic perspective, our
notion of memory is very primitive: a memory shape is just a string of
characters and the state of the memory is never allowed to evolve.

%% Syntactically, our type theory can be viewed as a kind of ``separation
%% type theory'' in that we care not just about provability of formulae
%% but the constructive content of our linear terms, as these act as
%% parsers. The design of \theoryabbv is based on the $\textrm{LNL}_D$
%% calculus which explicitly was designed to be a dependent
%% type-theoretic version of separation logic. Many concepts of
%% separation logic then have direct analogues in our system. The
%% characters $c$ are analogous to the ``points-to'' formulae $l \mapsto
%% x$ in separation logic. Our $\otimes,\lto,\tol$ are analogous to the
%% separating conjunction and magic-wand. The analogue of the persistence
%% modality $\square$ of separation logic is $\bigoplus_{\_:\uparrow A} I$,
%% which ``zeros out'' all but the parses from the empty string. If we
%% think of the portions of the string we are parsing as our notion of
%% resource, then these make sense as the ``resource-free'' elements of
%% the type $A$.

This semantic connection to separation logic suggests an avenue of
future work: to develop a program logic based on non-commutative
separation logic for verifying imperative implementations of
parsers. This could be implemented by modifying an existing separation
logic implementation or embedding the logic within \theoryabbv.

\DIFaddbegin \subsection{\DIFadd{Future Work}}
\DIFadd{In this work we have demonstrated feasibility of }\theoryabbv \DIFadd{for the
verification of formal grammar theory and sound-by-construction parsers. In
future work, we aim to extend }\theoryabbv \DIFadd{to be a practical tool for developing
verified parsing components of larger verified software systems.
}

\paragraph{\DIFadd{Verified Parser Generators}}
\DIFadd{We aim to extend our work on parsing to verify other algorithms. For instance,
our regular expression parser makes somewhat arbitrary choices in what parse to
produce when the grammar is ambiguous. In the future we aim to verify Frisch and
Cardelli's }\emph{\DIFadd{greedy}} \DIFadd{algorithm for regular expression
disambiguation. Defining this algorithm in }\theoryabbv \DIFadd{would provide soundness
by construction, but it is not obvious if the greediness property could be
verified easily as well.
}

\DIFadd{We also aim to adapt the approach used for our context-free grammar parsers for
$\LLL$ and $\LRR$/$\LALRR$ parsers, with the aim of developing a shared library of intrinsically
verified parsing utilities that can be usefully shared across different
parsers. We would also like to investigate how high-performance verified parsers
can be implemented in }\theoryabbv\DIFadd{. This might be done by developing an efficient
compiler for }\theoryabbv \DIFadd{directly or by developing a verified compiler to an
imperative system within }\theoryabbv \DIFadd{itself.
}

\DIFaddend \paragraph{Semantic Actions}

Our verification has mainly focused on the verification that a parser outputs a
correct concrete syntax tree for a grammar. However, in practice, parsers are
combined with a \emph{semantic action} that emits an \emph{abstract} syntax tree
that omits superfluous syntactic details that \DIFdelbegin \DIFdel{aren't needed }\DIFdelend \DIFaddbegin \DIFadd{are unneeded }\DIFaddend in later stages of
the overall program. We can define a semantic action in \theoryabbv for a linear
type $A$ with semantic outputs in a non-linear type $X$ to be a function \DIFdelbegin \DIFdel{$\ltonl {(A
  \lto \bigoplus_{\_:X} \top)}$}\DIFdelend \DIFaddbegin \DIFadd{$\ltonl
{(A \lto \LinSigTy{\_}{X}{\top})}$}\DIFaddend . That is, a semantic action is a function that
produces a semantic element of $X$ from the concrete parses of $A$. In future
work, we aim to study the question of verifying efficient implementations of
parsers with semantic actions\DIFaddbegin \DIFadd{, and integrating them into larger verified
systems}\DIFaddend .

\paragraph{Implementation}

Our Agda prototype implementation serves as a useful proof of concept
for showing what can be implemented in \theoryabbv, but it has
downsides we aim to address in future work. Firstly, it would be
preferable to work with the more intuitive type theoretic syntax we
have used in this work, rather than the combinator-style our shallow
embedding requires. Additionally, Agda itself does not have a
high-performance implementation, and so the parsers we implement in
Agda do not have competitive performance to industry parser
generators. In future work we aim to study if we can embed a proof of
the correctness of a parser generator that produces imperative
programs, and if the correctness of those imperative
programs can be proven within \theoryabbv.

\paragraph{Type Checking and Semantic Analysis}
Our focus in this work has been on the verification of parsers for
grammars over strings, but because \theoryabbv allows for the
definition of arbitrarily powerful grammars, the system could also be
used in principle for more sophisticated semantic analysis such as
scope checking or type checking. Alternatively, we could more directly
encode type type systems as linear types in a modified version of
\theoryabbv where linear types are not grammars over strings, but
\emph{type systems} over trees. This could analogously serve as a
framework for verified type checking and static analysis.

%% While parsing typically refers to the generation of semantic
%% objects from string input, many tasks in programming can be
%% viewed as parsing of objects with more structure, such as
%% trees with binding structure or graphs. Fundamental to the
%% frontend of many
%% programming language implementations are type systems. In
%% particular, \emph{type checking}
%% --- analogous to language recognition --- and \emph{typed
%%   elaboration} --- analogous to parsing --- arise when
%% producing a semantic object subject to some analysis. Just
%% as our string grammars were given as functors from $\String$
%% to $\Set$, we envision adapting the same philosophy
%% to functors from a \emph{category of trees} to $\Set$ to craft a syntax
%% that natively captures typed elaboration. This suggests an
%% unusual sort of bunched type theory, where context extension
%% no longer resembles concatenation of strings but instead
%% takes on the form of tree constructors.

% type system              ~~ Formal Grammar
% typing derivation        ~~ parse tree
% algorithmic type system  ~~ LL(1), LR(1) grammar
% Uniqueness of derivations ~~ unambiguous grammar
% type elaborator           ~~ semantic actions

\DIFdelbegin %DIFDELCMD < \bibliographystyle{plain}
%DIFDELCMD < \bibliography{refs.bib}
%DIFDELCMD < %%%
\DIFdelend \DIFaddbegin \newpage
\DIFaddend 

\DIFdelbegin %DIFDELCMD < \pagebreak
%DIFDELCMD < %%%
\DIFdelend %DIF >  \begin{acks}
%DIF >    This material is based upon work supported by the
%DIF >    \grantsponsor{GS100000001}{National Science
%DIF >      Foundation}{http://dx.doi.org/10.13039/100000001} under Grant
%DIF >    No.~\grantnum{GS100000001}{nnnnnnn} and Grant
%DIF >    No.~\grantnum{GS100000001}{mmmmmmm}.  Any opinions, findings, and
%DIF >    conclusions or recommendations expressed in this material are those
%DIF >    of the author and do not necessarily reflect the views of the
%DIF >    National Science Foundation.
%DIF >  \end{acks}

\DIFaddbegin \bibliography{refs.bib}

\newpage
\DIFaddend \ifappendix
\appendix
\DIFdelbegin %DIFDELCMD < 

%DIFDELCMD < %%%
\DIFdelend \section{Syntax}
\DIFdelbegin %DIFDELCMD < 

%DIFDELCMD < %%%
\DIFdelend \begin{figure}
\DIFaddbeginFL \begin{minipage}[t]{.35\textwidth}
  \DIFaddendFL \begin{align*}
    \el\DIFdelbeginFL \DIFdelFL{(}\DIFdelendFL \DIFaddbeginFL \left(\DIFaddendFL \Var\,M\DIFdelbeginFL \DIFdelFL{) }\DIFdelendFL \DIFaddbeginFL \right) \DIFaddendFL B &= B\,M\\
    \el\DIFdelbeginFL \DIFdelFL{(}\DIFdelendFL \DIFaddbeginFL \left(\DIFaddendFL \mathsf{K}\,A\DIFdelbeginFL \DIFdelFL{) }\DIFdelendFL \DIFaddbeginFL \right) \DIFaddendFL B &= A\\
    \el\DIFdelbeginFL \DIFdelFL{(\bigoplus A) }\DIFdelendFL \DIFaddbeginFL \left(\DIFaddFL{\bigoplus A}\right) \DIFaddendFL B &= \bigoplus_{y:Y}\el(A y)B\\
    \el\DIFdelbeginFL \DIFdelFL{(\bigamp A) }\DIFdelendFL \DIFaddbeginFL \left(\DIFaddFL{\bigamp A}\right) \DIFaddendFL B &= \bigamp_{y:Y}\el(A y)B\\
    \el\DIFdelbeginFL \DIFdelFL{(}\DIFdelendFL \DIFaddbeginFL \left(\DIFaddendFL A \otimes A'\DIFdelbeginFL \DIFdelFL{) }\DIFdelendFL \DIFaddbeginFL \right) \DIFaddendFL B &= \el(A)B \otimes \el(A')B
  \DIFdelbeginFL %DIFDELCMD < \\\\
%DIFDELCMD <     %%%
\DIFdelFL{\map(}\DIFdelendFL \DIFaddbeginFL \end{align*}
\end{minipage}%DIF > 
\begin{minipage}[t]{.65\textwidth}
\begin{align*}
    \DIFaddFL{\map}\left(\DIFaddendFL \Var\,M\DIFdelbeginFL \DIFdelFL{)}\DIFdelendFL \DIFaddbeginFL \right)\DIFaddendFL \,f &= f\,M\\
    \map\DIFdelbeginFL \DIFdelFL{(}\DIFdelendFL \DIFaddbeginFL \left(\DIFaddendFL \K\,A\DIFdelbeginFL \DIFdelFL{)}\DIFdelendFL \DIFaddbeginFL \right)\DIFaddendFL \,f &= \lambda a. a\\
    \map\DIFdelbeginFL \DIFdelFL{(\bigoplus A)}\DIFdelendFL \DIFaddbeginFL \left(\DIFaddFL{\bigoplus A}\right)\DIFaddendFL \,f &= \lambda a. \letin{\oplusinj y {a_y}}{a}{\oplusinj y {\map(A\,y)\,f\,a_y}}\\
    \map\DIFdelbeginFL \DIFdelFL{(}\DIFdelendFL \DIFaddbeginFL \left(\DIFaddendFL \bigwith\,A\DIFdelbeginFL \DIFdelFL{)}\DIFdelendFL \DIFaddbeginFL \right)\DIFaddendFL \,f &= \lambda a. \withlamb{y}{\map(A\,y)\,f\,(\withprj y a)}\\
    \map\DIFdelbeginFL \DIFdelFL{(}\DIFdelendFL \DIFaddbeginFL \left(\DIFaddendFL A \otimes A'\DIFdelbeginFL \DIFdelFL{)}\DIFdelendFL \DIFaddbeginFL \right)\DIFaddendFL \,f &= \lambda b. \letin{(a,a')}{b}{(\map(A)\,f\,a,\map(A')\,f\,a')}
  \end{align*}
\DIFaddbeginFL \end{minipage}%DIF > 
  \DIFaddendFL \caption{Strictly positive functors functorial actions}
  \DIFaddbeginFL \label{fig:spf-act}
\DIFaddendFL \end{figure}

In this section we include the elided syntactic forms, as well as
definitions and basic properties of linear and non-linear
substitution.

\DIFaddbegin \DIFadd{In \mbox{%DIFAUXCMD
\cref{fig:spf-act}}\hskip0pt%DIFAUXCMD
, we provide the functorial actions for the strictly
positive functors defined in \mbox{%DIFAUXCMD
\cref{fig:iilt}}\hskip0pt%DIFAUXCMD
. We also include the full set of
rules for non-linear types in \mbox{%DIFAUXCMD
\cref{fig:full-non-linear-types}}\hskip0pt%DIFAUXCMD
, and we provide
their judgmental equalities in \mbox{%DIFAUXCMD
\cref{fig:jdg-eq-nonlinear}}\hskip0pt%DIFAUXCMD
. Finally,
\mbox{%DIFAUXCMD
\cref{fig:jdgeq} }\hskip0pt%DIFAUXCMD
contains the judgmental equalities between linear terms in
}\theoryabbv\DIFadd{.
}

\subsection{\DIFadd{Non-linear Types}}
\DIFadd{We define sum types, list types, and $Fin~n$ from primitve non-linear types.
}

\DIFadd{For $X , Y : U$, define the sum type $X + Y$ as,
}

\[
\DIFadd{X + Y = }\SigTyLimit{b}{Bool}{elim_{Bool}(U, X, Y)(b)}
\]

\DIFadd{For $n : Nat$, define $Fin~n$ as,
}

\begin{align*}
  \DIFadd{Fin~0 }&\DIFadd{= \bot }\\
  \DIFadd{Fin~(suc~n) }&\DIFadd{= 1 + (Fin~n)
}\end{align*}

\DIFadd{For $X : U$, define $List~X$ as,
}

\[
  \DIFadd{List~X = }\SigTyLimit{n}{Nat}{\PiTyLimit{k}{Fin~n}{X}}
  \]

\DIFaddend \begin{figure}
  \DIFaddbeginFL \footnotesize
  \DIFaddendFL \begin{mathpar}
    \DIFdelbeginFL %DIFDELCMD < \inferrule{~}{\ctxwff \cdot}
%DIFDELCMD <     \and
%DIFDELCMD <     \inferrule{\ctxwff \Gamma \\ \ctxwffjdg \Gamma X}{\ctxwff {\Gamma, x : X}}
%DIFDELCMD <     \inferrule{\Gamma \vdash X : U_i}{\ctxwffjdg \Gamma X}
%DIFDELCMD < %%%
\DIFdelendFL \DIFaddbeginFL \inferrule{~}{\Gamma \vdash 1 \isSmall}
\DIFaddendFL 

    \DIFdelbeginFL %DIFDELCMD < \boxed{\linctxwffjdg \Gamma A}
%DIFDELCMD < %%%
\DIFdelendFL \DIFaddbeginFL \inferrule{~}{\Gamma \vdash Bool \isSmall}
\DIFaddendFL 

    \DIFdelbeginFL %DIFDELCMD < \inferrule{~}{\linctxwff \Gamma \cdot}
%DIFDELCMD <     \and
%DIFDELCMD <     \inferrule{\linctxwff \Gamma \Delta \\ \linctxwffjdg \gamma A}{\linctxwff \Gamma {\Delta, a : A}}
%DIFDELCMD <   %%%
\DIFdelendFL \DIFaddbeginFL \inferrule{~}{\Gamma \vdash \bot \isSmall}

    \inferrule{~}{\Gamma \vdash Nat \isSmall}

    \inferrule{\Gamma \vdash X \isSmall \and \Gamma,x:X \vdash Y \isSmall}{\Gamma \vdash \PiTyLimit {x}{X}{Y} \isSmall}

    \inferrule{\Gamma \vdash X \isSmall \and \Gamma,x:X \vdash Y \isSmall}{\Gamma \vdash \SigTyLimit {x}{X}{Y} \isSmall}

    \inferrule{\Gamma \vdash M : U}{\Gamma \vdash \unquoteTy M \isSmall}

    \inferrule{\Gamma \vdash A \isSmallLin}{\Gamma \vdash {\ltonl A} \isSmall}

    \inferrule{\Gamma \vdash X \isSmall \and \nonlinterm \Gamma M X \and
      \nonlinterm \Gamma N X}{\Gamma \vdash {M =_{X} N} \isSmall}
  \DIFaddendFL \end{mathpar}
  \caption{\DIFdelbeginFL \DIFdelFL{Context rules}\DIFdelendFL \DIFaddbeginFL \DIFaddFL{Small Non-linear Types}\DIFaddendFL }
\end{figure}

%DIF < % \begin{figure}
%DIF < %   \begin{mathpar}
%DIF < %   \boxed{\Gamma \vdash M : X}
\DIFaddbegin \begin{figure}
  \footnotesize
  \begin{mathpar}
    \inferrule{~}{\Gamma \vdash I \isSmallLin}
\DIFaddendFL 

    %DIF < %   \inferrule{~}{\Gamma, x : X, \Gamma' \vdash x : X}
%DIF < %   %
%DIF < %   \and
%DIF < %   %
%DIF < %   \inferrule{\Gamma \vdash M : Y \quad \ctxwffjdg \Gamma {X \equiv Y}}{\Gamma \vdash M : X}
%DIF < %   %
%DIF < %   \and
%DIF < %   %
%DIF < %   \inferrule{~}{\Gamma \vdash () : 1}
%DIF < %   %
%DIF < %   \and
%DIF < %   %
%DIF < %   \inferrule{\Gamma \vdash M : X \\ \Gamma \vdash N : \subst Y M x}{\Gamma \vdash (M, N) : \SigTy x X Y}
%DIF < %   %
%DIF < %   \and
%DIF < % %
%DIF < %   \inferrule{\Gamma \vdash M : \SigTy x X Y}{\Gamma \vdash \pi_1\, M : X}
%DIF < %   %
%DIF < %   \and
%DIF < %   %
%DIF < %   \inferrule{\Gamma \vdash M : \SigTy x X Y}{\Gamma \vdash \pi_2\, M : \subst Y {\pi_1\, M} x}
%DIF < %   \and
%DIF < %   \inferrule{\Gamma, x : X \vdash M : Y}{\Gamma \vdash \lamb x M : \PiTy x X Y}
%DIF < %   %
%DIF < %   \and
%DIF < %   %
%DIF < %   \inferrule{\Gamma \vdash M : \PiTy x X Y \\ \Gamma \vdash N : X}{\Gamma \vdash \app M {N} : \subst Y {N} x}
%DIF < %   %
%DIF < %   \and
%DIF < %   %
%DIF < %   \inferrule{\Gamma \vdash M \equiv N : X}{\Gamma \vdash \mathsf{refl} : M =_X N}
%DIF < %   \and
%DIF < %   \textrm{TODO: inductive types (boolean, nat) with their eliminators allowed in }
%DIF < %   \end{mathpar}
%DIF < %   \caption{Intuitionistic typing}
%DIF < %   \label{fig:inttyping}
%DIF < % \end{figure}
\DIFaddbeginFL \inferrule{c \in \Sigma}{\Gamma \vdash {\literal{c}} \isSmallLin}
\DIFaddendFL 

    \DIFaddbeginFL \inferrule{\Gamma \vdash A \isSmallLin \and \Gamma \vdash B \isSmallLin}{
      \Gamma \vdash {A \otimes B} \isSmallLin}

    \inferrule{\Gamma \vdash A \isSmallLin \and \Gamma \vdash B \isSmallLin}{
      \Gamma \vdash {A \lto B} \isSmallLin}

    \inferrule{\Gamma \vdash A \isSmallLin \and \Gamma \vdash B \isSmallLin}{
      \Gamma \vdash {A \tol B} \isSmallLin}

    \inferrule{\Gamma \vdash X \isSmall \and \Gamma,x:X \vdash A \isSmallLin}{\Gamma \vdash \LinSigTyLimit {x}{X}{A} \isSmallLin}

    \inferrule{\Gamma \vdash X \isSmall \and \Gamma,x:X \vdash A \isSmallLin}{\Gamma \vdash \LinPiTyLimit {x}{X}{A} \isSmallLin}

    \inferrule{\Gamma \vdash A \isSmallLin \and
      \Gamma \vdash B \isSmallLin \and
      \nonlinterm \Gamma f {\ltonl{(A \lto B)}} \and
      \nonlinterm \Gamma g {\ltonl{(A \lto B)}}}{\Gamma \vdash {\equalizer
        {a}{f}{g}} \isSmallLin}

    \inferrule{\Gamma \vdash M : L}{\Gamma \vdash \unquoteTy M \isSmallLin}
  \end{mathpar}
  \caption{\DIFaddFL{Small Linear Types}}
\end{figure}

\DIFaddend \begin{figure}
  \begin{mathpar}
  %DIF < 
    \DIFdelbeginFL %DIFDELCMD < \inferrule{\Gamma ; a : A , \Delta \vdash e : B}{\Gamma ; \Delta \vdash \lambtol a e : B\tol A}
%DIFDELCMD <     \and
%DIFDELCMD <     \inferrule{\Gamma ; \Delta \vdash e : B \tol A \\ \Gamma ; \Delta' \vdash e' : A}{\Gamma ; \Delta', \Delta \vdash \apptol e {e'} : B}
%DIFDELCMD <   %%%
\DIFdelendFL \DIFaddbeginFL \footnotesize
    \boxed{\inferrule{\ctxwffjdg \Gamma X}{\nonlinterm \Gamma M X}}

    \inferrule{~}{\nonlinterm {\Gamma , x : X , \Gamma'} {x} {X}}

    \inferrule{\nonlinterm \Gamma M Y \and \ctxwffjdg \Gamma {X \equiv Y}}{\nonlinterm {\Gamma} {M} {X}}

    \inferrule{~}{\nonlinterm {\Gamma} {tt} {1}}

    \inferrule
    {\nonlinterm \Gamma M \bot \and \ctxwffjdg \Gamma X}
    {\nonlinterm \Gamma {elim_{\bot}(X, M)} {X}}

    \inferrule{~}{\nonlinterm \Gamma {true} {Bool}}

    \inferrule{~}{\nonlinterm \Gamma {false} {Bool}}

    \inferrule
    {
      \ctxwffjdg {\Gamma , b : Bool} {X(b)} \and
      \nonlinterm \Gamma {M_{0}} {X(false)} \and
      \nonlinterm \Gamma {M_{1}} {X(true)}
    }
    {\nonlinterm
        {\Gamma, b : Bool}
        {elim_{Bool}(X, M_0, M_1)(b)}
        {X(b)}
    }

    \\

    \inferrule{~}{\nonlinterm {\Gamma} {0} {Nat}}

    \inferrule{\nonlinterm {\Gamma} {n} {Nat}}{\nonlinterm {\Gamma} {suc~n} {Nat}}

    \inferrule{\ctxwffjdg {\Gamma , n : Nat} {X(n)} \and
           \nonlinterm {\Gamma} {M_0} {X(0)} \and
           \nonlinterm {\Gamma, n : Nat, x : X(n)} {M_{suc}(n , x)} {X(suc~n)}}
         {\nonlinterm
           {\Gamma, n : Nat}
           {elim_{Nat}(X,M_0,M_{suc})(n)}
           {X(n)}}

    \\

    \inferrule{\nonlinterm \Gamma M X \and \nonlinterm \Gamma {N} {\subst Y {M}
        {x}}}{\nonlinterm {\Gamma} {(M , N)} {\SigTyLimit {x} {X} {Y}}}

    \inferrule{\nonlinterm {\Gamma} {M} {\SigTyLimit {x} {X} {Y}}}
              {\nonlinterm {\Gamma} {M.fst} X}

    \inferrule{\nonlinterm {\Gamma} {M} {\SigTyLimit {x} {X} {Y}}}
              {\nonlinterm {\Gamma} {M.snd} {\subst Y {M.fst} {x}}}

    \inferrule{\ctxwffjdg \Gamma {\PiTyLimit {x} {X} {Y}} \and \nonlinterm
      {\Gamma , x : X} {M} {Y}}
              {\nonlinterm {\Gamma} {\lamb {x} {M}} {\PiTyLimit {x} {X} {Y}}}

    \inferrule{\nonlinterm {\Gamma} {M} {\PiTyLimit {x} {X} {Y}} \and
      \nonlinterm \Gamma N X}
              {\nonlinterm \Gamma {M~N} {\subst Y {N} {x}}}

    \inferrule{\ctxwffjdg \Gamma X}{\nonlinterm \Gamma {\quoteTy X} U}

    \inferrule{\linctxwffjdg \Gamma A}{\nonlinterm \Gamma {\quoteTy A} L}

    \inferrule{\linterm \Gamma \cdot e A}{\nonlinterm \Gamma e \ltonl{A}}

  \DIFaddendFL \end{mathpar}
  \caption{\DIFdelbeginFL \DIFdelFL{Linear terms (extended)}\DIFdelendFL \DIFaddbeginFL \DIFaddFL{Non-linear Typing}\DIFaddendFL }
  \DIFaddbeginFL \label{fig:full-non-linear-types}
\DIFaddendFL \end{figure}

\begin{figure}
  \DIFaddbeginFL \footnotesize
  \DIFaddendFL \begin{mathpar}
    \DIFdelbeginFL %DIFDELCMD < \boxed{\Gamma ; \Delta \vdash e \equiv e' : A}
%DIFDELCMD < %%%
\DIFdelendFL \DIFaddbeginFL \boxed{
      \inferrule
      {\nonlinterm \Gamma M X \and
       \nonlinterm \Gamma N X}
      {\nonlinterm \Gamma {M \equiv N} {X}}}
\DIFaddendFL 

  \DIFaddbeginFL \inferrule{\nonlinterm \Gamma M 1 \and \nonlinterm \Gamma N 1}{\nonlineq \Gamma M N 1}

  %DIF >  \inferrule{\nonlinterm \Gamma M \bot \and \nonlinterm \Gamma N \bot}{\nonlineq \Gamma M N \bot}

  \inferrule{\nonlinterm {\Gamma , x : X} {M} {Y} \and \nonlinterm {\Gamma} {N} {X}}{\nonlineq \Gamma {\left(\lamb {x} {M} \right)~N} {\subst M {N}
      {x}} {X}}

  \inferrule{\nonlinterm \Gamma M {\PiTyLimit {x}{X}{Y}}}{\nonlineq \Gamma {M} {\lamb {x} {M~x}} {\PiTyLimit{x}{X}{Y}}}

  \inferrule{\nonlinterm \Gamma M {X} \and \nonlinterm {\Gamma , x : X} {N}
    {\subst Y {M} {x}}}{\nonlineq \Gamma {(M , N).fst} {M} {X}}

  \inferrule{\nonlinterm \Gamma M {X} \and \nonlinterm {\Gamma , x : X} {N}
    {\subst Y {M} {x}}}{\nonlineq \Gamma {(M , N).snd} {N} {\subst Y {M} {x}}}

  \inferrule{\nonlinterm \Gamma {M} {\SigTy {x} {X} {Y}}}{\nonlineq \Gamma {(M.fst , M.snd)} {M} {\SigTyLimit{x}{X}{Y}}}


  \inferrule
  {
    \nonlinterm \Gamma {M_{0}} {X(false)} \and
    \nonlinterm \Gamma {M_{1}} {X(true)}
  }
  {\nonlineq
      {\Gamma}
      {elim_{Bool}(X, M_0,M_1)(false)}
      {M_0}
      {X(b)}
  }

  \inferrule
  {
    \nonlinterm \Gamma {M_{0}} {X(false)} \and
    \nonlinterm \Gamma {M_{1}} {X(true)}
  }
  {\nonlineq
      {\Gamma}
      {elim_{Bool}(X, M_0,M_1)(true)}
      {M_1}
      {X(b)}
  }

  \inferrule{\nonlinterm {\Gamma} {M_0} {X(0)} \and
      \nonlinterm {\Gamma, n : Nat, x : X(n)} {M_{suc}(n , x)} {X(suc~n)}}
      {\nonlineq {\Gamma} {elim_{Nat}(X,M_0,M_{suc})(0)} {M_0} {X(0)}}

  \inferrule{\nonlinterm {\Gamma} {M_0} {X(0)} \and
      \nonlinterm {\Gamma, n : Nat, x : X(n)} {M_{suc}(n , x)} {X(suc~n)}
    }
      {\nonlineq {\Gamma, n : Nat} {elim_{Nat}(X,M_0,M_{suc})(suc~n)} {M_{suc}(n ,
          elim_{Nat}(X,M_0,M_{suc})(n))} {X(suc~n)}}

  \inferrule{\nonlinterm \Gamma M U}{\nonlineq \Gamma {\quoteTy {\unquoteTy {M}}}
  {M} {U}}

  \inferrule{\nonlinterm \Gamma M L}{\nonlineq \Gamma {\quoteTy {\unquoteTy {M}}}
  {M} {L}}
  \end{mathpar}
  \caption{\DIFaddFL{Judgmental equality for non-linear terms}}
  \label{fig:jdg-eq-nonlinear}
\end{figure}

\begin{figure}
  \footnotesize
  \begin{mathpar}
    \boxed{
      \inferrule
      {\linterm \Gamma \Delta e A \and
       \linterm \Gamma \Delta e' A}
      {\Gamma ; \Delta \vdash e \equiv e' : A}}

    \DIFaddendFL % \inferrule{~}{\Gamma; \cdot \vdash G^{-1}\, (G \, t ) \equiv t: A}
%
%
    \inferrule{\Gamma ; \Delta , a : A \vdash e : C \\ \Gamma ; \Delta' \vdash e' : A }{\Gamma; \Delta,\Delta' \vdash \app {(\lamblto a e)} {e'} \equiv \subst e {e'} a : C}
%
    \and
%
    \inferrule{\Gamma ; \Delta \vdash e : A \lto B}{\Gamma; \Delta \vdash e \equiv \lamblto a {\app e a} : A \lto B}
%
    \and
%
    \DIFdelbeginFL %DIFDELCMD < \inferrule{\Gamma ; a : A , \Delta \vdash e : C \\ \Gamma ; \Delta' \vdash e' : A }{\Gamma; \Delta , \Delta' \vdash \app {(\lambtol a e)} {e'} \equiv \subst e {e'} a : C}
%DIFDELCMD < %%%
\DIFdelendFL \DIFaddbeginFL \inferrule{\Gamma ; a : A , \Delta \vdash e : C \\ \Gamma ; \Delta' \vdash e' : A }{\Gamma; \Delta , \Delta' \vdash \apptol {(\lambtol a e)} {e'} \equiv \subst e {e'} a : C}
\DIFaddendFL %
    \and
%
    \DIFdelbeginFL %DIFDELCMD < \inferrule{\Gamma ; \Delta \vdash e : B \tol A}{\Gamma; \Delta \vdash e \equiv \lambtol a {\app e a} : B \tol A}
%DIFDELCMD < %%%
\DIFdelendFL \DIFaddbeginFL \inferrule{\Gamma ; \Delta \vdash e : B \tol A}{\Gamma; \Delta \vdash e \equiv \lambtol a {\apptol e a} : B \tol A}
\DIFaddendFL %
    \and
%
    \inferrule{\Gamma , x : X \vdash e : A \\ \Gamma \vdash M : X}{\Gamma; \Delta \vdash \app {(\dlamb x e)} {M} \equiv \subst {e} M x : C}
%
    \and
%
    \DIFdelbeginFL %DIFDELCMD < \inferrule{\Gamma ; \Delta \vdash e : \LinPiTy {x} {X} {A}}{\Gamma; \Delta \vdash e \equiv \dlamb x {\app e x} : \LinPiTy x X A}
%DIFDELCMD < %%%
\DIFdelendFL \DIFaddbeginFL \inferrule{\Gamma ; \Delta \vdash e : \LinPiTyLimit {x} {X} {A}}{\Gamma; \Delta \vdash e \equiv \dlamb x {\app e x} : \LinPiTyLimit x X A}
\DIFaddendFL %
    \and
%
%     \inferrule{~}{\Gamma; \Delta \vdash e \equiv e' : \top}
% %
%     \and
%
%     \inferrule{~}{\Gamma; \Delta \vdash e_i \equiv \pi_i (e_1, e_2) : A_i}
% %
%     \and
% %
%     \inferrule{~}{\Gamma; \Delta \vdash e \equiv (\pi_1 e, \pi_2 e) : A\& B}
%
    \and
%
    \inferrule{\Gamma ; \Delta_1 , \Delta_2 \vdash e : C}{\Gamma; \Delta_1 , \Delta_2 \vdash \letin {()} {()} e \equiv e : C}
    %
    \and
    %
    \inferrule{\Gamma ; \Delta_2 \vdash e : I \\ \Gamma ; \Delta_1 , a : A , \Delta_3 \vdash e' : C}{\Gamma; \Delta_1 , \Delta_3 \vdash \letin {()} e {\subst {e'} {()} a} \equiv \subst {e'} e a : C}
%
    \and
%
    \inferrule{\Gamma ; \Delta_2 \vdash e : A \\ \Gamma ; \Delta_3 \vdash e' : B \\ \Gamma ; \Delta_1 , a : A , b : B , \Delta_4 \vdash e'' : C}{\Gamma; \Delta_1 , \Delta_2, \Delta_3 , \Delta_4 \vdash \letin {(a, b)} {(e , e')} e'' \equiv e'' \{ e/a , e'/b \} : C}
%
    \and
    %
    \inferrule{\Gamma ; \Delta_2 \vdash e : A \otimes B \\ \Gamma ; \Delta_1 , c : A \otimes B , \Delta_3 \vdash e' : C }{\Gamma; \Delta_1, \Delta_2, \Delta_3 \vdash \letin {(a , b)} e {\subst {e'} {(a, b)} c} \equiv \subst {e'} e c : C}
%
    \and
%
    \inferrule{\Gamma \vdash M : X \\ \Gamma ; \Delta_2 \vdash e : A \\ \Gamma , x : X \vdash \Delta_1 , a : A , \Delta_3 }{\Gamma;\Delta_1 , \Delta_2 , \Delta_3 \vdash \letin {\sigma~x~a} {\sigma~M~e} {e'} \equiv e' \{ M/x , e/a \} : C}
    %
    \and
    %
    \DIFdelbeginFL %DIFDELCMD < \inferrule{\Gamma ; \Delta_1, y : \LinSigTy {x} {X} {A}, \Delta_2 \vdash e' : C \\ \Gamma ; \Delta_2 \vdash e : \LinSigTy {x} {X} {A}}{\Gamma; \Delta_1 , \Delta_2 , \Delta_3 \vdash \letin {\sigma~x~a} e {\subst {e'} {\sigma~x~a} y} \equiv \subst {e'} e y : C}
%DIFDELCMD <     %%%
\DIFdelendFL \DIFaddbeginFL \inferrule{\Gamma ; \Delta_1, y : \LinSigTyLimit {x} {X} {A}, \Delta_2 \vdash e' : C \\ \Gamma ; \Delta_2 \vdash e : \LinSigTyLimit {x} {X} {A}}{\Gamma; \Delta_1 , \Delta_2 , \Delta_3 \vdash \letin {\sigma~x~a} e {\subst {e'} {\sigma~x~a} y} \equiv \subst {e'} e y : C}
    \DIFaddendFL \and
    %
    \inferrule
    {\Gamma ; \Delta \vdash e : A \\ \Gamma ; \Delta \vdash f~e \equiv g~e}
    {\Gamma ; \Delta \vdash \equalizerpi {\equalizerin {e}} \equiv e : A}
    %
    \and
    %
    \inferrule
    {\Gamma ; \Delta \vdash e : \equalizer e f g}
    {\Gamma ; \Delta \vdash \equalizerin {\equalizerpi {e}} \equiv e :
      \equalizer e f g}
\end{mathpar}
  \caption{Judgmental equality for linear terms}
  \label{fig:jdgeq}
\end{figure}
\DIFdelbegin %DIFDELCMD < \steven{Need to include premises for the judgmental equality rules}
%DIFDELCMD < %%%
\DIFdelend 

\DIFaddbegin \subsection{\DIFadd{Substitutions}}
\DIFaddend \begin{definition}
  The set of (non-linear) substitutions $\gamma \in
  \textrm{Subst}(\Gamma,\Gamma')$ where $\Gamma \isCtx$ and $\Gamma'
  \isCtx$ is defined by recursion on $\Gamma$:
  \begin{align*}
    \textrm{Subst}(\Gamma,\cdot) &= \{\cdot \}\\
    \textrm{Subst}(\Gamma,\Gamma',x:A) &= \{ (\gamma,M/x) \pipe \gamma \in \textrm{Subst}(\Gamma,\Gamma') \wedge \Gamma \vdash M : A[\gamma] \}
  \end{align*}
  simultaneously with an action of substitution on types, terms,
  etc. in the standard way.

  %% \max{do we need identity and composition of substitutions and their functoriality?}
  %% By induction we can define an identity substitution
  %% $\textrm{Subst}(\Gamma,\Gamma)$ which maps each variable to
  %% itself. We write this substitution simply as $\Gamma$.
  %% \begin{align*}
  %%   \cdot \circ \gamma' &= \cdot\\
  %%   (\gamma,M/x) \circ \gamma' &= (\gamma\circ \gamma') , M[\gamma']/x
  %% \end{align*}

  It is straightforward, but laborious to establish that all forms in
  the type theory that are parameterized by a non-linear context
  $\Gamma$ support the admissible actions of a substitution $\gamma
  \in \Subst(\Gamma',\Gamma)$ \DIFdelbegin \DIFdel{in \mbox{%DIFAUXCMD
\Cref{fig:non-linear-substitution}}\hskip0pt%DIFAUXCMD
}\DIFdelend \DIFaddbegin \DIFadd{given in \mbox{%DIFAUXCMD
\cref{fig:non-linear-substitution,fig:substact}}\hskip0pt%DIFAUXCMD
}\DIFaddend .
  %% Further, these all satisfy identity and composition equations M[\Gamma] = M and M[\gamma][\gamma'] = M[(\gamma \circ \gamma')]
\end{definition}

\begin{figure}
  \DIFaddbeginFL \footnotesize
  \DIFaddendFL \begin{mathpar}
    \DIFdelbeginFL %DIFDELCMD < \inferrule
%DIFDELCMD <     {\Gamma \vdash X \isTy}
%DIFDELCMD <     {\Gamma' \vdash X[\gamma] \isTy}
%DIFDELCMD < %%%
\DIFdelendFL \DIFaddbeginFL \inferrule
    {\ctxwffjdg \Gamma X}
    {\ctxwffjdg {\Gamma'} {X[\gamma]}}
\DIFaddendFL 

    \inferrule
    {\Gamma \vdash X \isSmall}
    {\Gamma' \vdash X[\gamma] \isSmall}

    \DIFdelbeginFL %DIFDELCMD < \inferrule
%DIFDELCMD <     {\Gamma \vdash X \equiv Y}
%DIFDELCMD <     {\Gamma' \vdash X[\gamma] \equiv Y[\gamma]}
%DIFDELCMD < %%%
\DIFdelendFL \DIFaddbeginFL \inferrule
    {\ctxwffjdg \Gamma {X \equiv Y}}
    {\ctxwffjdg {\Gamma'} {X[\gamma] \equiv Y[\gamma]}}
\DIFaddendFL 

    \DIFdelbeginFL %DIFDELCMD < \inferrule
%DIFDELCMD <     {\Gamma \vdash M : X}
%DIFDELCMD <     {\Gamma' \vdash M[\gamma] : X[\gamma]}
%DIFDELCMD < %%%
\DIFdelendFL \DIFaddbeginFL \inferrule
    {\nonlinterm \Gamma {M} {X}}
    {\nonlinterm {\Gamma'} {M[\gamma]} {X[\gamma]}}
\DIFaddendFL 

    \DIFdelbeginFL %DIFDELCMD < \inferrule
%DIFDELCMD <     {\Gamma \vdash M \equiv N : X}
%DIFDELCMD <     {\Gamma' \vdash M[\gamma] \equiv N[\gamma] : X[\gamma]}
%DIFDELCMD < %%%
\DIFdelendFL \DIFaddbeginFL \inferrule
    {\nonlineq \Gamma {M} {N} {X}}
    {\nonlineq {\Gamma'} {M[\gamma]} {N[\gamma]} {X[\gamma]}}
\DIFaddendFL 

    \inferrule
    {\Gamma \vdash \Delta \isLinCtx}
    {\Gamma' \vdash \Delta[\gamma] \isLinCtx}

    \inferrule
    {\Gamma \vdash A \isLinTy}
    {\Gamma' \vdash A[\gamma] \isLinTy}

    \DIFdelbeginFL %DIFDELCMD < \inferrule
%DIFDELCMD <     {\Gamma \vdash A \equiv B}
%DIFDELCMD <     {\Gamma' \vdash A[\gamma] \equiv B[\gamma]}
%DIFDELCMD < %%%
\DIFdelendFL \DIFaddbeginFL \inferrule
    {\linctxwffjdg \Gamma {A \equiv B}}
    {\linctxwffjdg {\Gamma'} {A[\gamma] \equiv B[\gamma]}}
\DIFaddendFL 

    \inferrule
    {\Gamma;\Delta \vdash e : A}
    {\Gamma'; \Delta[\gamma] \vdash e[\gamma] : A[\gamma]}
\DIFdelbeginFL \DIFdelFL{j
}\DIFdelendFL 

    \inferrule
    {\Gamma;\Delta \vdash e \equiv f : A}
    {\Gamma'; \Delta[\gamma] \vdash e[\gamma] \equiv f[\gamma] : A[\gamma]}
  \DIFdelbeginFL \DIFdelFL{j
  }\DIFdelendFL \end{mathpar}  
  \caption{Non-linear substitution}
  \label{fig:non-linear-substitution}
\end{figure}

\begin{definition}
  Let $\Gamma \vdash \Delta \isLinCtx$ and $\Gamma \vdash \Delta'
  \isLinCtx$. The set of linear substitutions $\Subst(\Delta',\Delta)$
  is defined by recursion on $\Delta$:
  \begin{align*}
    \Subst(\Delta',\cdot) &= \{ \cdot \pipe \Delta' = \cdot \}\\
    \Subst(\Delta',(\Delta,a:A)) &= \{ (\delta, e/a) \pipe \delta \in \Subst(\Delta_1, \Delta) , \Delta_2 \vdash e : A , \Delta' = (\Delta_1,\Delta_2)\}
  \end{align*}

  Given substitutions $\delta_1 \in \Subst(\Delta_1', \Delta_1)$ and
  $\delta_2 \in \Subst(\Delta_2', \Delta_2)$, we can define a
  substitution $\delta_1,\delta_2 \in
  \Subst((\Delta_1',\Delta_2'),(\Delta_1,\Delta_2))$. Furthermore, for
  any substitution $\delta \in \Subst(\Delta,(\Delta_1,\Delta_2))$, we
  can deconstruct $\delta = \delta_1,\delta_2$ with $\delta_1 \in
  \Subst(\Delta_1', \Delta_1)$ and $\delta_2 \in \Subst(\Delta_2',
  \Delta_2)$.
  %% Again for any $\Gamma \vdash \Delta$, we can define an identity
  %% substitution mapping each variable to itself. Additionally we can define composition of 
\end{definition}

\begin{figure}
  \DIFaddbeginFL \footnotesize
  \DIFaddendFL \begin{align*}
    a[e/a] &= e\\
    (e_1,e_2)[\delta_1,\delta_2] &= (e_1[\delta_1], e_2[\delta_2])\\
    (\letin {(a , b)} e {e'})[\delta_1,\delta_2,\delta_3] &= \letin{(a,b)} {e[\delta_2]}{e'[\delta_1,a/a,b/b,\delta_2]}\\
    ()[\cdot] &= ()\\
    \letin{()} e {e'}[\delta_1,\delta_2,\delta_3] &= \letin{()} {e[\delta_2]}{e'[\delta_1,a/a,b/b,\delta_2]}\\
    (\lamblto {a} e)[\delta] &= \lamblto a {e[\delta,a/a]}\\
    (\applto {e'} {e})[\delta_1,\delta_2] &= \applto {e'[\delta_1]} {e[\delta_2]}\\
    (\lamblto {a} e)[\delta] &= \lamblto a {e[\delta,a/a]}\\
    (\applto {e'} {e})[\delta_1,\delta_2] &= \applto {e'[\delta_1]} {e[\delta_2]}\\
    (\lambtol {a} e)[\delta] &= \lamblto a {e[a/a,\delta]}\\
    (\apptol {e'} {e})[\delta_1,\delta_2] &= \apptol {e'[\delta_1]} {e[\delta_2]}\\
    (\dlamb x e)[\delta] &= \dlamb x {e[\delta]}\\
    (e\,.\pi\,M)[\delta] &= (e[\delta]\,.\pi\,M)\\
    (\sigma\,M\,e)[\delta] &= \sigma\,M\,e[\delta]\\
    (\letin{\sigma\,x\,a}{e}{e'})[\delta_1,\delta_2,\delta_3] &= \letin{\sigma\,x\,a} {e[\delta_2]}{e'[\delta_1,\delta_3]}\\
    (\equalizerin{e})[\delta] &= \equalizerin{e[\delta]}\\
    (\equalizerpi{e})[\delta] &= \equalizerpi{e[\delta]}
  \end{align*}
\caption{Action of substitution on linear terms}
\label{fig:substact}
\end{figure}
\DIFdelbegin %DIFDELCMD < \steven{captilizataion should be consistent over all figures. idk if all caps or
%DIFDELCMD < all lowercase is beter?}
%DIFDELCMD < %%%
\DIFdelend 

\begin{definition}
  Given any $\Gamma ; \Delta \vdash e : A$ and $\delta \in
  \Subst(\Delta',\Delta)$, we define the action of the substitution on
  $e$ in \cref{fig:substact}, frequently using the inversion principle to split
  the substitution into constituent components.
  By induction on linear term and equality judgments, we establish the
  following admissible rules for $\delta \in \Subst(\Delta',\Delta)$:
  \begin{mathpar}
    \inferrule
    {\Gamma; \Delta \vdash e : A}
    {\Gamma; \Delta' \vdash e[\delta] : A}

    \inferrule
    {\Gamma; \Delta \vdash e \equiv f : A}
    {\Gamma; \Delta' \vdash e[\delta] \equiv f[\delta'] : A}
  \end{mathpar}

  %% \max{identity and composition?}
\end{definition}

\section{Denotational Semantics}
Here we extend the denotational semantics from
\cref{sec:semantics-and-metatheory} to cover all of \theoryname
syntax. Here we will freely use that the category of grammars is a
complete, co-complete biclosed monoidal category, and use categorical
notation for the constructions in the denotational semantics. For
example, we will use the same notation $I,\otimes,\lto,\tol$ for the
biclosed monoidal structure of $\Grammar$ that we do for the
corresponding syntactic notions.

\begin{definition}[Denotation of Linear Contexts]
  The semantics of linear contexts $\Gamma \vdash \Delta \isLinCtx$ is
  defined as follows:
  \max{make sure this aligns with our interpretation of $\lto$/$\tol$}
  \begin{align*}
    \sem{\cdot}\,\gamma &= I \\
    \sem{\Delta,x:A}\,\gamma &= \sem{\Delta}\,\gamma \otimes \sem{A}\gamma \\
  \end{align*}
\end{definition}

\begin{definition}[Denotation of Linear Substitutions]
  The semantics of a linear substitution
  $\delta : \textrm{Subst}(\Delta' , \Delta)$ are given as maps
  $\semg{\delta} : \semg{\Delta'} \to {\Delta}$. Define $\semg{\delta}$ by
  recursion on $\delta$:
  \begin{align*}
    \semg{\cdot} &= id_{I} \\
    \semg{\delta,e/a} &= \semg{\delta} \otimes \semg{e} \circ m_{\Delta_{1},\Delta_{2}}
  \end{align*}
where $\Delta_{2} \vdash e : A$ and $\Delta' = \Delta_{1},\Delta_{2}$.
\end{definition}

\begin{theorem}
  For any $\Gamma \vdash \Delta_1,\Delta_2 \isLinCtx$ and $\gamma \in
  \sem{\Gamma}$ there is a natural isomorphism $m_{\Delta_1,\Delta_2} :
  \sem{\Delta_1, \Delta_2}\gamma \cong \sem{\Delta_1}\gamma \otimes
  \sem{\Delta_2}\gamma$.

  This can be extended to a sequence of contexts of any length.
\end{theorem}
\begin{proof}
Construct $m_{\Delta_{1},\Delta_{2}}$ by recursion on $\Delta_{2}$.
\begin{align*}
  m_{\Delta_{1},\cdot} & = \rho^{-1} \\
  m_{\Delta_{1},(\Delta_{2}, a : A)} & = \alpha \circ m_{\Delta_{1},\Delta_{2}} \otimes id
\end{align*}
\end{proof}

\begin{lemma}
  \label{lem:subst}
  For each term $\Delta \vdash e : A$ and substitution
  $\delta : \textrm{Subst}(\Delta', \Delta)$, the semantics of $\delta$ acting
  on $e$ splits into the composition $\semg{e[\delta]} = \semg{e} \circ \semg{\delta}$.
\end{lemma}

\subsection{Grammar Semantics for Linear Terms}
Here we define denotations of linear terms. Note that the denotations
interpret typing derivations, not raw terms, as the data of how
contexts are split is needed in order to construct the correct
associator functions. Further, we demonstrate that the denotational
semantics respects the equational theory of \theoryabbv. The
correctness of the equational theory heavily relies on the
\emph{coherence theorem} for monoidal categories. The coherence
theorem says that any diagram in a monoidal category constructed using
only associators $\alpha_{A,B,C} : (A \otimes B) \otimes C \cong A
\otimes (B \otimes C)$, unitors $\rho_A : A \otimes I \cong A$ and
$\lambda_A : I \otimes A \cong A$ and compositions and tensor products
of these, commutes. We call a morphism built in this way a
\emph{generalized associator}.

\subsubsection{Variables}
Note that the denotation of a singleton context $a : A$ is given as
\[
  \semg{a : A} = \semg{\cdot , a : A} = I \otimes \semg{A}
\]
\DIFdelbegin %DIFDELCMD < 

%DIFDELCMD < %%%
\DIFdelend So for $a: A \vdash a : A$, the denotation of a single variable term
$\semg{a} : \semg{a : A} \to \semg{A}$ is given by the left unitor
\[
  \semg{a} = \lambda
\]

\subsubsection{Linear Unit}
% I
\paragraph{$I$-Introduction}
$\semg{()} : \semg{\cdot} \to \semg{I}$.
\[
\semg{()} = id_{I}
\]

\paragraph{$I$-Elimination}
$\semg{\letin {()} e e'} : \semg{\Delta'_{1},\Delta,\Delta'_{2}} \to \semg{C}$
defined in the following diagram,

\begin{center}
\begin{tikzcd}
  \semg{\Delta'_1,\Delta,\Delta'_2} \arrow[r , "m_{(\Delta'_1,\Delta),\Delta'_2}"] &
  \semg{\Delta'_1,\Delta} \otimes \semg{\Delta'_2}
    \arrow[r , "m_{\Delta'_1,\Delta} \otimes id"] &
  \left(\semg{\Delta'_1} \otimes \semg{\Delta} \right) \otimes \semg{\Delta'_2}
    \arrow[d , "(id \otimes \semg{e}) \otimes id"] \\
    \semg{\Delta'_1,\Delta'_2} \arrow[d , "\semg{e'}"]
  & \arrow[l , "m^{-1}_{\Delta'_1,\Delta'_2}"]
    \semg{\Delta'_1} \otimes \semg{\Delta'_2}
  & \left(\semg{\Delta'_1} \otimes I \right) \otimes \semg{\Delta'_2}
  \arrow[l , "\rho \otimes id"] \\
  \semg{C}
\end{tikzcd}
\end{center}

We demonstrate that the denotations of the introduction and elimination forms
for $I$ obey the $\beta$ and $\eta$ equalities for $I$.

\paragraph{$I\beta$}
Given
$\Delta'_{1},\cdot,\Delta'_{2} \vdash e' : C$, the desired
$\beta$ law is
\[
  \semg{\letin {()} {()} {e'}} = \semg{e}
\]
\begin{proof}
\begin{align*}
  \semg{\letin {()} {()} {e'}}
  & = \semg{e'} \circ m^{-1}_{\Delta'_{1},\Delta'_{2}} \circ \rho \otimes id \circ (id \otimes \semg{()}) \otimes id \circ m_{\Delta_{1},\cdot} \otimes id \circ m_{(\Delta'_{1},\cdot),\Delta'_{2}} \\
  & = \semg{e'} \circ m^{-1}_{\Delta'_{1},\Delta'_{2}} \circ \rho \otimes id \circ (id \otimes id) \otimes id \circ m_{\Delta_{1},\cdot} \otimes id \circ m_{\Delta'_{1},\Delta'_{2}} \\
  & = \semg{e'} \circ m^{-1}_{\Delta'_{1},\Delta'_{2}} \circ \rho \otimes id \circ \rho^{-1} \otimes id \circ m_{\Delta'_{1},\Delta'_{2}} \\
  & = \semg{e'} \tag{coherence}
\end{align*}
\end{proof}

\paragraph{$I\eta$}
Similarly, for $\Delta_{1}, a : A, \Delta_{3} \vdash e' : C$ and
$\Delta_{2} \vdash e : I$ the desired $\eta$ law is
\[
\semg{\letin {()} {e} {e'[()/a]}} = \semg{e'[e/a]}
\]
However, through application of \cref{lem:subst} it suffices to handle the case
where $e$ is a variable $a'$. That is,
\begin{align*}
\letin {()} {e} {e'[()/a]} &= (\letin {()} {a'} {e'[()/a]})[e/a'] \\
e'[e/a] &= e'[a'/a][e/a]
\end{align*}
so without loss of generality we may take may take $e$ to be variable $a'$. We
will additionally use this style of argumentation when necessary throughout this section.

\begin{proof}
\begin{align*}
  \semg{\letin {()} {a'} {e'[()/a]}}
  & = \semg{e'} \circ m^{-1}_{\Delta'_{1},\Delta'_{2}} \circ \rho \otimes id \circ (id \otimes \semg{a'}) \otimes id \circ m_{\Delta_{1},\cdot} \otimes id \circ m_{\Delta'_{1},\Delta'_{2}} \\
  & = \semg{e'} \circ m^{-1}_{\Delta'_{1},\Delta'_{2}} \circ \rho \otimes id \circ (id \otimes \lambda) \otimes id \circ m_{\Delta_{1},\cdot} \otimes id \circ m_{\Delta'_{1},\Delta'_{2}} \\
  & = \semg{e'} \tag{coherence}
\end{align*}

Because
$m^{-1}_{\Delta'_{1},\Delta'_{2}} \circ \rho \otimes id \circ (id \otimes \lambda) \otimes id \circ m_{\Delta_{1},\cdot} \otimes id \circ m_{\Delta'_{1},\Delta'_{2}}$
is a composition of generalized associators from
$\semg{\Delta'_{1},\Delta'_{2}}$ to itself, it is equal to the identity by the coherence theorem for monoidal categories.

Further by \cref{lem:subst},
\begin{align*}
  \semg{e'[a'/a]}
  &= \semg{e'} \circ \semg{a'/a} \tag{\cref{lem:subst}} \\
  &= \semg{e'} \tag{coherence}
\end{align*}
Again by the coherence theorem, $\semg{a'/a} = id$. Thus, $\eta$ law for $I$
holds in the denotational semantics.
\end{proof}

%tensor
\subsubsection{Tensor}
\paragraph{$\otimes$-Introduction}
$\semg{(e_{1}, e_{2})} : \semg{\Delta,\Delta'} \to \semg{A \otimes B}$ is given
by the diagram

\begin{center}
\begin{tikzcd}
  \semg{\Delta,\Delta'} \arrow[r, "m_{\Delta,\Delta'}"] &
  \semg{\Delta} \otimes \semg{\Delta'} \arrow[r, "\semg{e_1} \otimes \semg{e_2}"] &
  \semg{A} \otimes \semg{G}
\end{tikzcd}
\end{center}

\paragraph{$\otimes$-Elimination}
$\semg{\letin {(a,b)} e f} : \semg{\Delta'_{1},\Delta,\Delta'_{2}} \to \semg{C}$
defined via the diagram,

\begin{center}
\begin{tikzcd}
  \semg{\Delta'_1,\Delta,\Delta'_2} \arrow[r , "m_{(\Delta'_1,\Delta),\Delta'_2}"] &
  \semg{\Delta'_1,\Delta} \otimes \semg{\Delta'_2}
    \arrow[r , "m_{\Delta'_1,\Delta} \otimes id"] &
  \left(\semg{\Delta'_1} \otimes \semg{\Delta} \right) \otimes \semg{\Delta'_2}
    \arrow[d , "(id \otimes \semg{e}) \otimes id"] \\
  \semg{c} & \arrow[l , "\semg{f}"]
  \left( \left( \semg{\Delta'_1} \otimes \semg{A} \right) \otimes \semg{B}\right) \otimes \semg{\Delta'_2}
  & \left(\semg{\Delta'_1} \otimes \left( \semg{A} \otimes \semg{B}\right) \right) \otimes \semg{\Delta'_2}
    \arrow[l , "\alpha \otimes id"]
\end{tikzcd}
\end{center}

\paragraph{$\otimes\beta$}
The desired $\beta$ equality for $\otimes$ is,
\[
  \semg{\letin {(a,b)} {(a',b')} {f}} = \semg{f[a'/a,b'/b]}
\]
\DIFdelbegin %DIFDELCMD < 

%DIFDELCMD < %%%
\DIFdelend \begin{proof}
The left hand side reduces as follows,
\begin{align*}
  \semg{\letin {(a,b)} {(a',b')} {f}}
  & = \semg{f} \circ \alpha \otimes id \circ (id \otimes \semg{(a' , b')}) \otimes id \circ m_{\Delta,\Delta'} \\
  & = \semg{f} \circ \alpha \otimes id \circ (id \otimes \lambda \otimes \lambda \circ m_{a : A, b : B})) \otimes id \circ m_{\Delta,\Delta'} \\
  & = \semg{f} \tag{coherence}
\end{align*}

Which is equal to the right hand side,
\begin{align*}
  \semg{f[a'/a,b'/b]}
  & = \semg{f} \circ \semg{a'/a,b'/b} \\
  & = \semg{f} \tag{coherence}
\end{align*}
\end{proof}

\paragraph{$\otimes\eta$}
The desired $\eta$ equality for $\otimes$ is,
\[
  \semg{\letin {(a,b)} {c'} {f[(a,b)/c]}} = \semg{f[c'/c]}
\]
\DIFdelbegin %DIFDELCMD < 

%DIFDELCMD < %%%
\DIFdelend \begin{proof}
\begin{align*}
  \semg{\letin {(a,b)} {c'} {f[(a,b)/c]}}
  & = \semg{f[(a,b)/c]} \circ \alpha \otimes id \circ (id \otimes \semg{c'}) \otimes id \circ m_{\Delta,\Delta'} \\
  & = \semg{f} \circ \semg{(a,b)/c} \circ \alpha \otimes id \circ (id \otimes \semg{c'}) \otimes id \circ m_{\Delta,\Delta'} \\
  & = \semg{f} \tag{coherence}
\end{align*}

\begin{align*}
  \semg{f[c'/c]}
  &= \semg{f} \circ \semg{c'/c} \tag{\cref{lem:subst}} \\
  &= \semg{f} \tag{coherence}
\end{align*}
\end{proof}


\subsubsection{$\lto$-Functions}
% linear function
\paragraph{$\lto$-Introduction}
$\semg{\lamblto a e} : \semg{\Delta} \to \semg{A \lto B}$ is defined using the
natural isomorphism
$\phi : Hom(\semg{\Delta} \otimes \semg{A}, \semg{B}) \to Hom(\semg{\Delta}, \semg{A \lto B})$
that is provided by the adjunction between $\semg{- \otimes A}$ and $\semg{A \lto -}$.
\DIFdelbegin %DIFDELCMD < 

%DIFDELCMD < %%%
\DIFdelend \[
\semg{\lamblto a e} = \phi\left( \semg{e} \right)
\]
\DIFdelbegin %DIFDELCMD < 

%DIFDELCMD < %%%
\DIFdelend \paragraph{$\lto$-Elimination}
$\semg{\applto {e'} {e}} : \semg{\Delta,\Delta'} \to \semg{B}$ is defined by the
diagram,

\begin{center}
\begin{tikzcd}
  \semg{\Delta,\Delta'} \arrow[r , "m_{\Delta,\Delta'}"] &
  \semg{\Delta} \otimes \semg{\Delta'} \arrow[r , "id \otimes \semg{e}"] &
  \semg{\Delta} \otimes \semg{A} \arrow[r , "\phi^{-1}\left( \semg{e'} \right)"]
  &
  \semg{B}
\end{tikzcd}
\end{center}

\paragraph{$\lto\beta$}
The $\beta$ rule for $\lto$ is given by,
\[
  \semg{\applto {(\lamblto {a} {e})} {a'}} = \semg{e[a'/a]}
\]
\begin{proof}
\begin{align*}
  \semg{\applto {(\lamblto {a} {e})} {a'}}
  & = \phi^{-1}(\semg{\lamblto {a} {e}}) \circ id \otimes \semg{a'} \circ m_{\Delta,\Delta'} \\
  & = \phi^{-1}(\phi(\semg{e})) \circ id \otimes \semg{a'} \circ m_{\Delta,\Delta'} \\
  & = \semg{e} \circ id \otimes \semg{a'} \circ m_{\Delta,\Delta'} \\
  & = \semg{e} \circ id \otimes \lambda \circ m_{\Delta,\Delta'} \\
  & = \semg{e} \tag{coherence}
\end{align*}

\begin{align*}
  \semg{e[a'/a]}
  & = \semg{e} \circ \semg{a'/a} \tag{\cref{lem:subst}} \\
  & = \semg{e} \tag{coherence}
\end{align*}
\end{proof}

\paragraph{$\lto\eta$}
The $\eta$ rule for $\lto$ is given by,
\[
  \semg{\lamblto {a} {\applto e a}} = \semg{e}
\]
\DIFdelbegin %DIFDELCMD < 

%DIFDELCMD < %%%
\DIFdelend \begin{proof}
\begin{align*}
  \semg{\lamblto {a} {\applto e a}}
  & = \phi(\semg{\applto e a}) \\
  & = \phi(\phi^{-1}(\semg{e}) \circ id \otimes \semg{a} \circ m_{\Delta,a:A}) \\
  & = \phi(\phi^{-1}(\semg{e}) \circ id \otimes \lambda \circ m_{\Delta,a:A}) \\
  & = \phi(\phi^{-1}(\semg{e})) \tag{coherence} \\
  & = \semg{e}
\end{align*}
\end{proof}


\subsubsection{$\tol$-Functions}
\paragraph{$\tol$-Introduction}
Just as with the other linear function type, we have an adjunction between
$\semg{A \otimes -}$ and $\semg{- \tol A}$. $\semg{\lambtol a e} : \semg{\Delta} \to \semg{B \tol A}$ is defined using the
natural isomorphism
$\psi : Hom(\semg{A} \otimes \semg{\Delta}, \semg{B}) \to Hom(\semg{\Delta}, \semg{B \tol A})$
induced by this adjunction. In particular, $\semg{\lambtol a e}$ is given by $\psi$
acting on the following diagram

\begin{center}
\begin{tikzcd}
  \semg{A} \otimes \semg{\Delta} \arrow[r , "\lambda^{-1} \otimes id"] &
  \semg{a : A} \otimes \semg{\Delta} \arrow[r , "m^{-1}_{a : A , \Delta}"] &
  \semg{a : A , \Delta} \arrow[r , "\semg{e}"] &
  \semg{B}
\end{tikzcd}
\end{center}

\paragraph{$\tol$-Elimination}
The application of a $\tol$-function,
$\semg{\apptol {e} {e'}} : \semg{\Delta',\Delta} \to \semg{B}$ defined by the diagram

\begin{center}
\begin{tikzcd}
  \semg{\Delta',\Delta} \arrow[r , "m_{\Delta',\Delta}"] &
  \semg{\Delta'} \otimes \semg{\Delta} \arrow[r , "\semg{e'} \otimes id"] &
  \semg{A} \otimes \semg{\Delta} \arrow[r , "\psi^{-1}\left( \semg{e} \right)"] &
  \semg{B}
\end{tikzcd}
\end{center}

\paragraph{$\tol\beta$}
The $\beta$ rule for $\tol$ is given by,
\[
  \semg{\apptol {(\lambtol {a} {e})} {a'}} = \semg{e[a'/a]}
\]
\begin{proof}
\begin{align*}
  \semg{\apptol {(\lambtol {a} {e})} {a'}}
  & = \psi^{{-1}}(\semg{\lambtol {a} {e}}) \circ \semg{a'} \otimes id \circ m_{a:A,\Delta} \\
  & = \psi^{{-1}}(\semg{\lambtol {a} {e}}) \circ \lambda \otimes id \circ m_{a:A,\Delta} \\
  & = \psi^{{-1}}(\psi(\semg{e} \circ m^{-1}_{a:A,\Delta} \circ \lambda^{-1} \otimes id)) \circ \lambda \otimes id \circ m_{\Delta',\Delta} \\
  & = \semg{e} \circ m^{-1}_{a:A,\Delta} \circ \lambda^{-1} \otimes id \circ \lambda \otimes id \circ m_{a:A,\Delta} \\
  & = \semg{e}
\end{align*}

\begin{align*}
  \semg{e[a'/a]}
  & = \semg{e} \circ \semg{a'/a} \tag{\cref{lem:subst}} \\
  & = \semg{e} \tag{coherence}
\end{align*}
\end{proof}

\paragraph{$\tol\eta$}
The $\eta$ rule for $\tol$ is given by,
\[
  \semg{\lambtol {a} {\apptol e a}} = \semg{e}
\]
\begin{proof}
\begin{align*}
  \semg{\lambtol {a} {\apptol e a}}
  & = \phi(\semg{\apptol e a} \circ m^{-1}_{a:A,\Delta} \circ \lambda^{-1} \otimes id) \\
  & = \phi(\phi^{-1}(\semg{e}) \circ \semg{a} \otimes id \circ m_{a:A,\Delta} \circ m^{-1}_{a:A,\Delta} \circ \lambda^{-1} \otimes id) \\
  & = \phi(\phi^{-1}(\semg{e}) \circ \lambda \otimes id \circ m_{a:A,\Delta} \circ m^{-1}_{a:A,\Delta} \circ \lambda^{-1} \otimes id) \\
  & = \phi(\phi^{-1}(\semg{e})) \\
  & = \semg{e}
\end{align*}
\end{proof}

\subsubsection{$\bigamp$-Products}
% Products
\paragraph{$\bigamp$-Introduction}
$\semg{\dlamb x e} : \semg{\Delta} \to \prod_{x : X} {\sem{A}(\gamma , x)}$ is
defined by the universal property of the product
\[
\semg{\dlamb x e} = \left( \sem{e}(\gamma , x) \right)_{(x : X)}
\]

\paragraph{$\bigamp$-Elimination}
$\semg{e.\pi~M} : \semg{\Delta} \to \sem{A}(\gamma , M)$ is defined using the
projection out of the product,

\begin{center}
\begin{tikzcd}
  \semg{\Delta} \arrow[r , "\semg{e}"] &
  \prod_{x : X} {\sem{A}(\gamma , x)} \arrow[r , "\pi_M"] &
  \sem{A}(\gamma , M)
\end{tikzcd}
\end{center}

\paragraph{$\bigamp\beta$}
The $\beta$ law for $\bigamp$ is given by,
\[
  \sem{(\dlamb x e).\pi~M}(\gamma, x) = \sem{e[M/x]}(\gamma, x)
\]
\begin{proof}
\begin{align*}
  \semg{(\dlamb x e).\pi~M}
  & = \pi_{M} \circ \semg{\dlamb x e} \\
  & = \pi_{M} \circ (\sem{e}(\gamma , x))_{(x : X)} \\
  & = \sem{e}(\gamma , M)
\end{align*}
by the universal property of the product.


\begin{align*}
  \sem{e[M/x]}(\gamma , x)
  & = \sem{e}(\gamma , x) \circ  \sem{M/x}(\gamma , x) \\
  & = \sem{e}(\gamma , M)
\end{align*}
\end{proof}

\paragraph{$\bigamp\eta$}
The $\eta$ law for $\bigamp$ is given by,
\[
  \semg{(\dlamb x {e.\pi~x})} = \semg{e}
\]
\begin{proof}
\begin{align*}
  \semg{(\dlamb x {e.\pi~x})}
  & = (\semg{e.\pi~x})_{x : X} \\
  & = (\pi_x \circ \semg{e})_{x : X} \\
  & = \semg{e}
\end{align*}
\end{proof}
by the universal property of the product.

\subsubsection{$\bigoplus$-Sums}
% Sums
\paragraph{$\bigoplus$-Introduction}
$\semg{\sigma~M~e} : \semg{\Delta} \to \coprod_{x : X} \sem{A}(\gamma, x)$

\begin{center}
\begin{tikzcd}
  \semg{\Delta} \arrow[r , "\semg{e}"] &
  \sem{A}(\gamma, M) \arrow[r , "i_M"] &
  \coprod_{x : X} {\sem{A}(\gamma , x)}
\end{tikzcd}
\end{center}

\paragraph{$\bigoplus$-Elimination}
$\semg{\letin {\sigma~x~a} {e} {e'}} : \semg{\Delta'_{1},\Delta,\Delta'_{2}} \to \semg{C}$
is defined in the diagram

\begin{center}
% https://tikzcd.yichuanshen.de/#N4Igdg9gJgpgziAXAbVABwnAlgFyxMJZABgBpiBdUkANwEMAbAVxiRAB124YBbAc2CcAIjAY46AcgD6ARlLDR4+exFjJUgEwBfEFtLpMufIRRzK1es1aIOXXgIVrpcx+K0ACThDw94nu-yCKorq2rr6IBjYeAREcjJUtIwsbJwMMABmOAAU-tyBruoyHl4+fpz5DsFqJewATlh8ABY4AJT+3li+cHn2Qari0mF6BtHGcaQaiZYpNpwAMlhgAMqNACoAnos8uO7AAB4ewAAaRxW8-SHOpHTuiO4AgsoDoVrZnHx0PDy3pO77rR0I0ihhiJhIk2myWstkWK3WWy6uwOR1OezSmRyvR4lycsjeHy+P3cfwBHTKcHYYHcNNp5xxDwJ7E+31+-3anAazTa5K65QCuMGmiZLOJpMB4VGRliKDICQs0NS7HSWVy9KqL2ctU63X8cNWfE222Rhz2aKC+WAjPezKJbIBtS5LQ57B1-MqgtekpBYxlyDkAGYoVYlZaAMIiu3i71RaXgshBhUhuYBAQR3SJGBQPjwIigDJ1CA8JBkEA4CBIORJZMgHhSYA2zWyZ4hVotvFhagMOgAI1EAAVQeMbCqcN6C0XK9Ry0gNEnZrX64VroUtKU+T0sFAQF3ewOhzKQKPx4Xi4g52WK4gAKzzmHZLe83Xq4AwLQut2b7e7vsMQe+kwj0xE9J0QANpyvAA2O82GQbFXwkSNWRJdl3Aoetsn2O53GOCUf33AC2GPYEJzPAAWCCkHA6sFx4AA9YAAFpigw5dmzoRAHjbNjOyPPc-wPQDiIiUikFvS8kAomiYR3Pjf3-OMiOAkjTyQaCJMQAB2LQKC0IA
\begin{tikzcd}[column sep =large]
{\semg{\Delta'_1,\Delta,\Delta'_2}} \arrow[r, "{m_{(\Delta'_1,\Delta),\Delta'_2}}"]                                                                                                           & {\semg{\Delta'_1,\Delta} \otimes \semg{\Delta'_2}} \arrow[d, "{m_{\Delta'_1,\Delta}\otimes id}"]                             \\
{\left( \semg{\Delta'_1} \otimes \coprod_{x:X} {\sem{A}(\gamma , x)} \right) \otimes \semg{\Delta'_2}} \arrow[d, "d"]                                                                 & \left( \semg{\Delta'_1} \otimes \semg{\Delta} \right) \otimes \semg{\Delta'_2} \arrow[l, "(id \otimes \semg{e}) \otimes id"] \\
{\coprod_{x : X} {\left( \sem{\Delta'_1}(\gamma , x) \otimes
      \sem{A}(\gamma , x) \right) \otimes \sem{\Delta'_2}(\gamma , x)}} \arrow[r, "\coprod_{x:X}({m^{-1}_{(\Delta'_1,a:A),\Delta'_2}})"] & {\coprod_{x : X} {\sem{\Delta'_1,a : A,\Delta'_2}(\gamma , x)}} \arrow[d, "{[ \sem{e'}(\gamma , x) ]_{(x : X)}}"]     \\
\semg{C}                                                                                                                                                                                      & {\sem{C}(\gamma, x)} \arrow[l]
\end{tikzcd}
\end{center}

where $d$ is the distributivity morphism, and the last morphism implicitly
weakens $\sem{C}$.

\paragraph{$\bigoplus\beta$}
The $\beta$ rule for $\bigoplus$ is given by,
\[
  \semg{\letin {\sigma~x~a} {\sigma~M~a'} {e'}} = \semg{e'[M/x,a'/a]}
\]
\begin{proof}
  \begin{align*}
  \semg{\letin {\sigma~x~a} {\sigma~M~a'} {e'}}
  & = {[ \sem{e'}(\gamma , x) ]_{(x : X)}} \circ \coprod_{x:X}(m^{-1}) \circ d \circ (id
  \otimes \semg{\sigma~M~a'}) \otimes id \circ m \otimes id \circ m \\
  & = {[ \sem{e'}(\gamma , x) ]_{(x : X)}} \circ \coprod_{x:X}m^{-1} \circ d \circ (id
  \otimes (i_M \circ \semg{a'})) \otimes id \circ m \otimes id \circ m \\
  & = {[ \sem{e'}(\gamma , x) ]_{(x : X)}} \circ \coprod_{x:X} m^{-1} \circ d \circ (id \otimes i_M) \otimes id \circ (id \otimes  \lambda) \otimes id \circ m \otimes id \circ m \\
  & = {[ \sem{e'}(\gamma , x) ]_{(x : X)}} \circ \coprod_{x:X} m^{-1} \circ i_M \circ (id \otimes  \lambda) \otimes id \circ m \otimes id \circ m \\
  & = \sem{e'}(\gamma , M) \circ m^{-1} (id \otimes  \lambda) \otimes id \circ m \otimes id \circ m \\
  & = \sem{e'}(\gamma , M) \tag{coherence}
  \end{align*}

  \begin{align*}
    \semg{e'[M/x,a'/a]}
    &= \semg{e'} \circ \semg{M/x,a'/a} \tag{\cref{lem:subst}} \\
    &= \sem{e'}(\gamma , x)
  \end{align*}
\end{proof}

\paragraph{$\bigoplus\eta$}
It suffices to show
\[
  \semg{\letin {\sigma~x~a} {c'} {f[\sigma~x~a/c]}} = \semg{f[c'/c]} = \semg{f}
  \]
First, expanding the left hand side, we have.
\begin{proof}
  \begin{align*}
    \semg{\letin {\sigma~x~a} {c'} {f[\sigma~x~a/c]}}
    &= [ \semg{f[\sigma~x~a/c]} ]_{(x : X)} \circ \coprod_{x:X}(m^{-1}) \circ d \circ (id \otimes \lambda) \otimes id \circ m \otimes id \circ m\\
    &= [ \semg{f} \circ (id \otimes i_x) \otimes id ]_{(x : X)} \circ \coprod_{x:X}(m^{-1}) \circ d \circ (id \otimes \lambda) \otimes id \circ m \otimes id \circ m\\
    &= \semg{f} \circ [ (id \otimes i_x) \otimes id \circ (m^{-1})]_{(x : X)} \circ d \circ (id \otimes \lambda) \otimes id \circ m \otimes id \circ m\\
  \end{align*}
Since the domain has the universal property of a coproduct (due to distributivity), to prove
this is equal to $\semg{f}$, it is sufficient to prove they are equal
when composed with the injections:

\begin{align*}
  \semg{f} &\circ [ (id \otimes i_x) \otimes id \circ (m^{-1})]_{(x : X)} \circ d \circ (id \otimes \lambda) \otimes id \circ m \otimes id \circ m \circ (id \otimes i_y) \otimes id\\
  &= \semg{f} \circ [ (id \otimes i_x) \otimes id \circ (m^{-1})]_{(x : X)} \circ d  \circ (id \otimes i_y) \otimes id \circ (id \otimes \lambda) \otimes id \circ m \otimes id \circ m \tag{naturality}\\
  &= \semg{f} \circ [ (id \otimes i_x) \otimes id \circ (m^{-1})]_{(x : X)} \circ i_y \otimes id \circ (id \otimes \lambda) \otimes id \circ m \otimes id \circ m \tag{naturality}\\
  &= \semg{f} \circ (id \otimes i_y) \otimes id \circ (m^{-1}) \otimes id \circ (id \otimes \lambda) \otimes id \circ m \otimes id \circ m \\
  &= \semg{f} \circ (m^{-1}) \otimes id \circ (id \otimes \lambda) \otimes id \circ m \otimes id \circ m \circ (id \otimes i_y) \otimes id \\
  &= \semg{f} \circ (id \otimes i_y) \otimes id \tag{coherence}\\
\end{align*}
\end{proof}

\subsubsection{Equalizer}
%equalizer
\paragraph{Equalizer Introduction}
$\semg{\equalizerin{e}} : \semg{\Delta} \to \semg{\equalizer e f g}$ where
$\semg{e} : \semg{\Delta} \to \semg{A}$ and
$\semg{f} \circ \semg{e} = \semg{g} \circ \semg{e}$. By the universal property
of the equalizer the preceding equality induces a unique morphism
$\semg{\Delta} \to Eq(\semg{f} , \semg{g}) = \semg{\equalizer e f g}$. Define
$\semg{\equalizerin{e}}$ to be this map.

\paragraph{Equalizer Elimination}
$\semg{\equalizerpi e} : \semg{\Delta} \to \semg{A}$ is defined using the map
$\pi_{eq}$ from $Eq(\semg{f} , \semg{g})$ to the domain of $f$ and $g$.
\DIFdelbegin %DIFDELCMD < 

%DIFDELCMD < %%%
\DIFdelend \[
  \semg{\equalizerpi e} = \pi_{eq} \circ \semg{e}
\]
\DIFdelbegin %DIFDELCMD < 

%DIFDELCMD < %%%
\DIFdelend \paragraph{Equalizer $\beta$}
The $\beta$ rule for $\equalizer e f g$ is given as,
\[
\semg{\equalizerpi {\equalizerin e}} = \semg{e}
\]
where $\semg{f} \circ \semg{e} = \semg{g} \circ \semg{e}$.
\begin{proof}
  In $\semcat$ the universal property of $Eq(\semg{f}, \semg{g})$ implies that
  the following diagram commutes, implying the $\beta$ rule.

% https://q.uiver.app/#q=WzAsNCxbMSwwLCJcXHNlbWd7QX0iXSxbMiwwLCJcXHNlbWd7Qn0iXSxbMCwwLCJFcShcXHNlbWd7Zn0sXFxzZW1ne2d9KSJdLFswLDEsIlxcc2VtZ3tcXERlbHRhfSJdLFswLDEsIlxcc2VtZ3tmfSIsMCx7Im9mZnNldCI6LTF9XSxbMCwxLCJcXHNlbWd7Z30iLDEseyJvZmZzZXQiOjF9XSxbMiwwLCJcXHBpX3tlcX0iXSxbMywwLCJcXHNlbWd7ZX0iLDJdLFszLDIsIlxcc2VtZ3tcXGVxdWFsaXplcmluIGV9Il1d
\begin{center}
\begin{tikzcd}
	{Eq(\semg{f},\semg{g})} & {\semg{A}} & {\semg{B}} \\
	{\semg{\Delta}}
	\arrow["{\pi_{eq}}", from=1-1, to=1-2]
	\arrow["{\semg{f}}", shift left, from=1-2, to=1-3]
	\arrow["{\semg{g}}", shift right, swap, from=1-2, to=1-3]
	\arrow["{\semg{\equalizerin e}}", from=2-1, to=1-1]
	\arrow["{\semg{e}}"', from=2-1, to=1-2]
\end{tikzcd}
\end{center}
\end{proof}

\paragraph{Equalizer $\eta$}

The $\eta$ rule for $\equalizer e f g$ is given as,
\[
\semg{\equalizerin {\equalizerpi e}} = \semg{e}
\]
\begin{proof}
  Likewise, the universal property of $Eq(\semg{f}, \semg{g})$ implies $\eta$
  rule via this diagram.

\begin{center}
\begin{tikzcd}
	{Eq(\semg{f},\semg{g})} & {\semg{A}} & {\semg{B}} \\
	{\semg{\Delta}}
	\arrow["{\pi_{eq}}", from=1-1, to=1-2]
	\arrow["{\semg{f}}", shift left, from=1-2, to=1-3]
	\arrow["{\semg{g}}", shift right, swap, from=1-2, to=1-3]
	\arrow["{\semg{e}}", from=2-1, to=1-1]
	\arrow["{\semg{\equalizerpi e}}"', from=2-1, to=1-2]
\end{tikzcd}
\end{center}
\end{proof}
\DIFaddbegin 


\subsection{\DIFadd{Grammar Semantics Respects Additional Axioms}}
\DIFadd{We verify that the denotational semantics validates each of the axioms we have
assumed.
}

\paragraph{\DIFadd{\mbox{%DIFAUXCMD
\cref{ax:dist}}\hskip0pt%DIFAUXCMD
}} \DIFadd{Distributivity
}\begin{theorem}[\Agda{\fillerlink}]
  \DIFadd{In $\Grammar$, $\semg{\lamblto{e}{\letin{\sigma\,f\,e'}{e}{\withlamb{x}{e'\,.\pi\,x}}}}$ has
  an inverse.
}\end{theorem}
\begin{proof}
  \DIFadd{Distributivity is true in the denotational semantics, as the category $\Grammar$ is a topos, which are well-known to be distributive.
  The following map forms the desired inverse,
  }\[
    \DIFadd{\lamb {\gamma} {\lamb {w} {\lamb {p} {(\lamb {\left( \lamb {x} {(p~x).fst} \right)} , \lamb {x} {(p~x).snd})}}}
  }\]
\end{proof}

\paragraph{\DIFadd{\mbox{%DIFAUXCMD
\cref{ax:disjointness}}\hskip0pt%DIFAUXCMD
}} \DIFadd{$\sigma$-Disjointness
}\begin{theorem}[\Agda{\fillerlink}]
  \DIFadd{In $\Grammar$, $\semg{\sigma x}$ and $\semg{\sigma x'}$ are disjoint for
  $x \neq x'$.
}\end{theorem}
\begin{proof}
  \DIFadd{This is trivially true, as the denotation of linear sum types is as $\Sigma$
  types in $\Grammar$. For any input, the first projections of $\semg{\sigma x}$
  is $x$ and the first projection of
  $\semg{\sigma x'}$ is $x'$. Because $x \neq x'$, the images of
  $\semg{\sigma x}$ and $\semg{\sigma x'}$ cannot agree.
}\end{proof}

\paragraph{\DIFadd{\mbox{%DIFAUXCMD
\cref{ax:string-top}}\hskip0pt%DIFAUXCMD
}} \DIFadd{String is strongly equivalent to $\top$.
}\begin{theorem}[\Agda{\fillerlink}]
  \DIFadd{In $\Grammar$, $\semg{!}$ has an inverse.
}\end{theorem}
\begin{proof}
  \DIFadd{Because $\semg{\top}w$ is a singleton set for all $\gamma$ and $w$, it suffices to show that
  $\semg{\StringGram}w$ is likewise a singleton.
}

  \DIFadd{First, we prove by induction on $w$ that $\semg{\StringGram}w$ is a retract of
  $\semg{\LinSigTy{w}{\StringSem}{\internalize{w}}}w$, a sum over a nonlinear
  type of strings. Then, again by
  induction on $w$, we show that
  $\semg{\LinSigTy{w}{\StringSem}{\internalize{w}}}w$ is isomorphic to
  $\semg{\top}w$.
}

  \DIFadd{Each $\semg{\internalize{w}}w'$ is inhabited if and only if $w$ is equal to
  $w'$, and the parses of $w$ for $\semg{\internalize{w}}$ are unique. That is,
  $\semg{\LinSigTy{w}{\StringSem}{\internalize{w}}}w$ is a singleton set.
  So, $\semg{\StringGram}w$ is a retract of a singleton set, and is itself
  singleton. Thus, $\semg{\StringGram}w \cong \semg{\top}w$ in $\Set$.
}\end{proof}

\DIFaddend \fi
\end{document}
