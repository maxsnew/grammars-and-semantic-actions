#+title: A Logic for Regular Expressions
#+LATEX_HEADER: \usepackage{proof}
#+LATEX_HEADER: \newcommand{\sep}{-\kern-.25em\raisebox{-.62ex}{*}\ }
#+LATEX_HEADER: \newcommand{\sepinv}{\raisebox{-.62ex}{*}\kern-.5em-\ }
#+LATEX_HEADER: \newcommand{\then}{~\textbf{then}~}
#+LATEX_HEADER: \newcommand{\foldr}{\textbf{foldr}}
#+LATEX_HEADER: \newcommand{\foldl}{\textbf{foldl}}
#+LATEX_HEADER: \newcommand{\fold}{\textbf{fold}}
#+LATEX_HEADER: \newcommand{\roll}{\textbf{roll}}
#+LATEX_HEADER: \newcommand{\letbf}{\textbf{let}~}
#+LATEX_HEADER: \newcommand{\inbf}{~\textbf{in}~}
#+LATEX_HEADER: \newcommand{\case}{\textbf{case}~}
#+LATEX_HEADER: \newcommand{\appbf}{\textbf{app}}
#+LATEX_HEADER: \newcommand{\andbf}{~\textbf{and}~}
* Introduction
Can we formalize the behavior of regular expressions in a fragment of bunched implications (BI). Just as in BI, we have additive connectives --- $\top , \land , \to , \bot , \lor$ --- and multiplicative connectives --- $\varepsilon , \ast, \sep, \sepinv$. Likewise, there is a multiplicative and additive distinction between combination of contexts.

Here contexts are trees built up as,

$$\Gamma ::= p : g ~|~ \bullet ~|~ \Gamma, \Gamma ~|~ \Gamma ; \Gamma$$

That is, contexts are either built from proof terms (parse trees) of a type (syntactic grammar), empty, combined multiplicatively, or combined additively. In the style of O'Hearn and Pym (The Logic of Bunched Implications), the multiplicative and additive unit contexts are both simply the empty context here. As in BI, intuitively we think of multiplicative combination as denoting some resource management. That is, $p : g, q : g'$ is a context that marks that we have a $g$ parse /followed by/ a $g'$ parse. While additive combination of contexts means that each grammar simulatenously holds --- i.e. $p : g ; q : g'$ means simulatenously, $g$ and $g'$ are simulaneously parsed.

For a small example, consider the string $w = abbbbb$. Multiplicatively, $w$ matches the pattern $a$ followed by the pattern $b^{*}$ --- i.e. $w$ matches $a \ast b^{*}$. /Note/: we may often omit the tensor product and take things to be implicitly tensor-producted together via juxtaposition --- so we can simply write $ab^*$ instead of $a \ast b^*$.

Additively, $w$ matches the both patterns $a \ast b^*$ and $ab \ast b^*$. So we could say that that $w$ matches $(a b^*) \land (ab b^*)$, the additive (cartesian) product of $ab^*$ and $abb^*$.

** Contexts as trees
The syntactic construction of contexts as above leads to an intuitive view of contexts as trees with nodes labeled by $;$ and $,$. Just as in O'Hearn and Pym, $\Gamma(-)$ represents an incomplete tree with a hole where a subtree may be placed. We often only want to reason about these trees up to equivalence, as to avoid fixating too much on any one given representation.

Our notion of equivalence is similar to that of O'Hearn and Pym's, except we do not require $\bullet$ and $,$ to be a commutative monoid, only a monoid. We do not want commutativity so that we may model the ordered nature of strings to be parsed.

* The Rules
- Identity and Structure

$$ \infer[  Id] {p : g \vdash p : g}{} $$

$$ \infer[\equiv] {\Delta \vdash \phi(p) : g}{\Gamma \vdash p : g }$$ where $\Gamma \equiv \Delta$ and $\phi : \Gamma \to \Delta$ is an isomorphism witnessing the equivalence

$$ \infer[W] {\Gamma(\Delta ; \Delta') \vdash p : g}{\Gamma(\Delta) \vdash p : g }$$

$$ \infer[C] {\Gamma(\Delta ) \vdash p : g}{\Gamma(\Delta ; \Delta) \vdash p : g }$$


- Tensor Product

  The intended semantics modeling the concatenation of strings. A parse of $g \ast g'$ is a parse of $g$ followed by a parse of $g'$. That is, an input string $w$ should match $g \ast g'$ if there is a split $w = w_1 w_2$ such that $w_1$ matches $g$ and $w_2$ matches g'$. However, I don't want to focus too much on semantics in this moment, as this is simply the syntactic inference rules.

 $$ \infer[\ast I]  {\Gamma_1, \Gamma_2 \vdash p \then q : g_1 \ast g_2} {\Gamma_1 \vdash p : g_1 & \Gamma_2 \vdash q : g_2} $$

 $$ \infer[\ast E]  {\Gamma(\Delta) \vdash \letbf (u \then v) = p \inbf q : g} {\Delta \vdash p : g_1 \ast g_2 & \Gamma(u : g_1, v : g_2) \vdash q : g}$$

- Disjunction

$$  \infer[\lor I_i]  {\Gamma \vdash \textbf{in}_i(p) : g_1 \lor g_2} {\Gamma \vdash p : g_i} $$
 $$ \infer[\lor E]  {\Delta(\Gamma) \vdash \case p \{ \textbf{in}_0 u \mapsto q_1, \textbf{in}_1 v \mapsto q_2 \} : g} {\Gamma \vdash p : g_1 \lor g_2 & \Delta(u : g_1) \vdash q_1 : g &  \Delta(v : g_2) \vdash q_2 : g}$$

- Conjunction

  Parsing a $g \land g'$ means an input string should match both $g$ and $g'$ simultaneously.

$$ \infer[\land I]  {\Gamma ; \Delta \vdash (p, q) : g_1 \land g_2} {\Gamma \vdash p : g_1 & \Delta \vdash q : g_2} $$
$$ \infer[\land E_i]  {\Gamma \vdash \pi_i (p) : g_i}{\Gamma \vdash p : g_1 \land g_2} $$(deprecated?)
$$ \infer[\land E]  {\Gamma(\Delta) \vdash \letbf (u \andbf v) = q \inbf p  : g}{\Gamma(u : g_1 ; v : g_2) \vdash p : g & \Delta \vdash q : g_1 \land g_2} $$


- Magic Wand
$$ \infer[\sep I] {\Gamma \vdash \lambda^{\sep} x . p : g_1 \sep g_2} { x : g_1 , \Gamma \vdash p : g_2} $$
$$ \infer[\sep E] {\Gamma_1, \Gamma_2 \vdash \appbf_{\sep}(q, p) : g_2} { \Gamma_1 \vdash p : g_1 & \Gamma_2 \vdash q : g_1 \sep g_2 }$$
$$ \infer[\sepinv I] {\Gamma \vdash \lambda^{\sep} x . p : g_2 \sepinv g_1}
{ \Gamma , x : g_1  \vdash p : g_2} $$
$$ \infer[\sepinv E] {\Gamma_1, \Gamma_2 \vdash \appbf_{\sepinv}(q, p) : g_2} { \Gamma_2 \vdash p : g_1 & \Gamma_1 \vdash q : g_1 \sepinv g_2 }$$


- Bottom
$$ \infer[\bot E]  {\Gamma \vdash \textbf{absurd} : g'} {\Gamma \vdash p : \bot} $$

- Empty String
$$ \infer[\varepsilon I]  {\bullet \vdash () : \varepsilon} {} $$
$$ \infer[\varepsilon E]  {\Delta(\Gamma) \vdash \letbf () = p \inbf q : g} {\Gamma \vdash p : \varepsilon & \Delta(\bullet) \vdash q : g} $$


- Implication
$$\infer[\to I]{\Gamma \vdash \lambda x . q : g \to g'}{\Gamma ; x : g \vdash q : g'}$$

$$\infer[\to E]{\Gamma ; \Delta \vdash f(p) : g'}{\Gamma \vdash f : g \to g' & \Delta \vdash p : f}$$

- Kleene Star

Construct it as a special case of $\lor$ --- i.e. $\varepsilon \lor (g \ast g*)$. Then constrain it to be the least fixed point through the fold construction. The two introduction forms make it so that the star is an algebra of $F(X) = 1 + gX$, whereas the elimination form constrains it to be an /initial/ algebra.
$$  \infer[Kl I \varepsilon]  {\Gamma \vdash \textbf{in}_{\varepsilon} : g^{\ast}}{\Gamma \vdash () : \varepsilon} $$

$$  \infer[Kl I \ast]  {\Gamma_1, \Gamma_2 \vdash \textbf{in}_{\ast} (p , q) : g^{\ast}}{\Gamma_1 \vdash p : g & \Gamma_2 \vdash q : g^{\ast}} $$
$$ \infer [Kl \ast E] {\Gamma \vdash \foldr (p_{\varepsilon} , p_{\ast}) : g'}{\Gamma \vdash p : g^{\ast} & \bullet \vdash p_{\varepsilon} : g' & x : g, y : g' \vdash p_{\ast} : g'} $$

- Least Fixed Point Operator
Introduce a least fixed point operator. This could then be used to make $g^{\ast}$ admissible, as well as make more general recursive grammars.
$$\infer[\mu I] {\Gamma \vdash \roll (p) : \mu \alpha . g }{\Gamma \vdash p : g[\mu \alpha . g / \alpha]}$$
$$ \infer[\mu E] {\Gamma \vdash \fold (p, q) : h} {\Gamma \vdash p : \mu \alpha . g & x : g[h / \alpha] \vdash q : h} $$

We also want a least fixed point operator for /mutually recursive/ definitons.

** Some TODOS
*** TODO we originally had two eliminators $\land E_i$, but I think the rule $\land E$ is preferable and more closely linked to whats in the original BI paper. It gives a richer way to affect the contexts
*** TODO I think its justified to take the additive empty context and the multiplicative empty context to be the same, but I'm not positive on this
*** TODO I'm not sure that all my proof terms are put together in a reasonable way. I mean is $\letbf (u \andbf v) = q \inbf p$ good syntax???
*** TODO come up with better tex for the wands, especially $\sepinv$
*** TODO actually unsure at the moment what the appropriate inference rule is for mutually recursive definitions. Don't want to get too lost in the formalism at the moment.

** Algebras of which functors???
Pedro suggests that when categorifying Kleene Algebra, one needs to require that the Kleene star is the initial algebra of 3 distinct functors --- $1 + GX$, $1 + XG$, and one other.

In our formalism, we are taking as primitive that the star is an intial algebra of $1 + GX$. And given the same premises as $Kl \ast E$ we are able to construct a term $\foldl$ that demonstrates that the star is also an algebra of $1 + XG$. It however is not clear to me if this an intial algebra, or if we can expect to do this for the third functor.

*** TODO track down a good description of the third functor
*** TODO type up foldl
* Encoding Automata
** Nondeterministic Finite Automata (NFA)
Using the mutually recursive constructor for grammars, we are able to construct a grammar where the internal mutually recrusive terms represent the states of an NFA and the constructors of these mutually recursive terms correspond to the transitions of the NFA. We write a syntactic form declaring what it means to be in /NFA form/, then we build an isomorphism between a given regular expresssion and an equivalent grammar in NFA form. We may think of this process as internalizing Thompson's construction for the NFA of a regular expression to our formalism.

*** NFA Form
Intuitively we think of each construct $Y_i$ as some state internal to the automaton. We reserve a marked start state and end state $Y_{start}$ and $Y_{end}$, respectively. Let the following occur over some given alphabet $\Sigma$,

$$ guard ::= \varepsilon ~|~ c$$
where $c \in \Sigma$

$$state ::= (guard \ast Y_j) ~|~ (internalState \lor internalState)$$

Then a gramamr is said to be in NFA form if it has the form,

$$\mu (Y_{start} = <state>, Y_1 = <state>, \dots, Y_n = <state>, Y_{end} = \varepsilon ) \inbf Y_{start}$$

As suggestively notated, we think of this an NFA with one start state and one accepting state, without loss of generality.

**** TODO there is surely a cleaner and more succint way to write what an NFA form is recursively but its 7pm and I'm tired. I will fix these silly issues tomorrow
**** TODO type up proofs of equivalence

*** Proof that NFA is equivalent to a regular expression
We do this in two steps. First, we inductively show the equivalence of the regular expression with an intermediate grammar that nearly matches the NFA form, except it is not flattened. Then we show the equivalence of this intermediate representation and its flattening (which is in NFA form). Combining each of these proof, we arrive at a provably correct NFA that recognizes the given regular expression.
