# Intrinsic Verification of Parsers and Formal Grammar Theory in Dependent Lambek Calculus

## Getting Started

### Compiling the repository

From the `code/Cubical` directory, `make` will compile `README.agda` which imports the entirety of the project. This may take longer than 30 minutes, as it will also compile the dependencies from `cubical` and `cubical-categorical-logic` if they are not already built. You may also build `README.agda` interactively by loading the file with [agda-mode](https://agda.readthedocs.io/en/v2.7.0.1/tools/emacs-mode.html).

If the compilation of `README.agda` doesn't immediately crash, and you can see it checking submodules, it is very likely that the there will be no technical difficulties. We have also included the target `make litmus` which builds only the `Grammar` submodule as a shorter litmus test to check for issues of technical compatibility.

`README.agda` imports several files of the form `${dir}.Everything`. In a directory `dir`, the module `${dir}.Everything` imports all the modules within `dir` or any of its submodules. These `Everything` files are automatically generated by the target `make gen-and-check-everythings`.

## High-Level Layout
This repository is split into the following directories 
- `String` - contains the defintion as the list type over some fixed alphabet, and some associated utilities. `String := List ⟨ Alphabet ⟩`, where `Alphabet : hSet`
- `Grammar` - defining the primitive linear types in Dependent Lambek Calculus. Linear types are encoded as functions from strings to types, written as `Grammar ℓA = String → Type ℓA`.
- `Term` - defining parse transformers between grammars. A parse transformer between `A` and `B` is written as the type `A ⊢ B` (or `Term A B`).
- `Parser` - the definition of a parser as described in definiton 4.4.
- `Automata` - defining the following automata formalisms as grammars: DFAs, NFAs, deterministic (but not necessarily finite) automata, and Turing machines.
- `Examples` - containing intrinsically verified parsers for the Dyck grammar and an arithmetic expression grammar. Additionally has the examples from the figures in Section 2 encoded.
- `Thompson` - a verification of Thompson's construction: from a regular expression `r` construct an NFA and prove that it is strongly equivalent to `r`.
- `Determinization` - a verification of the powerset construction for determinization. Given an NFA `N`, construct a DFA that is weakly equivalent to it.
- `Cubical` - utilities that supplement the `Cubical` standard library. This is a soft fork of the `Cubical` standard library, and so we put these pieces of code under the same `Cubical` namespace. Ideally we will pull request these into the `Cubical` standard library, as they are general purpose utilities that are not grammar-specific.
- `Lexer` - An experimental module that defines a lexer as a translation between alphabets. Used for experimenting with language ergonomics, but not a dependence to any claims presented.

## Dependent Lambek Calculus
Dependent Lambek Calculus (`Lambekᴰ`) is a domain-specific dependent type theory for verified parsing and formal grammar theory. We use linear types as a syntax for formal grammars, and parsers can be written as linear terms. The linear typing restriction provides a form of intrinsic verification that a parser yields only valid parse trees for the input string. 

We build an implementation of Dependent Lambek Calculus by encoding its denotational semantics within Cubical Agda. The syntax is then interpreted via the following encodings:

### Non-linear Types
In the denotational semantics of `Lambekᴰ`, the non-linear types are interpreted in the category `Set`. In our implementation, we simply reuse Agda's type system to encode the non-linear fragment of the theory. That is, the universe of non-linear types `U` is interpreted in Agda as `Type ℓ` (at some level `ℓ`).

Because we reuse the host language to encode the non-linear types, we give no embedded syntax for the non-linear fragment of `Lambekᴰ`. That is, the non-linear `Lambekᴰ` types `A B : U` are interpreted as Agda types `A B : Type ℓ`, and non-linear terms `A ⊢ B` of `Lambekᴰ` are interperted with the ordinary function type in Agda `A → B`.

### Linear Types
In the denotational semantics, linear types are represented as type-valued functions over strings. We use that same encoding in this implementation with the `Grammar` type, given in `Grammar.Base`

``` agda
Grammar : Type (ℓ-suc ℓA)
Grammar = String → Type ℓA
```

For a grammar `A : Grammar ℓA` and a string `w : String`, we view the type `A w : Type ℓA` as the set of parse trees demonstrating that `w` matches the grammar `A`. An inhabitant `p : A w` then is a single parse tree.

#### Grammar Connectives
The typing connectives of `Lambekᴰ` are given in the `Grammar` module. Here we include a summary of a selection of the modules.

##### Concatenation
For instance, the concatenation of grammars is given in `Grammar.LinearProduct.Base`:

``` agda
_⊗_ : Grammar ℓA → Grammar ℓB → Grammar (ℓ-max ℓA ℓB)
(A ⊗ B) w = Σ[ s ∈ Splitting w ] A (s .fst .fst) × B (s .fst .snd)
```

where `Splitting w` is the data of two substrings `w₁ w₂ : String`, such that `w₁ ++ w₂ ≡ w`. In the above code, `w₁` and `w₂` are written as `s .fst .fst` and `s .fst .snd`, respectively. That is, a string matches the concatenation of `A` and `B` if that string may be split into two substrings such that `A` matches the left substring and `B` matches the right substring.

##### Linear Unit
Similarly, the nullary concatenation, written as `I` in the paper, is given in `Grammar.Epsilon.Base`:

``` agda
ε : Grammar ℓ-zero
ε w = w ≡ []
```

A string matches `ε` if and only if it is the empty string.

##### Empty Grammar
The empty grammar `0 : L`, which is given as a nullary sum, is implemented in `Grammar.Bottom.Base` using Agda's empty type,

``` agda
⊥ : Grammar ℓ-zero
⊥ _ = Empty.⊥
```

The nullary additive sum above is more usually named `0`, as in the paper, while the name `⊥` is reserved for the nullary multiplicative sum. 

We have not implemented the multiplicative sum `⅋` so this name clash has not yet been problematic for us, although we want to clarify that this development has used a nonstandard name.

##### Products and Sums
In `Grammar.Product.Base` we define indexed conjunction as a `Π`-type.

``` agda
&ᴰ : {X : Type ℓX} → (X → Grammar ℓA) → Grammar (ℓ-max ℓX ℓA)
&ᴰ {X = X} f w = ∀ (x : X) → f x w
```

Given `A : X → Grammar ℓA`, the grammar `&ᴰ A` may sometimes be written as `&[ x ∈ X ] A x`.

We can define binary products as an indexed product over `Bool`, which we give in `Grammar.Product.Binary.Inductive`. 

We may instead define an admissible primitive where the binary product is implemented semantically as a pair, given in `Grammar.Product.Binary.Cartesian`.

``` agda
_&_ : Grammar ℓA → Grammar ℓB → Grammar (ℓ-max ℓA ℓB)
(A & B) w = A w × B w
```

Further, in `Grammar.Product.Binary.Cartesian.Properties`, we prove that these two different binary product types are equivalent.

Similarly, we define indexed sums in `Grammar.Sum.Base`. Likewise, we have two choices for how we may wish to implement a binary sum (given in `Grammar.Sum.Binary.Inductive` and `Grammar.Sum.Binary.Cartesian`) and we prove them equivalent in `Grammar.Sum.Binary.Cartesian.Properties`.

### Linear Terms

In `Lambekᴰ`, a derivation `Γ ; A ⊢ B ` denotes a parse transformer that translates parses of grammar `A` into parses of grammar `B`. These parse transformers are encoded in this `Term.Base` via the type,

``` agda
module _ (A : Grammar ℓA) (B : Grammar ℓB) where
  Term : Type (ℓ-max ℓA ℓB)
  Term : ∀ (w : String) → A w → B w
```

That is, a parse transformer from `A` to `B` is a function that that maps parses of `A` to parses of `B` for each input string `w`.

We often write `A ⊢ B` as a synonym for `Term A B`.

#### Contexts
The contexts in the implementation are unary, so the `Lambekᴰ` derivations of the form `A , B ⊢ C` are represented in the code as terms `A ⊗ B ⊢ C`. Similarly, `Lambekᴰ` terms in the empty context `∙ ⊢ A` are represented in the code as terms `ε ⊢ A`.

#### `A ⊢ B` vs. `↑ (A ⊸ B)`
In the paper syntax, we write `↑ (A ⊸ B)` to describe the parse transformers from `A` to `B`. 

For a grammar `C`, `↑ C` denotes the parses of `C` in the empty context and we define this encoding in `Term.Nullary`. By leveraging the adjunction between `⊗` and `⊸`, and using the fact that `ε` is the unit for `⊗`, it is true that `↑ (A ⊸ B)` and `A ⊢ B` are equivalent types. This equivalence is proven in `Grammar.LinearFunction.Base` with `Term≅Element`. However, in this implementation we almost exclusively use `A ⊢ B` to encode parser transformers instead of `↑ (A ⊸ B)`.

Because the two types are equivalent, the choice of representation does affect any of the semantic claims. Additionally, the use of `A ⊢ B` is preferable as it has favorable definitional behavior over `↑ (A ⊸ B)`. 

#### Some Example Linear Terms
The linear terms in our implementation are written in a combinatory style, rather than in exactly the same syntax presented in the paper. The parse transformers in the implementation must then be built up from base combinators in a point-free style.

For instance, in Figure 1 of the paper we provide the linear term in `Lambekᴰ`:

``` agda
f : ↑ (＂ a ＂ ⊗ ＂ b ＂ ⊸ (＂ a ＂ ⊗ ＂ b ＂) ⊕ ＂ c ＂)
f (a , b) = inl (a ⊗ b)
```

`f` matches on its input which is a tensor and introduces two parse trees, `a : ＂ a ＂` and `b : ＂ b ＂`. Then `f` recombines these parse trees with a tensor and calls `inl`. 

Our implementation captures the same parse transformer, except we do not have the ability to introduce named variables in this manner. Instead, the same parse transformer is implemented in `Examples.Section2.Figure1` using the `inl` combinator from `Grammar.Sum.Binary.Cartesian`.

``` agda
f : ＂ a ＂ ⊗ ＂ b ＂ ⊢ ＂ a ＂ ⊗ ＂ b ＂ ⊕ ＂ c ＂
f = inl
```

Similarly, in Figure 3 we give the `Lambekᴰ` term

``` agda
g : ↑ (＂ a ＂ ⊗ ＂ b ＂ ⊸ ((＂ a ＂ *)  ⊗ ＂ b ＂) ⊕ ＂ c ＂)
g (a , b) = inl (cons a nil ⊗ b)
```

This same parse transformer is given via combinators in `Examples.Section2.Figure3`

``` agda
g : ＂ a ＂ ⊗ ＂ b ＂ ⊢ (＂ a ＂ *) ⊗ ＂ b ＂ ⊕ ＂ c ＂
g = inl ∘g (CONS ∘g id ,⊗ NIL ∘g ⊗-unit-r⁻) ,⊗ id
```

Note, in this implementation the contexts `A , ·`, `· , A`, and `A` are encoded differently. On paper, the empty context is definitionally a unit for context extension, but in the term `g` above we must manually insert an empty piece of the context through the combinator `⊗-unit-r⁻ : ∀ {A : Grammar ℓA} → A ⊢ A ⊗ ε`.

Further note that we write programs in this implementation via the composition operator `_∘g_`. Because of this, the parse transformers that we write are often read from right to left (or bottom to top when spanning multiple lines). This may be seen in the parse transformer given in `Examples.Section2.Figure4`

``` agda
h : (＂ a ＂ ⊗ ＂ a ＂) * ⊢ ＂ a ＂ *
h = fold*r (＂ a ＂ ⊗ ＂ a ＂)
  -- nil case : ε ⊢ ＂ a ＂ *
  NIL
  -- cons case : (＂ a ＂ ⊗ ＂ a ＂) ⊗ (＂ a ＂ *) ⊢ ＂ a ＂ *
  (CONS
  ∘g id ,⊗ CONS -- turn the left side of the ⊗ into an ＂ a ＂ *
  ∘g ⊗-assoc⁻ -- reassociate to ＂ a ＂ ⊗ (＂ a ＂ ⊗ ＂ a ＂ *)
  )
```

Looking at the case for `cons` when defining this fold, we read from the bottom up. That is, 

``` agda
CONS
∘g id ,⊗ CONS -- turn the left side of the ⊗ into an ＂ a ＂ *
∘g ⊗-assoc⁻ -- reassociate to ＂ a ＂ ⊗ (＂ a ＂ ⊗ ＂ a ＂ *)
```

defines a term `(＂ a ＂ ⊗ ＂ a ＂) ⊗ (＂ a ＂ *) ⊢ ＂ a ＂ *`, and this term first executes `⊗-assoc⁻`, then`id ,⊗ CONS`, and then finally `CONS`.

### Opacity
Many of the definitions in this repository are marked as `opaque`. [Opacity](https://agda.readthedocs.io/en/v2.7.0.1/language/opaque-definitions.html) is a feature in Agda that allow selective unfolding of definitions. 

When normalizing, a term defined in an `opaque` block will not reduce unless it is explicitly marked with an `unfolding`. Opacity is also infective in that any definition that wishes to unfold an `opaque` definiton must itself be marked as `opaque`.

We use opacity for a few reasons:

- Reducing typechecking time by limiting unnecessary normalizations.
- Putting up an explicit barrier between the embedded language and the means by which we encode the embedded language in Agda. By marking our language primitives as `opaque` we gain finer grained control of their unfoldings. In particular, we can ensure that certain equalities occur in the embedded equational theory of `Lambekᴰ` rather than by happenstance in Agda. 
- We can selectively unfold the definitions of language primitives to act as a rudimentary solver for `β`-reductions in `Lambekᴰ`. We will discuss this point in more detail in the caveats section below.

## Claims

The contributions of this codebase are 
1. A domain-specific language for building intrinsically verified parsers. This comprises the entire repository, but the bulk of language primitives are found in the `Grammar`, `Term`, `Parser`, and `String` modules.
2. Example tasks demonstrating how to program in the language. Such as,
  - A parser for an `LL(0)` grammar of balanced parentheses, given in `Examples.Dyck`.
  - A parser for an `LL(1)` grammar of arithmetic expressions, given in `Examples.BinOp`.
  - A verification of Thompson's construction, given in `Thompson.Equivalence`.
  - A verification of the powerset construction for NFA determinization, given in `Determinization.WeakEquivalence`.

Below, we include the code locations of the specific claims made in the paper.

### Definition 4.1

> Grammars `A` and `B` are __weakly equivalent__ if there exists parse transformers `f : ↑ (A ⊸ B)` and `g : ↑ (B ⊸ A)`.

Defined in `Grammar.Equivalence.Base` as `LogicalEquivalence`, which we often write as `A ≈ B`. 

The `Equivalence` module also contains a definiton named `isWeaklyEquivalent`, which is the characterization that two grammar are __semantically__ weakly equivalent, in the sense that they recognize the same sets of strings. 

Despite the naming, the notion of "weak equivalence" from the paper is captured by the `LogicalEquivalence` type which is encoded using the equational theory of the embedded language, rather than a semantic characterization.

> `A` is a __retract__ of `B` if `A` and `B` are weakly equivalent and `λ a . g (f (a)) ≡ λ a . a`

This property of retraction is not given an explicit definition in the codebase, but we often capture the data of being a retraction as input to a helper function. For instance, the definition of `hasRetraction→isMono` from `Grammar.Equivalence.Base` takes in arguments that encode the retraction property.

> They are __strongly equivalent__ if further the other composition is the identity, i.e., `λ b . f (g (b)) ≡ λ b . b`.

Defined in `Grammar.Equivalence.Base` as `StrongEquivalence`, often written as `A ≅ B`.

### Definition 4.2

> A grammar `A` is __unambiguous__ if for every linear type `B` and `f g : ↑ (B ⊸ A)` then `f ≡ g`.

Defined as `umambiguous` in `Grammar.Properties.Base`. The same file also includes other equivalent characterizations of unambiguity for a grammar `A` that are provable in `Lambekᴰ`:

- The unique map `A ⊢ ⊤` being a monomorphism, where `⊤` is defined in `Grammar.Top.Base`.
- The map `Δ : A ⊢ A & A` being an isomorphism, where `&` is a binary product defined in `Grammar.Product.Binary.Cartesian`.

There is also a semantic notion of unambiguity, given in `Grammar.HLevels.Base` via the type `Lang`. This explicitly encodes that a grammar has at most one parse tree for every input string. This semantic characterization of unambiguity is not meant to be used directly, as it involves semantic reasoning that does not occur inside of the embedded language. However, it is useful for axiomatizing language primitives. 

For instance, in `Grammar.Epsilon.Properties` we use the semantic notion of unambiguity to axiomatize the fact that `ε` is an unambiguous grammar.

### Lemma 4.3

> If `B` is unambiguous and `A` is a retract of `B`, then `A` is unambiguous.

Given in `Grammar.Properties.Base` as `isUnambiguousRetract`.

>If a disjunction `⊕[ x ∈ X ] A x` is unambiguous then each `A x` is unambiguous

Given in `Grammar.Sum.Unambiguous` as `unambiguous⊕ᴰ`. This defintion requires that the map `σ : A x ⊢ ⊕[ x ∈ X ] A` is a monomorphism, which is axiomatized in the same file as `isMono-σ`.

### Definition 4.4

> A parser for a linear type `A` is a function `↑ (string ⊸ A ⊕ B)` where `B` is a linear type that is __disjoint__ from `A` in that we can implement a function `↑ (A & B ⊸ 0)`

This definiton of disjointness is given in `Grammar.Properties.Base`.

The definiton of a parser is given in `Parser.Base`.

### Lemma 4.5

> If `A ⊕ B` is unambiguous, then `A` and `B` are disjoint.

Given for all indexed sums as `hasDisjointSummands⊕ᴰ` in `Grammar.Sum.Unambiguous`. This lemma is a consequence of the disjoint constructors axioms, which is encoded in the same file as `equalizer→⊥`.

For the binary sums implemented in `Grammar.Sum.Binary.Cartesian`, we prove this lemma in `Grammar.Sum.Binary.Cartesian.Properties` under the name `unambig-⊕-is-disjoint`.

### Lemma 4.6

> If `A` is weakly equivalent to `B`, then any parser for `A` can be extended to a parser for `B`.

Defined as `≈Parser` in `Grammar.Parser.Base`.

### Theorem 4.7

> A parser for the accepting traces of a DFA.

DFAs are defined in `Automata.DFA.Base`. The type `DFA` is implemented using a more general construction `DeterministicAutomaton` from `Automata.Deterministic`.

`DeterministicAutomaton` encodes a determinstic labelled transition system over a type of states `Q : Type ℓ`. A `DFA` is then a `DeterministicAutomaton` where the type of states is a finite set. 

A parser for a `DeterministicAutomaton` is given in `Automata.Deterministic` as `AccTraceParser`. Because `DFA` is just a special case of this more general automaton, `AccTraceParser` is also a parser for `DFA`s.

### Theorem 4.8 (Determinization)

> For an NFA `N`, there exists a DFA `D` such that `Parse D` is weakly equivalent to `Parse N`.

NFAs are defined in `Automata.NFA.Base`. The determinization construction is given in `Determinization.WeakEquivalence` as `NFA≈DFA`. 

There are a couple non-linear analyses we perform over an NFA `N : NFA` to enable this construction:

1. Given several traces through `N`, the determinization construction needs to deterministically choose one. A priori, the states of `N`, the type of labelled transitions in `N`, and the type of `ε`-transitions in `N` are each finite sets. To enable the choice function for traces through `N`, we require each of these types are not just finite sets, but that they are further __finitely ordered__. By making each of these types ordered, we then have a well-defined way to choose the __smallest__ trace when determinizing. 
  - The definitions of `isFinSet` (a finite set) and `isFinOrd` (a finite order) may be found in the Cubical standard library under `Cubical.Data.FinSet`.

2. When building the powerset DFA, we define a `DFA` whose states are `ε`-closed subsets of states in `N`. To begin to reason about these `ε`-closed subsets, we need to decide if there is a path in `N` between any two states solely through `ε`-transitions. To make that decision, in `Cubical.Data.Quiver.Reachability` we prove that in any finite `Quiver` we can decide whether any two nodes are connected. To build the `ε`-closed subsets for the DFA, we then instantiate our `Quiver.Reachability` module with a `Quiver` whose nodes are the states of `N` and whose edges are the `ε`-transitions of `N`. 

### Theorem 4.9 (Thompson's Construction)

> For a regular expression `r`, there exists and NFA `N` such that `r` is strongly equivalent to `Parse N`.

In `Grammar.RegularExpression.Base`, we define a non-linear type of regular expressions and its interpretation as grammars (`RegularExpression` and `RegularExpression→Grammar`, respectively).

Then in the module `Thompson.Construction`, we have a submodule for each regular expression constructor. Each of these submodules builds the corresponding `NFA` and proves that the `NFA` is strongly equivalent to that case of regular expression. For instance, `Thompson.Construction.Literal` takes in a character `c : ⟨ Alphabet ⟩`, constructs `literalNFA c : NFA ℓ-zero`, and then proves that the parses of `literalNFA c` and `＂ c ＂` are strongly equivalent as grammars.

`Thompson.Equivalence` gathers up all of the equivalences from `Thompson.Construction` and recursively proves that every regular expression is strongly equivalent to the parses of its corresponding `NFA`.

### Theorem 4.10

> `Dyck` is strongly equivalent to `Parse M`, and therefore we can build a `Dyck` parser.

`Examples.Dyck` contains `Dyck`, the grammar of balanced parentheses. In this file we instantiate the module `Automata.Deterministic` to build the machine `M` depicted in Figure 12. 

The term `Dyck≅Trace` shows that the accepting traces of this automaton are strongly equivalent to `Dyck`. Finally, `DyckParser` is the `Parser` for `Dyck` that arises from porting the parser for the accepting traces of the automaton over the equivalence between `Dyck` and the type of accepting traces.

### Theorem 4.11

> We construct a parser for `Exp` by showing that it is weakly equivalent to the accepting traces from the opening state with its stack set to `0`.

In `Examples.BinOp`, we define a grammar of arithmetic expressions over a binary operation `+`. 

The module `LL⟨1⟩` found in this file defines the grammars `EXP` and `ATOM`. The module `Automaton` in this file defines the lookahead automaton given in Figure 13. 

`Automaton.TraceParser` defines a parser for the accepting traces beginning at any state in the lookahead automaton.

The module `Soundness` in this file builds a term `buildExp` from the accepting traces out of the initial state of the automaton to `EXP`. The module `Completeness` builds a term `mkTrace` from `EXP` to the type of accepting traces from the initial state.

We then combine the terms `buildExp` and `mkTrace` in `AccTrace≈EXP` to show that `EXP` and `Trace true (0 , Opening)` are weakly equivalent. `EXPParser` is then the parser for `EXP` that arises from combining `AccTrace≈EXP` and `Automaton.TraceParser`.

### Theorem 4.12

> For any Turing machine `T`, we construct a grammar that accepts the same language as `T`.

In `Grammar.Reify.Base`, we define the `Reify` grammar 

``` agda
module _ (P : String → Type ℓA) where
  Reify : Grammar ℓA
  Reify = ⊕[ w ∈ String ] ⊕[ x ∈ P w ] ⌈ w ⌉
```

In some sense, `Reify` is like a foreign function interface that allows one to use the faculties of the non-linear fragment when reasoning about parsers. That is, `Reify` allows the user to treat the non-linear type-valued function `P` as if it were a proper linear type. 

In `Automata.Turing.OneSided.Base`, we non-linear define a type of Turing machine specifications `TuringMachine`. Then for a fixed Turing machine `TM : TuringMachine`, we define a type of traces through that machine `TuringTrace`.

We then define `Accepting : String → Type ℓ-zero` as the non-linear type of proofs that a given string is accepted by `TM` when ran from the initial state. Using `Reify`, we can then treat this non-linear function `Accepting` like it is a grammar.

``` agda
Turing : Grammar ℓ-zero
Turing = Reify Accepting
```

### Additonal Axioms Holds in the Semantics

> Additive conjunction distributes over additive disjunction

Given in `Grammar.Distributivity`.

> The constructors of sums are disjoint.

Given as `equalizer→⊥` in `Grammar.Sum.Unambiguous`.

> We add a function `read : ↑ (⊤ ⊸ string)`.

Given as `read` in `Grammar.String.Terminal`.

> `⊤` is strong equivalent to `string`.

Given as `string≅⊤` in `Grammar.String.Terminal`.

## Evaluation

We propose evaluating this codebase by running it through the Agda typechecker. To regenerate all the `Everything` files and check the entire project, from `code/Cubical` run

``` console
make clean
make gen-and-check-everythings
make
```

A successful compilation will complete with no errors. Once `agda` has typechecked a particular module, it will cache the result in the `_build/` directory. So repeated checks will be much faster.

You may also choose to typecheck individual files, either interactively or from the command line with `agda`. For instance,

``` console
agda README.agda
```

will again check the entire project.

``` console
agda Grammar/Everything.agda
```

will check the `Grammar` submodule. Or,

``` console
agda Thompson/Construction/Literal.agda 
```

will only check only the file `Thompson/Construction/Literal.agda`.

## Caveats

### Unsafe Pragmas

We make use of the `TERMINATING` pragma in the following locations:

- `Grammar.Inductive.HLevels` in the definitons of `encode` and `isRetract`.
- `Grammar.Inductive.Indexed` in the defintions of `recHomo` and `μ-η'`.

We use these pragmas because the way in which we roll our own encoding of inductive types is not structurally decreasing on their input. Because we have constructed the functors in `Grammar.Inductive.Functor` to be use their arguments strictly positively, we know that the naive recursion used in these definition does indeed terminate, but not in a manner that satisfies the termination checker.

Additonally we use a `NO_POSITIVITY_CHECK` pragma in:
- `Grammar.Inductive.Indexed`

This `NO_POSITIVITY_CHECK` is needed because our use of opacity in the connective `⊗` blocks the positivity checker for the definiton of `μ` in `Grammar.Inductive.Indexed`. If `⊗` were not opaque, [then this definiton would pass the positivity checker](https://github.com/agda/agda/issues/6970).

### Opacity

One of the benefits of using `opaque` definitons throughout the codebase is the enforcement of abstraction boundaries between the embedded language and Agda as a host language.

The most faithful encoding of `Lambekᴰ` would only break these abstraction boundaries when axiomatizing a language primitive. Usage of any language constructs, and proofs about any language constructs would then occur with any explicit `unfolding`. This strategy would guarantee that any proofs of equality between linear terms would follow from only from equational reasoning in `Lambekᴰ`. That is, there would be no possibility to "accidentally get an equality correct" by leveraging external reasoning available in Agda that isn't available in `Lambekᴰ`.

We have kept this attitude in mind throughout the development and tried to unfolding minimally. However, by unfolding we also can enable Agda to solve for `β`-equalities within `Lambekᴰ` that would otherwise be long chains that have us manually invoke lemmas.

#### An example with `⊗-intro`
For example, consider the following two terms,

``` agda
module _ (e : A ⊢ B) (f : B ⊢ C) (g : D ⊢ E) (h : E ⊢ F) where
  ϕ ψ : A ⊗ B ⊢ C ⊗ F
  ϕ = (f ∘g e) ,⊗ (h ∘g g)
  ψ = (f ,⊗ h) ∘g (e ,⊗ g)
```

A priori, `ϕ` and `ψ` are not defintionally equal. We may derive their equality, but that involves manual reasoning every time that we compose maps in parallel. Instead, if we unfold the defintion of `⊗-intro`, then `ϕ` and `ψ` become definitonally equal. So while in the strictest sense, by unfolding `⊗-intro` could have accidentally invoked external reasoning that doesn't hold in `Lambekᴰ`; in practice, we use Agda's definitional equality as a rudimentary solver for `β`-equalities that hold in `Lambekᴰ`.

We apply this same principle throughout all of our code. Any instance of unfolding is either a deliberate breaking of abstraction boundaries to build the language, or it is to have Agda solver for equalities that are derivable within `Lambekᴰ` anyway.

Here are the other terms that we unfold to solve for `β`-equalities:

- `⊕-elim` from `Grammar.Sum.Binary.Cartesian` so that we can leverage the definitional equalities that hold over Agda's `Sum` type.
- `π₁`/`π₂` from `Grammar.Product.Binary.Cartesian` so that we can leverage the definitional equalities that hold over Agda's `×` type.
- `⊕ᴰ-distL`/`⊕ᴰ-distR` from `Grammar.Sum.Properties`, which are distributivity of sums over `⊗`. These make the equalities `⊕ᴰ-distL-β`/`⊕ᴰ-distR-β` hold definitionally. 
- A combination of `⊗-intro`, `⊗-unit-l`/`⊗-unit-r`, `⊗-unit-l⁻`/`⊗-unit-r⁻` and `⊗-assoc` to use these equalities from `Grammar.LinearProduct.Base`:
  - `⊗-assoc⁻4⊗-intro`
  - `id,⊗id≡id`
  - `⊗-unit-r⁻⊗-intro`
  - `⊗-unit-l⁻⊗-intro`
  - `⊗-assoc⁻⊗-intro`
- `eq-intro` from `Grammar.Equalizer.Base` to expose that it is the identity on parse trees while additionally tagging a parse with a proof of equality.














